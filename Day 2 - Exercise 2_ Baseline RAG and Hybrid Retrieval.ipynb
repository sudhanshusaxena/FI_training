{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfac1c83",
   "metadata": {},
   "source": [
    "\n",
    "# Day 2 — Exercise 2: Baseline RAG & Hybrid Retrieval (with LiteLLM + RRF)\n",
    "\n",
    "**Objective:** Develop a retrieval-augmented generation (RAG) system with **hybrid retrieval**.\n",
    "\n",
    "**Tasks:**\n",
    "- Build a simple **RAG pipeline (retrieve → read)** using the vector store from Ex1 to answer 10 domain-specific queries.\n",
    "- Extend it to a **hybrid retrieval** system combining **BM25 (sparse)** and **dense embeddings**, and merge with **Reciprocal Rank Fusion (RRF)**.\n",
    "- Compare **Top-5 retrieved documents** for **BM25**, **Dense**, and **Hybrid** approaches.\n",
    "\n",
    "**Learning Outcome:** Learn the mechanics of RAG and how **hybrid retrieval** improves relevance over single-method retrieval.\n",
    "\n",
    "> Notes:\n",
    "> - This notebook uses **LiteLLM** as the API wrapper for embeddings & chat models, so you can switch providers with minimal code changes.\n",
    "> - Optional: a debug/enrichment panel shows how to query a FIL/Perplexity-style endpoint for web search (off by default).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5935c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n",
    "Install dependencies and import packages. Rerun this cell if you modify environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452854fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU litellm chromadb rank_bm25 faiss-cpu numpy pandas scikit-learn tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7f957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# For cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LiteLLM wrapper\n",
    "from litellm import embedding, completion\n",
    "\n",
    "# Optional: Chroma or FAISS (we'll support both paths)\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a62ee1",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configuration\n",
    "\n",
    "Centralize all knobs here. Update paths and keys before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382d112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Paths & Artifacts ====\n",
    "DATA_DIR = os.getenv(\"DOCS_DIR\", \"./data/docs\")           # Where raw docs live (used if rebuilding index)\n",
    "ARTIFACT_DIR = \"./artifacts/day2_ex2\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# Vector store choices: \"chroma\" or \"faiss\"\n",
    "VECTOR_STORE_KIND = os.getenv(\"VECTOR_STORE_KIND\", \"chroma\")\n",
    "\n",
    "# Persisted vector store paths\n",
    "CHROMA_DIR = os.path.join(ARTIFACT_DIR, \"chroma_store\")\n",
    "FAISS_INDEX_PATH = os.path.join(ARTIFACT_DIR, \"faiss.index\")\n",
    "FAISS_META_PATH = os.path.join(ARTIFACT_DIR, \"faiss_meta.json\")\n",
    "\n",
    "# ==== Retrieval Settings ====\n",
    "TOP_K_RETRIEVE = 5\n",
    "RRF_K = 60  # reciprocal rank fusion constant\n",
    "\n",
    "# ==== Embeddings ====\n",
    "# Two models to compare interchangeably via LiteLLM\n",
    "EMBED_MODEL_A = os.getenv(\"EMBED_MODEL_A\", \"text-embedding-3-small\")  # OpenAI\n",
    "EMBED_MODEL_B = os.getenv(\"EMBED_MODEL_B\", \"sentence-transformers/all-MiniLM-L6-v2\")  # HF via LiteLLM route (if configured)\n",
    "\n",
    "# Normalize vectors for cosine similarity (recommended)\n",
    "NORMALIZE_EMBEDDINGS = True\n",
    "\n",
    "# ==== Reader / Chat Model ====\n",
    "CHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")  # Any LiteLLM-supported chat model\n",
    "\n",
    "# ==== Keys / Environment ====\n",
    "# Set provider keys in env, e.g., OPENAI_API_KEY, etc.\n",
    "# If using Azure/OpenAI/Anthropic/etc., set their corresponding env vars as required by LiteLLM.\n",
    "os.environ.setdefault(\"LITELLM_LOG\", \"INFO\")  # or DEBUG\n",
    "\n",
    "# ==== Optional: FIL/Perplexity-style Web Debug Panel ====\n",
    "USE_FIL_PERPLEXITY = os.getenv(\"USE_FIL_PERPLEXITY\", \"false\").lower() == \"true\"\n",
    "FIL_HOST = os.getenv(\"FIL_HOST\", \"\")\n",
    "FIL_ACCESS_TOKEN = os.getenv(\"FIL_ACCESS_TOKEN\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe43a9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Document Loading & Chunking (fallback if vector store not found)\n",
    "\n",
    "If a persisted vector store isn't found, we will rebuild by:\n",
    "1. Loading files from `DATA_DIR` (txt/pdf/csv basic loader).\n",
    "2. Chunking text.\n",
    "3. Indexing into the chosen vector store (Chroma/FAISS) with LiteLLM embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a9711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt(path: str) -> str:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception:\n",
    "        with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "            return f.read()\n",
    "\n",
    "def read_csv(path: str) -> str:\n",
    "    df = pd.read_csv(path)\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "def read_pdf(path: str) -> str:\n",
    "    # minimal no-deps placeholder: in real use, install pypdf and parse properly\n",
    "    try:\n",
    "        import pypdf  # type: ignore\n",
    "        text = []\n",
    "        reader = pypdf.PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            text.append(page.extract_text() or \"\")\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception:\n",
    "        # Fallback: just record the file name\n",
    "        return f\"[PDF placeholder] {os.path.basename(path)} (install pypdf for full text)\"\n",
    "    \n",
    "def load_docs_from_dir(data_dir: str) -> List[Dict[str, Any]]:\n",
    "    paths = []\n",
    "    paths += glob.glob(os.path.join(data_dir, \"**/*.txt\"), recursive=True)\n",
    "    paths += glob.glob(os.path.join(data_dir, \"**/*.csv\"), recursive=True)\n",
    "    paths += glob.glob(os.path.join(data_dir, \"**/*.pdf\"), recursive=True)\n",
    "\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        ext = os.path.splitext(p)[1].lower()\n",
    "        if ext == \".txt\":\n",
    "            content = read_txt(p)\n",
    "        elif ext == \".csv\":\n",
    "            content = read_csv(p)\n",
    "        elif ext == \".pdf\":\n",
    "            content = read_pdf(p)\n",
    "        else:\n",
    "            continue\n",
    "        docs.append({\n",
    "            \"doc_id\": str(uuid.uuid4()),\n",
    "            \"source\": p,\n",
    "            \"text\": content\n",
    "        })\n",
    "    return docs\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, chunk_overlap: int = 100) -> List[str]:\n",
    "    # Simple whitespace chunker\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk = \" \".join(tokens[start:end])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - chunk_overlap\n",
    "        if chunk_size - chunk_overlap <= 0:\n",
    "            break\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878af90",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Embedding Helpers (LiteLLM)\n",
    "\n",
    "Two interchangeable embedding models are configured. We also support normalization and basic on-disk caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a70309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_texts(texts: List[str], model: str) -> np.ndarray:\n",
    "    # Batch for stability\n",
    "    BATCH = 64\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(texts), BATCH), desc=f\"Embedding with {model}\"):\n",
    "        batch = texts[i:i+BATCH]\n",
    "        resp = embedding(model=model, input=batch)\n",
    "        vecs = [d[\"embedding\"] for d in resp[\"data\"]]\n",
    "        all_vecs.extend(vecs)\n",
    "    arr = np.array(all_vecs, dtype=np.float32)\n",
    "    if NORMALIZE_EMBEDDINGS:\n",
    "        norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "        arr = arr / norms\n",
    "    return arr\n",
    "\n",
    "def save_json(path: str, obj: Any):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215483e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Vector Store Adapters (Chroma & FAISS)\n",
    "\n",
    "We try to **load** existing stores. If not available, we **rebuild** from `DATA_DIR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c7d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Corpus:\n",
    "    texts: List[str]\n",
    "    metadatas: List[Dict[str, Any]]\n",
    "    ids: List[str]\n",
    "\n",
    "def build_corpus_from_docs(docs: List[Dict[str, Any]], chunk_size=800, chunk_overlap=100) -> Corpus:\n",
    "    texts, metas, ids = [], [], []\n",
    "    for d in docs:\n",
    "        chunks = chunk_text(d[\"text\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        for idx, ch in enumerate(chunks):\n",
    "            texts.append(ch)\n",
    "            metas.append({\"source\": d[\"source\"], \"doc_id\": d[\"doc_id\"], \"chunk\": idx})\n",
    "            ids.append(str(uuid.uuid4()))\n",
    "    return Corpus(texts=texts, metadatas=metas, ids=ids)\n",
    "\n",
    "# --------- Chroma ----------\n",
    "def chroma_load_or_build(corpus: Corpus, embed_model: str):\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DIR, settings=ChromaSettings(allow_reset=True))\n",
    "    coll = client.get_or_create_collection(name=f\"day2_ex2_{embed_model.replace('/', '_')}\")\n",
    "\n",
    "    # If empty, build it\n",
    "    if coll.count() == 0:\n",
    "        vecs = embed_texts(corpus.texts, embed_model).tolist()\n",
    "        coll.add(documents=corpus.texts, metadatas=corpus.metadatas, ids=corpus.ids, embeddings=vecs)\n",
    "    return coll\n",
    "\n",
    "# --------- FAISS ----------\n",
    "def faiss_load_or_build(corpus: Corpus, embed_model: str):\n",
    "    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_META_PATH):\n",
    "        index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "        meta = load_json(FAISS_META_PATH)\n",
    "        return index, meta\n",
    "\n",
    "    vecs = embed_texts(corpus.texts, embed_model)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # works best with normalized vectors (cosine via dot product)\n",
    "    index.add(vecs)\n",
    "\n",
    "    meta = {\"texts\": corpus.texts, \"metadatas\": corpus.metadatas, \"ids\": corpus.ids, \"embed_model\": embed_model}\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "    save_json(FAISS_META_PATH, meta)\n",
    "    return index, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376b2c4",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Build / Load Vector Store\n",
    "\n",
    "- Tries to **load** a persisted store first.\n",
    "- If missing, **rebuilds** from `DATA_DIR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50edb692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No persisted store found; rebuilding from DATA_DIR = ./data/docs\n",
      "WARNING: No documents found in DATA_DIR. Please add some .txt/.csv/.pdf files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding with text-embedding-3-small: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Build or load store for EMBED_MODEL_A (primary dense retriever)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VECTOR_STORE_KIND \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     dense_store \u001b[38;5;241m=\u001b[39m chroma_load_or_build(corpus, EMBED_MODEL_A)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChroma collection size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dense_store\u001b[38;5;241m.\u001b[39mcount())\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mchroma_load_or_build\u001b[0;34m(corpus, embed_model)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# If empty, build it\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coll\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     vecs \u001b[38;5;241m=\u001b[39m embed_texts(corpus\u001b[38;5;241m.\u001b[39mtexts, embed_model)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     25\u001b[0m     coll\u001b[38;5;241m.\u001b[39madd(documents\u001b[38;5;241m=\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mids, embeddings\u001b[38;5;241m=\u001b[39mvecs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coll\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36membed_texts\u001b[0;34m(texts, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_vecs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m NORMALIZE_EMBEDDINGS:\n\u001b[0;32m---> 12\u001b[0m     norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(arr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m\n\u001b[1;32m     13\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr \u001b[38;5;241m/\u001b[39m norms\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/linalg/_linalg.py:2829\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2827\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m     s \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mconj() \u001b[38;5;241m*\u001b[39m x)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m-> 2829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[1;32m   2830\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mord\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load docs only if needed\n",
    "need_docs = False\n",
    "if VECTOR_STORE_KIND == \"chroma\":\n",
    "    # Check if there is already a persisted Chroma collection (we'll just attempt and see count)\n",
    "    client_probe = chromadb.PersistentClient(path=CHROMA_DIR, settings=ChromaSettings(allow_reset=True))\n",
    "    try:\n",
    "        probe = client_probe.get_collection(name=f\"day2_ex2_{EMBED_MODEL_A.replace('/', '_')}\")\n",
    "        empty = probe.count() == 0\n",
    "        if empty:\n",
    "            need_docs = True\n",
    "    except Exception:\n",
    "        need_docs = True\n",
    "else:\n",
    "    # FAISS\n",
    "    if not (os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_META_PATH)):\n",
    "        need_docs = True\n",
    "\n",
    "if need_docs:\n",
    "    print(\"No persisted store found; rebuilding from DATA_DIR =\", DATA_DIR)\n",
    "    docs = load_docs_from_dir(DATA_DIR)\n",
    "    if len(docs) == 0:\n",
    "        print(\"WARNING: No documents found in DATA_DIR. Please add some .txt/.csv/.pdf files.\")\n",
    "    corpus = build_corpus_from_docs(docs)\n",
    "else:\n",
    "    # If we don't need docs, still provide a corpus with metadata if available\n",
    "    docs = load_docs_from_dir(DATA_DIR)  # may be empty if you rely entirely on persisted store\n",
    "    corpus = build_corpus_from_docs(docs) if len(docs) else Corpus(texts=[], metadatas=[], ids=[])\n",
    "\n",
    "# Build or load store for EMBED_MODEL_A (primary dense retriever)\n",
    "if VECTOR_STORE_KIND == \"chroma\":\n",
    "    dense_store = chroma_load_or_build(corpus, EMBED_MODEL_A)\n",
    "    print(\"Chroma collection size:\", dense_store.count())\n",
    "else:\n",
    "    dense_index, dense_meta = faiss_load_or_build(corpus, EMBED_MODEL_A)\n",
    "    print(\"FAISS index size:\", dense_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744428f7",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Build Sparse (BM25) Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b88ee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dense_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m all_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m BATCH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 16\u001b[0m count \u001b[38;5;241m=\u001b[39m dense_store\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, count, BATCH):\n\u001b[1;32m     18\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m BATCH, count)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dense_store' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_for_bm25(text: str) -> List[str]:\n",
    "    # Simple whitespace tokenization; you can enhance with regex or nltk\n",
    "    return text.lower().split()\n",
    "\n",
    "if VECTOR_STORE_KIND == \"chroma\":\n",
    "    # Chroma: get texts directly\n",
    "    # We may have only a persisted store with no in-memory corpus; fetch docs in batches if needed\n",
    "    # For simplicity, we assume we have `corpus.texts`. If empty, we can pull from chroma.\n",
    "    if not corpus.texts:\n",
    "        # Pull all documents back (in chunks) — beware for large corpora\n",
    "        # Here, we assume moderate size\n",
    "        all_texts = []\n",
    "        all_metas = []\n",
    "        all_ids = []\n",
    "        BATCH = 500\n",
    "        count = dense_store.count()\n",
    "        for start in range(0, count, BATCH):\n",
    "            end = min(start + BATCH, count)\n",
    "            res = dense_store.get(include=[\"documents\", \"metadatas\", \"ids\"], limit=BATCH, offset=start)\n",
    "            all_texts.extend(res.get(\"documents\", []))\n",
    "            all_metas.extend(res.get(\"metadatas\", []))\n",
    "            all_ids.extend(res.get(\"ids\", []))\n",
    "        corpus = Corpus(texts=all_texts, metadatas=all_metas, ids=all_ids)\n",
    "\n",
    "    tokenized = [tokenize_for_bm25(t) for t in corpus.texts]\n",
    "    bm25 = BM25Okapi(tokenized)\n",
    "else:\n",
    "    # FAISS path: use dense_meta as source of texts\n",
    "    tokenized = [tokenize_for_bm25(t) for t in dense_meta[\"texts\"]]\n",
    "    bm25 = BM25Okapi(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ce56c",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Retrieval Functions: Dense, BM25, and RRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dense_search(query: str, top_k: int = TOP_K_RETRIEVE, embed_model: str = EMBED_MODEL_A):\n",
    "    # Embed the query\n",
    "    qvec = np.array(embedding(model=embed_model, input=query)[\"data\"][0][\"embedding\"], dtype=np.float32)\n",
    "    if NORMALIZE_EMBEDDINGS:\n",
    "        qvec = qvec / (np.linalg.norm(qvec) + 1e-12)\n",
    "\n",
    "    if VECTOR_STORE_KIND == \"chroma\":\n",
    "        # Use chroma similarity search with provided query embedding\n",
    "        res = dense_store.query(\n",
    "            query_embeddings=[qvec.tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\", \"ids\"]\n",
    "        )\n",
    "        hits = []\n",
    "        for i in range(len(res[\"ids\"][0])):\n",
    "            hits.append({\n",
    "                \"id\": res[\"ids\"][0][i],\n",
    "                \"text\": res[\"documents\"][0][i],\n",
    "                \"metadata\": res[\"metadatas\"][0][i],\n",
    "                \"score\": 1 - res[\"distances\"][0][i]  # convert distance to similarity-ish\n",
    "            })\n",
    "        return hits\n",
    "    else:\n",
    "        # FAISS\n",
    "        D, I = dense_index.search(np.expand_dims(qvec, axis=0), top_k)\n",
    "        hits = []\n",
    "        for rank, (dist, idx) in enumerate(zip(D[0], I[0])):\n",
    "            if idx == -1: \n",
    "                continue\n",
    "            hits.append({\n",
    "                \"id\": dense_meta[\"ids\"][idx],\n",
    "                \"text\": dense_meta[\"texts\"][idx],\n",
    "                \"metadata\": dense_meta[\"metadatas\"][idx],\n",
    "                \"score\": float(dist)  # inner product if normalized ~ cosine\n",
    "            })\n",
    "        return hits\n",
    "\n",
    "def bm25_search(query: str, top_k: int = TOP_K_RETRIEVE):\n",
    "    tokenized_query = tokenize_for_bm25(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    hits = []\n",
    "    for idx in top_idx:\n",
    "        if VECTOR_STORE_KIND == \"chroma\":\n",
    "            hits.append({\n",
    "                \"id\": corpus.ids[idx],\n",
    "                \"text\": corpus.texts[idx],\n",
    "                \"metadata\": corpus.metadatas[idx],\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "        else:\n",
    "            hits.append({\n",
    "                \"id\": dense_meta[\"ids\"][idx],\n",
    "                \"text\": dense_meta[\"texts\"][idx],\n",
    "                \"metadata\": dense_meta[\"metadatas\"][idx],\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "    return hits\n",
    "\n",
    "def rrf_merge(bm25_hits: List[Dict[str, Any]], dense_hits: List[Dict[str, Any]], k: int = RRF_K, top_k: int = TOP_K_RETRIEVE):\n",
    "    # Build rank maps\n",
    "    bm25_rank = {h[\"id\"]: r for r, h in enumerate(bm25_hits, start=1)}\n",
    "    dense_rank = {h[\"id\"]: r for r, h in enumerate(dense_hits, start=1)}\n",
    "\n",
    "    # Doc universe\n",
    "    doc_ids = set(bm25_rank.keys()) | set(dense_rank.keys())\n",
    "    fused = []\n",
    "    for did in doc_ids:\n",
    "        score = 0.0\n",
    "        if did in bm25_rank:\n",
    "            score += 1.0 / (k + bm25_rank[did])\n",
    "        if did in dense_rank:\n",
    "            score += 1.0 / (k + dense_rank[did])\n",
    "        fused.append((did, score))\n",
    "\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Build full records using dense/bm25 lookup (prefer dense text/metadata if available)\n",
    "    by_id = {}\n",
    "    for h in dense_hits + bm25_hits:\n",
    "        by_id[h[\"id\"]] = h\n",
    "    results = []\n",
    "    for did, fscore in fused[:top_k]:\n",
    "        rec = by_id[did].copy()\n",
    "        rec[\"rrf_score\"] = fscore\n",
    "        results.append(rec)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af70228b",
   "metadata": {},
   "source": [
    "\n",
    "## 9. RAG: Prompting & Answering\n",
    "\n",
    "We implement two pipelines:\n",
    "- **Baseline RAG (Dense-only)**\n",
    "- **Hybrid RAG (RRF over Dense + BM25)**\n",
    "\n",
    "Both produce answers with **citations**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e44c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a careful assistant. Answer using only the provided context. \"\n",
    "    \"If the answer is not in the context, say you don't know.\"\n",
    ")\n",
    "\n",
    "def build_context(docs: List[Dict[str, Any]]) -> str:\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.get(\"metadata\", {}).get(\"source\", \"unknown\")\n",
    "        lines.append(f\"[{i}] Source: {os.path.basename(src)}\\n{d['text'][:1000]}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "def cite_sources(docs: List[Dict[str, Any]]) -> str:\n",
    "    cites = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.get(\"metadata\", {}).get(\"source\", \"unknown\")\n",
    "        cites.append(f\"[{i}] {os.path.basename(src)}\")\n",
    "    return \"Citations: \" + \"; \".join(cites)\n",
    "\n",
    "def rag_answer(query: str, retriever: str = \"dense\") -> Dict[str, Any]:\n",
    "    if retriever == \"dense\":\n",
    "        hits = dense_search(query, top_k=TOP_K_RETRIEVE)\n",
    "    elif retriever == \"bm25\":\n",
    "        hits = bm25_search(query, top_k=TOP_K_RETRIEVE)\n",
    "    elif retriever == \"hybrid\":\n",
    "        hits = rrf_merge(bm25_search(query, TOP_K_RETRIEVE), dense_search(query, TOP_K_RETRIEVE))\n",
    "    else:\n",
    "        raise ValueError(\"retriever must be one of: dense | bm25 | hybrid\")\n",
    "\n",
    "    context = build_context(hits)\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer clearly and cite sources by index.\"\n",
    "\n",
    "    resp = completion(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    answer_text = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retriever\": retriever,\n",
    "        \"hits\": hits,\n",
    "        \"answer\": answer_text,\n",
    "        \"citations\": cite_sources(hits)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff9261",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Evaluation Queries (10)\n",
    "\n",
    "Edit these to match your domain. You can also load from a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option A: inline list of queries\n",
    "queries = [\n",
    "    \"What are the warranty terms for the product?\",\n",
    "    \"How do I reset the device to factory settings?\",\n",
    "    \"What are the installation prerequisites?\",\n",
    "    \"How often should I schedule maintenance?\",\n",
    "    \"What is the recommended operating temperature range?\",\n",
    "    \"How can I export data to CSV?\",\n",
    "    \"What are the safety precautions before setup?\",\n",
    "    \"How do I enable debug logging?\",\n",
    "    \"What is the process to request technical support?\",\n",
    "    \"How do I upgrade to the latest firmware?\"\n",
    "]\n",
    "\n",
    "# Option B: load from a CSV with a 'query' column\n",
    "# import pandas as pd\n",
    "# df_q = pd.read_csv(\"./data/queries_day2_ex2.csv\")\n",
    "# queries = df_q[\"query\"].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b784b3e",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Compare Top-5 Retrieved Documents (BM25 vs Dense vs Hybrid)\n",
    "\n",
    "We will collect the top-5 for each method and present side-by-side tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc731177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_hits(hits: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for rank, h in enumerate(hits, start=1):\n",
    "        rows.append({\n",
    "            \"rank\": rank,\n",
    "            \"id\": h[\"id\"],\n",
    "            \"score\": round(h.get(\"score\", 0.0), 4),\n",
    "            \"rrf_score\": round(h.get(\"rrf_score\", 0.0), 4) if \"rrf_score\" in h else None,\n",
    "            \"source\": os.path.basename(h.get(\"metadata\", {}).get(\"source\", \"unknown\")),\n",
    "            \"snippet\": (h[\"text\"][:200] + \"...\") if len(h[\"text\"]) > 200 else h[\"text\"]\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "all_comparisons = {}\n",
    "for q in queries:\n",
    "    bm25_hits = bm25_search(q, TOP_K_RETRIEVE)\n",
    "    dense_hits = dense_search(q, TOP_K_RETRIEVE, EMBED_MODEL_A)\n",
    "    hybrid_hits = rrf_merge(bm25_hits, dense_hits, RRF_K, TOP_K_RETRIEVE)\n",
    "\n",
    "    all_comparisons[q] = {\n",
    "        \"bm25\": summarize_hits(bm25_hits),\n",
    "        \"dense\": summarize_hits(dense_hits),\n",
    "        \"hybrid\": summarize_hits(hybrid_hits)\n",
    "    }\n",
    "\n",
    "# Show a sample query comparison\n",
    "sample_q = queries[0]\n",
    "print(\"Sample Query:\", sample_q)\n",
    "display(all_comparisons[sample_q][\"bm25\"])\n",
    "display(all_comparisons[sample_q][\"dense\"])\n",
    "display(all_comparisons[sample_q][\"hybrid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed0d9d",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Baseline vs Hybrid RAG Answers\n",
    "\n",
    "We generate answers using:\n",
    "- **Baseline RAG** (dense-only)\n",
    "- **Hybrid RAG** (RRF merged)\n",
    "\n",
    "and collect their outputs with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a10757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answers = []\n",
    "for q in tqdm(queries, desc=\"Answering queries\"):\n",
    "    baseline = rag_answer(q, retriever=\"dense\")\n",
    "    hybrid = rag_answer(q, retriever=\"hybrid\")\n",
    "    answers.append({\n",
    "        \"query\": q,\n",
    "        \"baseline_answer\": baseline[\"answer\"],\n",
    "        \"baseline_citations\": baseline[\"citations\"],\n",
    "        \"hybrid_answer\": hybrid[\"answer\"],\n",
    "        \"hybrid_citations\": hybrid[\"citations\"]\n",
    "    })\n",
    "\n",
    "df_answers = pd.DataFrame(answers)\n",
    "df_answers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21067a92",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Light-weight Retrieval Check (Optional)\n",
    "\n",
    "If you have a ground-truth relevance file (CSV) with columns: `query, doc_id` (multiple rows per query), \n",
    "we can compute a simple **Precision@5** for each method. If no ground truth is present, this section will skip gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a02f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relevance_path = \"./data/relevance.csv\"\n",
    "if os.path.exists(relevance_path):\n",
    "    rel_df = pd.read_csv(relevance_path)\n",
    "    rel_map = {}\n",
    "    for q, grp in rel_df.groupby(\"query\"):\n",
    "        rel_map[q] = set(grp[\"doc_id\"].astype(str).tolist())\n",
    "\n",
    "    rows = []\n",
    "    for q in queries:\n",
    "        bm25_ids = all_comparisons[q][\"bm25\"][\"id\"].astype(str).tolist()\n",
    "        dense_ids = all_comparisons[q][\"dense\"][\"id\"].astype(str).tolist()\n",
    "        hybrid_ids = all_comparisons[q][\"hybrid\"][\"id\"].astype(str).tolist()\n",
    "        gt = rel_map.get(q, set())\n",
    "\n",
    "        def p_at_5(ids):\n",
    "            return round(sum(1 for x in ids if x in gt) / max(1, len(ids)), 3)\n",
    "\n",
    "        rows.append({\n",
    "            \"query\": q,\n",
    "            \"bm25_p@5\": p_at_5(bm25_ids),\n",
    "            \"dense_p@5\": p_at_5(dense_ids),\n",
    "            \"hybrid_p@5\": p_at_5(hybrid_ids),\n",
    "        })\n",
    "    df_metrics = pd.DataFrame(rows)\n",
    "    display(df_metrics)\n",
    "else:\n",
    "    print(\"No ground-truth relevance file found at\", relevance_path, \"- skipping metric computation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7aaa9",
   "metadata": {},
   "source": [
    "\n",
    "## 14. (Optional) FIL/Perplexity Web Debug Panel\n",
    "\n",
    "Use this section to ping a FIL/Perplexity-style endpoint for web results when you need external confirmation.\n",
    "Toggle via `USE_FIL_PERPLEXITY=True` env var and set `FIL_HOST`, `FIL_ACCESS_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def perplexity_web_search(prompt: str) -> Dict[str, Any]:\n",
    "    assert FIL_HOST and FIL_ACCESS_TOKEN, \"Set FIL_HOST and FIL_ACCESS_TOKEN for FIL/Perplexity access.\"\n",
    "    url = f\"{FIL_HOST}/api/filcopilot/chat/perplexity/v1/\"\n",
    "    headers = {\"Authorization\": f\"Bearer {FIL_ACCESS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Be precise and concise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 800,\n",
    "        \"temperature\": 0.2,\n",
    "        \"return_images\": False,\n",
    "        \"search_after_date_filter\": None,\n",
    "        \"search_before_date_filter\": None,\n",
    "        \"search_domain_filter\": [],\n",
    "        \"search_context_size\": \"low\",\n",
    "        \"top_p\": 0.9,\n",
    "        \"search_mode\": \"web\",\n",
    "        \"return_related_questions\": False,\n",
    "        \"top_k\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=data, verify=False, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "if USE_FIL_PERPLEXITY:\n",
    "    try:\n",
    "        dbg = perplexity_web_search(\"Quick test query for Day 2 Ex2 debug panel\")\n",
    "        print(\"FIL/Perplexity result keys:\", list(dbg.keys())[:10])\n",
    "    except Exception as e:\n",
    "        print(\"Perplexity debug call failed:\", e)\n",
    "else:\n",
    "    print(\"FIL/Perplexity panel disabled. Set USE_FIL_PERPLEXITY=True to enable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1396df5",
   "metadata": {},
   "source": [
    "\n",
    "## 15. Results & Takeaways\n",
    "\n",
    "- **Top-5 Comparison:** Inspect the tables in Section 11 for each query. Hybrid (RRF) should balance **precision** (BM25) and **semantic recall** (Dense).\n",
    "- **Answers:** Check the **Baseline vs Hybrid** outputs in Section 12. Look for cases where hybrid surfaces a slightly off-keyword but semantically relevant chunk that improves the final answer.\n",
    "- **Metrics:** If you provided `relevance.csv`, compare **Precision@5**; hybrid typically outperforms single-method approaches.\n",
    "\n",
    "### What to tweak next\n",
    "- Try **different embedding models** via LiteLLM by changing `EMBED_MODEL_A/B`.\n",
    "- Adjust chunk size/overlap and stopwords/tokenization for BM25.\n",
    "- Experiment with `RRF_K` and `TOP_K_RETRIEVE`.\n",
    "- Add a **reranker** (cross-encoder) for Section 13+ in Day 2 Ex3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70e403",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: Inspect Full Comparison for a Specific Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_query(q: str):\n",
    "    display(all_comparisons[q][\"bm25\"])\n",
    "    display(all_comparisons[q][\"dense\"])\n",
    "    display(all_comparisons[q][\"hybrid\"])\n",
    "\n",
    "# Example:\n",
    "# compare_query(queries[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
