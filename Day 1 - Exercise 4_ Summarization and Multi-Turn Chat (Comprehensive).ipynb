{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Exercise 4: Summarization and Multi-Turn Chat (Comprehensive)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will:\n",
    "- **Understand** what LangChain is and why it's important for LLM applications\n",
    "- **Implement** text summarization using different techniques\n",
    "- **Master** conversational memory concepts and implementations\n",
    "- **Build** multi-turn chat applications with persistent memory\n",
    "- **Evaluate** summary quality and conversational coherence\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **LangChain Fundamentals** - Framework for building LLM applications\n",
    "2. **Text Summarization** - Extractive vs Abstractive approaches\n",
    "3. **Conversational Memory** - Buffer, Summary, and Token-based memory\n",
    "4. **Python Integration** - Loading mechanisms and error handling\n",
    "5. **Real-world Applications** - Document processing and chat systems\n",
    "\n",
    "**Estimated Time:** 90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Installation\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "**LangChain** is a powerful framework for developing applications powered by language models. It provides:\n",
    "\n",
    "- **Modular Components**: Pre-built tools for common LLM tasks\n",
    "- **Memory Management**: Sophisticated conversation memory systems\n",
    "- **Chain Composition**: Ability to link multiple LLM operations\n",
    "- **Integration Support**: Easy connection to various data sources and APIs\n",
    "\n",
    "### Why Use LangChain?\n",
    "\n",
    "1. **Simplified Development**: Reduces boilerplate code\n",
    "2. **Memory Management**: Built-in conversation memory\n",
    "3. **Scalability**: Production-ready components\n",
    "4. **Flexibility**: Modular architecture for customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install litellm langchain-core langchain-community langchain-openai python-dotenv\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "### Understanding Python Environment Variables\n",
    "\n",
    "Environment variables are a secure way to store sensitive information like API keys:\n",
    "\n",
    "- **Security**: Keeps secrets out of your code\n",
    "- **Flexibility**: Easy to change without code modifications\n",
    "- **Best Practice**: Industry standard for configuration management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set up your OpenAI API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key directly (for this exercise)\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "# Verify API key is set\n",
    "if os.environ.get('OPENAI_API_KEY'):\n",
    "    print(\"‚úÖ OpenAI API key configured successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå OpenAI API key not found. Please set it above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Libraries\n",
    "\n",
    "### Understanding Python Imports\n",
    "\n",
    "Let's understand what each import does:\n",
    "\n",
    "- **`litellm`**: Unified interface for multiple LLM providers\n",
    "- **`langchain_openai`**: OpenAI integration for LangChain\n",
    "- **`langchain.memory`**: Conversation memory management\n",
    "- **`langchain.schema`**: Core data structures\n",
    "- **`langchain.chains`**: Pre-built processing chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Current time: 2025-09-18 12:56:44\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LiteLLM for unified LLM interface\n",
    "import litellm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Additional utilities\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† LangChain Deep Dive\n",
    "\n",
    "### What is LangChain Really?\n",
    "\n",
    "LangChain is like a **Swiss Army knife for LLM applications**. It provides:\n",
    "\n",
    "#### 1. **Components** üß©\n",
    "- **LLMs**: Wrappers for different language models\n",
    "- **Prompts**: Templates for consistent prompt formatting\n",
    "- **Memory**: Systems to remember conversation history\n",
    "- **Chains**: Sequences of operations\n",
    "\n",
    "#### 2. **Memory Types** üß†\n",
    "- **Buffer Memory**: Stores all messages (simple but memory-intensive)\n",
    "- **Summary Memory**: Summarizes old conversations (memory-efficient)\n",
    "- **Token Buffer**: Limits memory by token count\n",
    "- **Entity Memory**: Remembers specific entities mentioned\n",
    "\n",
    "#### 3. **Why It Matters** üí°\n",
    "- **Consistency**: Standardized patterns for LLM apps\n",
    "- **Scalability**: Production-ready components\n",
    "- **Maintainability**: Modular, testable code\n",
    "- **Community**: Large ecosystem of extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize the Language Model\n",
    "\n",
    "### Understanding LLM Initialization\n",
    "\n",
    "When we initialize an LLM, we're setting up:\n",
    "- **Model Selection**: Which specific model to use\n",
    "- **Parameters**: Temperature, max tokens, etc.\n",
    "- **Connection**: How to communicate with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM connection successful!\n",
      "üìù Test response: Hello! Yes, I'm here and ready to help. How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Cost-effective model for learning\n",
    "    temperature=0.7,       # Balanced creativity vs consistency\n",
    "    max_tokens=1000,       # Reasonable response length\n",
    "    verbose=True           # Show what's happening\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    test_response = llm.invoke([HumanMessage(content=\"Hello! Can you confirm you're working?\")])\n",
    "    print(\"‚úÖ LLM connection successful!\")\n",
    "    print(f\"üìù Test response: {test_response.content[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Part 1: Text Summarization\n",
    "\n",
    "### Understanding Text Summarization\n",
    "\n",
    "Text summarization is the process of creating a shorter version of a text while preserving its key information.\n",
    "\n",
    "#### Types of Summarization:\n",
    "\n",
    "1. **Extractive Summarization** üìã\n",
    "   - Selects important sentences from the original text\n",
    "   - Preserves original wording\n",
    "   - Example: Highlighting key sentences\n",
    "\n",
    "2. **Abstractive Summarization** ‚úçÔ∏è\n",
    "   - Generates new sentences that capture the meaning\n",
    "   - More human-like and flexible\n",
    "   - Example: Paraphrasing the main ideas\n",
    "\n",
    "#### Why Summarization Matters:\n",
    "- **Information Overload**: Too much content to process manually\n",
    "- **Time Efficiency**: Quick understanding of long documents\n",
    "- **Decision Making**: Extract key points for faster decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Text: The Future of Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original text length: 2528 characters\n",
      "üìä Word count: 355 words\n",
      "\n",
      "üìÑ Sample text loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Sample long text for summarization\n",
    "sample_text = \"\"\"\n",
    "Artificial Intelligence (AI) has emerged as one of the most transformative technologies of the 21st century, \n",
    "fundamentally reshaping how we work, communicate, and solve complex problems. From its humble beginnings in \n",
    "the 1950s with simple rule-based systems, AI has evolved into sophisticated machine learning algorithms capable \n",
    "of processing vast amounts of data and making predictions with remarkable accuracy.\n",
    "\n",
    "The current landscape of AI is dominated by deep learning, a subset of machine learning that uses neural networks \n",
    "with multiple layers to model and understand complex patterns in data. This approach has led to breakthrough \n",
    "achievements in computer vision, natural language processing, and speech recognition. Companies like Google, \n",
    "Microsoft, and OpenAI have developed large language models that can generate human-like text, translate languages, \n",
    "and even write code.\n",
    "\n",
    "However, the rapid advancement of AI also brings significant challenges and ethical considerations. Issues such as \n",
    "algorithmic bias, job displacement, privacy concerns, and the potential for misuse of AI technologies have sparked \n",
    "intense debates among researchers, policymakers, and the general public. The development of AI governance frameworks \n",
    "and ethical guidelines has become crucial to ensure that AI benefits society while minimizing potential harms.\n",
    "\n",
    "Looking ahead, the future of AI promises even more revolutionary changes. Emerging areas such as artificial general \n",
    "intelligence (AGI), quantum computing integration, and brain-computer interfaces could unlock capabilities that \n",
    "seem almost magical today. The integration of AI into healthcare could lead to personalized medicine and early \n",
    "disease detection, while AI-powered climate models could help us better understand and combat climate change.\n",
    "\n",
    "Education is another domain where AI is expected to have a profound impact. Personalized learning systems could \n",
    "adapt to individual student needs, providing customized educational experiences that maximize learning outcomes. \n",
    "AI tutors could provide 24/7 support, making quality education more accessible to people around the world.\n",
    "\n",
    "The economic implications of AI are equally significant. While AI may automate certain jobs, it is also expected \n",
    "to create new types of employment and increase productivity across various sectors. The key challenge will be \n",
    "ensuring that the benefits of AI are distributed equitably and that workers are prepared for the changing job market \n",
    "through reskilling and upskilling programs.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìä Original text length: {len(sample_text)} characters\")\n",
    "print(f\"üìä Word count: {len(sample_text.split())} words\")\n",
    "print(\"\\nüìÑ Sample text loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Summarization\n",
    "\n",
    "Let's create a simple summarization function using our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing different summarization approaches...\n",
      "\n",
      "üìù BRIEF SUMMARY (100 words):\n",
      "Artificial Intelligence (AI) is a transformative technology reshaping work, communication, and problem-solving since its inception in the 1950s. Dominated by deep learning, AI has achieved breakthroughs in areas like computer vision and language processing, with major companies developing advanced language models. However, rapid AI advancement raises ethical concerns, including algorithmic bias and job displacement, necessitating governance frameworks. Future developments like artificial general intelligence (AGI) and AI in healthcare promise revolutionary changes. While AI may automate jobs, it could also create new employment opportunities, highlighting the importance of equitable benefits and reskilling initiatives for the evolving job market.\n",
      "Word count: 97\n",
      "\n",
      "üìù DETAILED SUMMARY (200 words):\n",
      "Artificial Intelligence (AI) is transforming the 21st century by reshaping work, communication, and problem-solving. Originating in the 1950s, AI has evolved from simple rule-based systems to advanced machine learning algorithms, particularly deep learning, which utilizes neural networks for complex data analysis. Major companies like Google, Microsoft, and OpenAI have developed large language models that can produce human-like text and perform tasks such as translation and coding.\n",
      "\n",
      "However, the rapid growth of AI raises ethical and societal concerns, including algorithmic bias, job displacement, and privacy issues, necessitating the establishment of governance frameworks to ensure AI's societal benefits while mitigating risks. Future advancements may include artificial general intelligence (AGI), quantum computing, and brain-computer interfaces, potentially revolutionizing fields like healthcare and education through personalized medicine and adaptive learning systems.\n",
      "\n",
      "Economically, while AI could automate jobs, it is also expected to create new employment opportunities and enhance productivity. A critical challenge will be to ensure equitable distribution of AI's benefits and to equip the workforce with necessary skills through reskilling and upskilling initiatives, thereby addressing the changing job landscape.\n",
      "Word count: 175\n",
      "\n",
      "üìù BULLET POINTS SUMMARY (150 words):\n",
      "- AI is a transformative technology reshaping work, communication, and problem-solving since the 1950s.\n",
      "- Evolved from simple rule-based systems to advanced machine learning, particularly deep learning using neural networks.\n",
      "- Achievements include advancements in computer vision, natural language processing, and speech recognition.\n",
      "- Companies like Google, Microsoft, and OpenAI have created large language models for text generation, translation, and coding.\n",
      "- Challenges include algorithmic bias, job displacement, privacy issues, and potential misuse, prompting debates on AI governance and ethics.\n",
      "- Future innovations may include artificial general intelligence (AGI), quantum computing, and brain-computer interfaces.\n",
      "- AI could revolutionize healthcare with personalized medicine and enhance education through personalized learning systems and AI tutors.\n",
      "- Economic impact includes job automation and creation, emphasizing the need for equitable distribution of AI benefits and workforce reskilling.\n",
      "Word count: 132\n"
     ]
    }
   ],
   "source": [
    "def create_summary(text: str, summary_type: str = \"brief\", max_length: int = 150) -> str:\n",
    "    \"\"\"\n",
    "    Create a summary of the given text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        summary_type: Type of summary ('brief', 'detailed', 'bullet_points')\n",
    "        max_length: Maximum length of summary in words\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define different prompt templates for different summary types\n",
    "    prompts = {\n",
    "        \"brief\": f\"Summarize the following text in {max_length} words or less, focusing on the main points:\\n\\n{text}\",\n",
    "        \"detailed\": f\"Provide a comprehensive summary of the following text in {max_length} words or less, including key details and implications:\\n\\n{text}\",\n",
    "        \"bullet_points\": f\"Summarize the following text as bullet points (maximum {max_length} words total):\\n\\n{text}\"\n",
    "    }\n",
    "    \n",
    "    prompt = prompts.get(summary_type, prompts[\"brief\"])\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {e}\"\n",
    "\n",
    "# Test different summary types\n",
    "print(\"üîç Testing different summarization approaches...\\n\")\n",
    "\n",
    "# Brief summary\n",
    "brief_summary = create_summary(sample_text, \"brief\", 100)\n",
    "print(\"üìù BRIEF SUMMARY (100 words):\")\n",
    "print(brief_summary)\n",
    "print(f\"Word count: {len(brief_summary.split())}\\n\")\n",
    "\n",
    "# Detailed summary\n",
    "detailed_summary = create_summary(sample_text, \"detailed\", 200)\n",
    "print(\"üìù DETAILED SUMMARY (200 words):\")\n",
    "print(detailed_summary)\n",
    "print(f\"Word count: {len(detailed_summary.split())}\\n\")\n",
    "\n",
    "# Bullet points summary\n",
    "bullet_summary = create_summary(sample_text, \"bullet_points\", 150)\n",
    "print(\"üìù BULLET POINTS SUMMARY (150 words):\")\n",
    "print(bullet_summary)\n",
    "print(f\"Word count: {len(bullet_summary.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Part 2: Conversational Memory\n",
    "\n",
    "### Understanding Conversational Memory\n",
    "\n",
    "Conversational memory is crucial for building chatbots that can maintain context across multiple turns of conversation.\n",
    "\n",
    "#### Why Memory Matters:\n",
    "- **Context Continuity**: Remember what was discussed earlier\n",
    "- **Personalization**: Adapt responses based on conversation history\n",
    "- **Coherence**: Maintain logical flow in conversations\n",
    "\n",
    "#### Types of Memory in LangChain:\n",
    "\n",
    "1. **ConversationBufferMemory** üìö\n",
    "   - Stores all conversation messages\n",
    "   - Simple but can become memory-intensive\n",
    "   - Best for: Short conversations\n",
    "\n",
    "2. **ConversationSummaryMemory** üìã\n",
    "   - Summarizes older parts of the conversation\n",
    "   - Memory-efficient for long conversations\n",
    "   - Best for: Extended dialogues\n",
    "\n",
    "3. **ConversationBufferWindowMemory** ü™ü\n",
    "   - Keeps only the last N messages\n",
    "   - Fixed memory usage\n",
    "   - Best for: Conversations with recent context focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Buffer Memory Implementation\n",
    "\n",
    "Let's implement a chat system with buffer memory that remembers all previous messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_12217/1800994397.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  buffer_memory = ConversationBufferMemory(\n",
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_12217/1800994397.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  buffer_conversation = ConversationChain(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['chat_history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatOpenAI(client...tory'), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m buffer_memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[1;32m      3\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a conversation chain with buffer memory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m buffer_conversation \u001b[38;5;241m=\u001b[39m ConversationChain(\n\u001b[1;32m      9\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     10\u001b[0m     memory\u001b[38;5;241m=\u001b[39mbuffer_memory,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Buffer Memory Chat System initialized!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ This system will remember everything we discuss.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:226\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     emit_warning()\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:226\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     emit_warning()\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/load/serializable.py:115\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationChain\n  Value error, Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['chat_history'] as inputs from memory, and input as the normal input key. [type=value_error, input_value={'llm': ChatOpenAI(client...tory'), 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "# Initialize Buffer Memory\n",
    "buffer_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create a conversation chain with buffer memory\n",
    "buffer_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Buffer Memory Chat System initialized!\")\n",
    "print(\"ü§ñ This system will remember everything we discuss.\\n\")\n",
    "\n",
    "# Function to chat with buffer memory\n",
    "def chat_with_buffer_memory(message: str) -> str:\n",
    "    \"\"\"\n",
    "    Chat with the buffer memory system.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "    \n",
    "    Returns:\n",
    "        str: AI's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = buffer_conversation.predict(input=message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the buffer memory system\n",
    "print(\"üó£Ô∏è Testing Buffer Memory Chat System:\\n\")\n",
    "\n",
    "# First message\n",
    "response1 = chat_with_buffer_memory(\"Hi! My name is Alex and I'm learning about AI.\")\n",
    "print(f\"üë§ User: Hi! My name is Alex and I'm learning about AI.\")\n",
    "print(f\"ü§ñ AI: {response1}\\n\")\n",
    "\n",
    "# Second message - testing memory\n",
    "response2 = chat_with_buffer_memory(\"What's my name and what am I learning about?\")\n",
    "print(f\"üë§ User: What's my name and what am I learning about?\")\n",
    "print(f\"ü§ñ AI: {response2}\\n\")\n",
    "\n",
    "# Third message - adding more context\n",
    "response3 = chat_with_buffer_memory(\"I'm particularly interested in natural language processing. Can you explain it briefly?\")\n",
    "print(f\"üë§ User: I'm particularly interested in natural language processing. Can you explain it briefly?\")\n",
    "print(f\"ü§ñ AI: {response3}\\n\")\n",
    "\n",
    "# Check memory contents\n",
    "print(\"üß† Current Memory Contents:\")\n",
    "print(f\"Messages stored: {len(buffer_memory.chat_memory.messages)}\")\n",
    "for i, message in enumerate(buffer_memory.chat_memory.messages):\n",
    "    msg_type = \"Human\" if isinstance(message, HumanMessage) else \"AI\"\n",
    "    print(f\"{i+1}. {msg_type}: {message.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Summary Memory Implementation\n",
    "\n",
    "Now let's implement a chat system with summary memory that condenses older conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Summary Memory\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create a conversation chain with summary memory\n",
    "summary_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Summary Memory Chat System initialized!\")\n",
    "print(\"ü§ñ This system will summarize older parts of our conversation.\\n\")\n",
    "\n",
    "# Function to chat with summary memory\n",
    "def chat_with_summary_memory(message: str) -> str:\n",
    "    \"\"\"\n",
    "    Chat with the summary memory system.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "    \n",
    "    Returns:\n",
    "        str: AI's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = summary_conversation.predict(input=message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the summary memory system with a longer conversation\n",
    "print(\"üó£Ô∏è Testing Summary Memory Chat System:\\n\")\n",
    "\n",
    "# Simulate a longer conversation\n",
    "conversation_turns = [\n",
    "    \"Hello! I'm Sarah, a data scientist working on machine learning projects.\",\n",
    "    \"I'm currently working on a project involving customer churn prediction.\",\n",
    "    \"We're using features like customer age, purchase history, and engagement metrics.\",\n",
    "    \"The main challenge is dealing with imbalanced data - we have many more retained customers than churned ones.\",\n",
    "    \"What techniques would you recommend for handling this imbalanced dataset?\",\n",
    "    \"Can you remind me what we discussed about my project so far?\"\n",
    "]\n",
    "\n",
    "for i, user_message in enumerate(conversation_turns, 1):\n",
    "    print(f\"Turn {i}:\")\n",
    "    print(f\"üë§ User: {user_message}\")\n",
    "    \n",
    "    ai_response = chat_with_summary_memory(user_message)\n",
    "    print(f\"ü§ñ AI: {ai_response}\\n\")\n",
    "    \n",
    "    # Show memory state after a few turns\n",
    "    if i == 3 or i == 6:\n",
    "        print(f\"üß† Memory State after Turn {i}:\")\n",
    "        print(f\"Summary: {summary_memory.moving_summary_buffer}\")\n",
    "        print(f\"Recent messages: {len(summary_memory.chat_memory.messages)}\\n\")\n",
    "    \n",
    "    # Add a small delay to make it more readable\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Memory Type Comparison\n",
    "\n",
    "Let's compare how different memory types handle the same conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_memory_types():\n",
    "    \"\"\"\n",
    "    Compare different memory types with the same conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test conversation\n",
    "    test_messages = [\n",
    "        \"I'm planning a trip to Japan next month.\",\n",
    "        \"I'm interested in visiting Tokyo and Kyoto.\",\n",
    "        \"I love traditional Japanese culture and modern technology.\",\n",
    "        \"What should I pack for the weather in March?\",\n",
    "        \"Also, can you remind me what cities I mentioned wanting to visit?\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize different memory types\n",
    "    memories = {\n",
    "        \"Buffer\": ConversationBufferMemory(memory_key=\"history\", return_messages=True),\n",
    "        \"Summary\": ConversationSummaryMemory(llm=llm, memory_key=\"history\", return_messages=True)\n",
    "    }\n",
    "    \n",
    "    # Create conversations for each memory type\n",
    "    conversations = {}\n",
    "    for name, memory in memories.items():\n",
    "        conversations[name] = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "    \n",
    "    print(\"üîç Comparing Memory Types with the Same Conversation:\\n\")\n",
    "    \n",
    "    # Run the same conversation through both memory types\n",
    "    for i, message in enumerate(test_messages, 1):\n",
    "        print(f\"üìù Turn {i}: {message}\\n\")\n",
    "        \n",
    "        for memory_type, conversation in conversations.items():\n",
    "            try:\n",
    "                response = conversation.predict(input=message)\n",
    "                print(f\"ü§ñ {memory_type} Memory Response:\")\n",
    "                print(f\"{response[:200]}...\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {memory_type} Memory Error: {e}\\n\")\n",
    "        \n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Show final memory states\n",
    "    print(\"üß† Final Memory States:\\n\")\n",
    "    \n",
    "    for memory_type, memory in memories.items():\n",
    "        print(f\"üìä {memory_type} Memory:\")\n",
    "        if hasattr(memory, 'moving_summary_buffer'):\n",
    "            print(f\"Summary: {memory.moving_summary_buffer}\")\n",
    "        print(f\"Messages stored: {len(memory.chat_memory.messages)}\")\n",
    "        print()\n",
    "\n",
    "# Run the comparison\n",
    "compare_memory_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Part 3: Advanced Features\n",
    "\n",
    "### Custom Memory Management\n",
    "\n",
    "Let's create a custom memory system that combines the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridMemory:\n",
    "    \"\"\"\n",
    "    A hybrid memory system that uses buffer memory for recent messages\n",
    "    and summary memory for older conversations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, buffer_size: int = 6):\n",
    "        self.llm = llm\n",
    "        self.buffer_size = buffer_size\n",
    "        self.recent_messages = []\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def add_message(self, message: str, is_human: bool = True):\n",
    "        \"\"\"\n",
    "        Add a message to memory.\n",
    "        \n",
    "        Args:\n",
    "            message: The message content\n",
    "            is_human: Whether the message is from human (True) or AI (False)\n",
    "        \"\"\"\n",
    "        msg_type = \"Human\" if is_human else \"AI\"\n",
    "        self.recent_messages.append(f\"{msg_type}: {message}\")\n",
    "        \n",
    "        # If we exceed buffer size, summarize older messages\n",
    "        if len(self.recent_messages) > self.buffer_size:\n",
    "            # Take the oldest messages to summarize\n",
    "            messages_to_summarize = self.recent_messages[:2]\n",
    "            self.recent_messages = self.recent_messages[2:]\n",
    "            \n",
    "            # Create summary\n",
    "            conversation_text = \"\\n\".join(messages_to_summarize)\n",
    "            summary_prompt = f\"Summarize this conversation briefly:\\n{conversation_text}\"\n",
    "            \n",
    "            try:\n",
    "                summary_response = self.llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "                new_summary = summary_response.content\n",
    "                \n",
    "                # Combine with existing summary\n",
    "                if self.summary:\n",
    "                    self.summary = f\"{self.summary} {new_summary}\"\n",
    "                else:\n",
    "                    self.summary = new_summary\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating summary: {e}\")\n",
    "    \n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the full conversation context.\n",
    "        \n",
    "        Returns:\n",
    "            str: The conversation context\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        if self.summary:\n",
    "            context_parts.append(f\"Previous conversation summary: {self.summary}\")\n",
    "        \n",
    "        if self.recent_messages:\n",
    "            context_parts.append(\"Recent conversation:\")\n",
    "            context_parts.extend(self.recent_messages)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get memory statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Memory statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"recent_messages_count\": len(self.recent_messages),\n",
    "            \"has_summary\": bool(self.summary),\n",
    "            \"summary_length\": len(self.summary) if self.summary else 0,\n",
    "            \"buffer_size\": self.buffer_size\n",
    "        }\n",
    "\n",
    "# Test the hybrid memory system\n",
    "print(\"üß† Testing Hybrid Memory System:\\n\")\n",
    "\n",
    "hybrid_memory = HybridMemory(llm, buffer_size=4)\n",
    "\n",
    "# Simulate a long conversation\n",
    "test_conversation = [\n",
    "    (\"Hello! I'm a software engineer working on a new mobile app.\", True),\n",
    "    (\"That sounds exciting! What kind of mobile app are you developing?\", False),\n",
    "    (\"It's a fitness tracking app with social features.\", True),\n",
    "    (\"Great! Social features can really help with user engagement. What specific social features are you planning?\", False),\n",
    "    (\"We want to include friend challenges, leaderboards, and workout sharing.\", True),\n",
    "    (\"Those are excellent features for building a community around fitness. Are you using any specific frameworks?\", False),\n",
    "    (\"We're using React Native for cross-platform development.\", True),\n",
    "    (\"React Native is a solid choice. How's the development process going so far?\", False),\n",
    "    (\"Pretty well, but we're facing some performance issues with the real-time features.\", True),\n",
    "    (\"Performance optimization is crucial for real-time features. Have you considered using WebSockets or similar technologies?\", False),\n",
    "    (\"Can you remind me what we discussed about my app project?\", True)\n",
    "]\n",
    "\n",
    "for i, (message, is_human) in enumerate(test_conversation, 1):\n",
    "    hybrid_memory.add_message(message, is_human)\n",
    "    \n",
    "    print(f\"Turn {i}: {'üë§ Human' if is_human else 'ü§ñ AI'}: {message}\")\n",
    "    \n",
    "    # Show memory stats every few turns\n",
    "    if i % 4 == 0:\n",
    "        stats = hybrid_memory.get_stats()\n",
    "        print(f\"\\nüìä Memory Stats after Turn {i}:\")\n",
    "        print(f\"Recent messages: {stats['recent_messages_count']}\")\n",
    "        print(f\"Has summary: {stats['has_summary']}\")\n",
    "        if stats['has_summary']:\n",
    "            print(f\"Summary length: {stats['summary_length']} characters\")\n",
    "        print()\n",
    "\n",
    "# Show final context\n",
    "print(\"\\nüß† Final Memory Context:\")\n",
    "print(hybrid_memory.get_context())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 4: Evaluation and Assessment\n",
    "\n",
    "### Summary Quality Evaluation\n",
    "\n",
    "Let's evaluate the quality of our summarization and memory systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary_quality(original_text: str, summary: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a summary.\n",
    "    \n",
    "    Args:\n",
    "        original_text: The original text\n",
    "        summary: The generated summary\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    original_words = len(original_text.split())\n",
    "    summary_words = len(summary.split())\n",
    "    compression_ratio = summary_words / original_words\n",
    "    \n",
    "    # Use LLM to evaluate quality\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Please evaluate this summary on a scale of 1-10 for each criterion:\n",
    "    \n",
    "    Original Text:\n",
    "    {original_text[:500]}...\n",
    "    \n",
    "    Summary:\n",
    "    {summary}\n",
    "    \n",
    "    Rate the summary (1-10) for:\n",
    "    1. Accuracy: How well does it capture the main points?\n",
    "    2. Completeness: Are all important points included?\n",
    "    3. Conciseness: Is it appropriately brief?\n",
    "    4. Clarity: Is it easy to understand?\n",
    "    \n",
    "    Respond in this format:\n",
    "    Accuracy: X/10\n",
    "    Completeness: X/10\n",
    "    Conciseness: X/10\n",
    "    Clarity: X/10\n",
    "    Overall: X/10\n",
    "    Comments: [brief explanation]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        evaluation_response = llm.invoke([HumanMessage(content=evaluation_prompt)])\n",
    "        llm_evaluation = evaluation_response.content\n",
    "    except Exception as e:\n",
    "        llm_evaluation = f\"Error in LLM evaluation: {e}\"\n",
    "    \n",
    "    return {\n",
    "        \"original_words\": original_words,\n",
    "        \"summary_words\": summary_words,\n",
    "        \"compression_ratio\": round(compression_ratio, 3),\n",
    "        \"compression_percentage\": round((1 - compression_ratio) * 100, 1),\n",
    "        \"llm_evaluation\": llm_evaluation\n",
    "    }\n",
    "\n",
    "# Evaluate our earlier summaries\n",
    "print(\"üìä Evaluating Summary Quality:\\n\")\n",
    "\n",
    "# Evaluate brief summary\n",
    "brief_eval = evaluate_summary_quality(sample_text, brief_summary)\n",
    "print(\"üìù BRIEF SUMMARY EVALUATION:\")\n",
    "print(f\"Original words: {brief_eval['original_words']}\")\n",
    "print(f\"Summary words: {brief_eval['summary_words']}\")\n",
    "print(f\"Compression: {brief_eval['compression_percentage']}% reduction\")\n",
    "print(f\"\\nLLM Evaluation:\\n{brief_eval['llm_evaluation']}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Evaluate detailed summary\n",
    "detailed_eval = evaluate_summary_quality(sample_text, detailed_summary)\n",
    "print(\"üìù DETAILED SUMMARY EVALUATION:\")\n",
    "print(f\"Original words: {detailed_eval['original_words']}\")\n",
    "print(f\"Summary words: {detailed_eval['summary_words']}\")\n",
    "print(f\"Compression: {detailed_eval['compression_percentage']}% reduction\")\n",
    "print(f\"\\nLLM Evaluation:\\n{detailed_eval['llm_evaluation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "\n",
    "### Exercise 5: Build Your Own Chat Application\n",
    "\n",
    "Now it's your turn! Create a chat application that combines summarization and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartChatBot:\n",
    "    \"\"\"\n",
    "    A smart chatbot that combines summarization and memory management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, name: str = \"Assistant\"):\n",
    "        self.llm = llm\n",
    "        self.name = name\n",
    "        self.memory = HybridMemory(llm, buffer_size=6)\n",
    "        self.conversation_count = 0\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a user message and return a response.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's message\n",
    "        \n",
    "        Returns:\n",
    "            str: The bot's response\n",
    "        \"\"\"\n",
    "        self.conversation_count += 1\n",
    "        \n",
    "        # Add user message to memory\n",
    "        self.memory.add_message(user_message, is_human=True)\n",
    "        \n",
    "        # Get conversation context\n",
    "        context = self.memory.get_context()\n",
    "        \n",
    "        # Create prompt with context\n",
    "        prompt = f\"\"\"\n",
    "        You are {self.name}, a helpful AI assistant. Use the conversation context to provide relevant responses.\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Current user message: {user_message}\n",
    "        \n",
    "        Respond naturally and helpfully, referencing previous conversation when relevant.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            ai_response = response.content\n",
    "            \n",
    "            # Add AI response to memory\n",
    "            self.memory.add_message(ai_response, is_human=False)\n",
    "            \n",
    "            return ai_response\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {e}\"\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get chatbot statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Chatbot statistics\n",
    "        \"\"\"\n",
    "        memory_stats = self.memory.get_stats()\n",
    "        return {\n",
    "            \"conversation_turns\": self.conversation_count,\n",
    "            \"memory_stats\": memory_stats\n",
    "        }\n",
    "    \n",
    "    def summarize_conversation(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a summary of the entire conversation.\n",
    "        \n",
    "        Returns:\n",
    "            str: Conversation summary\n",
    "        \"\"\"\n",
    "        context = self.memory.get_context()\n",
    "        \n",
    "        summary_prompt = f\"\"\"\n",
    "        Please provide a comprehensive summary of this conversation:\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Include:\n",
    "        - Main topics discussed\n",
    "        - Key information shared\n",
    "        - Any decisions or conclusions reached\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating summary: {e}\"\n",
    "\n",
    "# Create and test the smart chatbot\n",
    "print(\"ü§ñ Creating Smart ChatBot with Hybrid Memory...\\n\")\n",
    "\n",
    "smart_bot = SmartChatBot(llm, \"Alex\")\n",
    "\n",
    "# Test conversation\n",
    "test_messages = [\n",
    "    \"Hi Alex! I'm working on a machine learning project for my university.\",\n",
    "    \"It's about predicting house prices using various features like location, size, and age.\",\n",
    "    \"I'm using Python with scikit-learn for the implementation.\",\n",
    "    \"The main challenge is feature engineering - I'm not sure which features are most important.\",\n",
    "    \"I've tried linear regression but the results aren't great. What other algorithms should I try?\",\n",
    "    \"Can you remind me what my project is about and what challenges I mentioned?\"\n",
    "]\n",
    "\n",
    "print(\"üó£Ô∏è Testing Smart ChatBot:\\n\")\n",
    "\n",
    "for i, message in enumerate(test_messages, 1):\n",
    "    print(f\"Turn {i}:\")\n",
    "    print(f\"üë§ User: {message}\")\n",
    "    \n",
    "    response = smart_bot.chat(message)\n",
    "    print(f\"ü§ñ Alex: {response}\\n\")\n",
    "    \n",
    "    # Show stats every few turns\n",
    "    if i % 3 == 0:\n",
    "        stats = smart_bot.get_stats()\n",
    "        print(f\"üìä Bot Stats after Turn {i}:\")\n",
    "        print(f\"Conversation turns: {stats['conversation_turns']}\")\n",
    "        print(f\"Memory: {stats['memory_stats']}\\n\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Get final conversation summary\n",
    "print(\"üìã Final Conversation Summary:\")\n",
    "final_summary = smart_bot.summarize_conversation()\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion and Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "Congratulations! You've successfully completed Day 1 - Exercise 4. Here's what you've mastered:\n",
    "\n",
    "#### üß† **LangChain Fundamentals**\n",
    "- Understanding of LangChain's modular architecture\n",
    "- Knowledge of different memory types and their use cases\n",
    "- Practical experience with conversation chains\n",
    "\n",
    "#### üìÑ **Text Summarization**\n",
    "- Implementation of different summarization approaches\n",
    "- Understanding of extractive vs abstractive summarization\n",
    "- Quality evaluation techniques for summaries\n",
    "\n",
    "#### üí¨ **Conversational Memory**\n",
    "- Buffer memory for complete conversation history\n",
    "- Summary memory for memory-efficient long conversations\n",
    "- Custom hybrid memory combining both approaches\n",
    "\n",
    "#### üîß **Python Integration**\n",
    "- Environment variable management for API keys\n",
    "- Error handling and robust application design\n",
    "- Object-oriented programming for chatbot development\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "\n",
    "1. **Experiment** with different memory configurations\n",
    "2. **Try** different LLM models and compare results\n",
    "3. **Build** your own chatbot with custom features\n",
    "4. **Explore** LangChain's other components (agents, tools, etc.)\n",
    "\n",
    "### üìö **Additional Resources**\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Memory Management Best Practices](https://python.langchain.com/docs/modules/memory/)\n",
    "\n",
    "### üéØ **Assessment Questions**\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **When would you use ConversationBufferMemory vs ConversationSummaryMemory?**\n",
    "2. **What are the trade-offs between different summarization approaches?**\n",
    "3. **How would you handle very long conversations that exceed token limits?**\n",
    "4. **What metrics would you use to evaluate conversation quality?**\n",
    "\n",
    "**Great job completing this exercise! You're now ready for more advanced LangChain concepts.** üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
