{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9dab8a4",
   "metadata": {},
   "source": [
    "# Day 2 — Exercise 5: Agent Planning and Tool Integration (Plan–Act–Observe)\n",
    "\n",
    "## Background & Plan\n",
    "**Why this matters.** Real-world tasks rarely fit in one prompt. Agents follow a **Plan → Act → Observe** loop to break work into steps and call external tools (search, calculator, code) to extend what an LLM can do.\n",
    "\n",
    "**What we’ll build (final objective).** A minimal yet capable **Plan–Act–Observe agent** that:\n",
    "- Plans the next action given the current goal and scratchpad.\n",
    "- Invokes tools: **DuckDuckGo web search**, a **safe calculator**, and a **restricted Python executor**.\n",
    "- Observes results, updates its scratchpad, and iterates until it can produce a final answer with brief citations when web is used.\n",
    "\n",
    "**How steps unfold (basic → intermediate → advanced).**\n",
    "1. **Stage A (Core loop):** Prompt, scratchpad, and parsing of Thought/Action/Action Input/Observation.\n",
    "2. **Stage B (Tools):** Plug in three tools (search, calc, code). Add a simple tool router.\n",
    "3. **Stage C (Control & Safety):** Timeouts, step limits, and result formatting with optional citations.\n",
    "4. **Stage D (Demos & Tests):** Three diverse tasks that trigger multiple tool calls.\n",
    "\n",
    "**Requirements.**\n",
    "- **Python:** 3.10+\n",
    "- **Libraries (pinned):**\n",
    "  - `openai==1.40.2` (or set `LITELLM_PROVIDER` to use LiteLLM-compatible endpoints)\n",
    "  - `duckduckgo-search==5.3.1`\n",
    "  - `tiktoken==0.7.0` (token counting, optional)\n",
    "- **Env vars:**\n",
    "  - `OPENAI_API_KEY` (or set `LITELLM_BASE_URL` and `LITELLM_MODEL` if routing through LiteLLM; keep `OPENAI_API_KEY` empty in that case.)\n",
    "\n",
    "> ⚠️ **No secrets in code.** Read keys from environment variables only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81302d",
   "metadata": {},
   "source": [
    "## Plan Outline\n",
    "- **Stage A → Core Plan–Act–Observe Loop**\n",
    "- **Stage B → Add Tools (Search, Calculator, Python Exec)**\n",
    "- **Stage C → Safety, Limits, and Answer Formatting**\n",
    "- **Stage D → Testing with 3 Tasks**\n",
    "- **Wrap-Up + Quick Install**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7f6fd",
   "metadata": {},
   "source": [
    "## Stage A — Core Plan–Act–Observe Loop\n",
    "*Goal:* Implement a minimal loop where the LLM proposes a Thought and (optionally) a Tool `Action` with `Action Input`, we then capture an `Observation`, append to scratchpad, and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d1a49e-9983-4d22-aacf-98c34eb720ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API KEY: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "print(\"API KEY:\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570d4ac",
   "metadata": {},
   "source": [
    "### A1. System & helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3ab560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A1: Imports and environment\n",
    "import os, time, json, re, math, textwrap\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Callable, Optional, Any\n",
    "\n",
    "# Model selection: default to OpenAI; allow LiteLLM-style routing via envs\n",
    "USE_LITELLM = bool(os.getenv(\"LITELLM_BASE_URL\"))\n",
    "MODEL_DEFAULT = os.getenv(\"LITELLM_MODEL\", \"gpt-4o-mini\") if USE_LITELLM else os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "if USE_LITELLM:\n",
    "    import requests\n",
    "    def llm_chat(messages, model=MODEL_DEFAULT, temperature=0.2):\n",
    "        url = os.environ[\"LITELLM_BASE_URL\"].rstrip(\"/\") + \"/chat/completions\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature, \"stream\": False}\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "else:\n",
    "    from openai import OpenAI\n",
    "    _client = OpenAI()\n",
    "    def llm_chat(messages, model=MODEL_DEFAULT, temperature=0.2):\n",
    "        resp = _client.chat.completions.create(model=model, messages=messages, temperature=temperature)\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a careful, tool-using assistant that follows a Plan–Act–Observe loop.\n",
    "When you need to use a tool, reply ONLY in this format:\n",
    "\n",
    "Thought: <your reasoning, brief>\n",
    "Action: <tool_name>\n",
    "Action Input: <json-serialized input for that tool>\n",
    "\n",
    "If you have enough information to answer, reply ONLY in this format:\n",
    "\n",
    "Final Answer: <your concise answer with brief citations if web was used>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2701dca",
   "metadata": {},
   "source": [
    "**Explanation.** We provide a system prompt and an abstraction `llm_chat` that can call either OpenAI or a LiteLLM-compatible endpoint. The prompt constrains the agent to a predictable schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b05905",
   "metadata": {},
   "source": [
    "### A2. Scratchpad + parser for Thought/Action blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b11e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Parse model output into {thought, action, action_input, final}\n",
    "ACTION_RE = re.compile(r\"^Thought:\\s*(?P<thought>.*?)\\nAction:\\s*(?P<action>[\\w_\\-]+)\\nAction Input:\\s*(?P<input>.*)\\s*\\Z\", re.DOTALL)\n",
    "FINAL_RE = re.compile(r\"^Final Answer:\\s*(?P<final>.*)\\Z\", re.DOTALL)\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    thought: str\n",
    "    action: Optional[str] = None\n",
    "    action_input: Optional[dict] = None\n",
    "    observation: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    goal: str\n",
    "    scratchpad: List[Step] = field(default_factory=list)\n",
    "    used_web: bool = False\n",
    "\n",
    "def parse_agent_reply(text: str) -> Dict[str, Any]:\n",
    "    m_final = FINAL_RE.match(text.strip())\n",
    "    if m_final:\n",
    "        return {\"final\": m_final.group(\"final\").strip()}\n",
    "    m_action = ACTION_RE.match(text.strip())\n",
    "    if m_action:\n",
    "        raw_input = m_action.group(\"input\").strip()\n",
    "        try:\n",
    "            action_input = json.loads(raw_input) if raw_input else {}\n",
    "        except json.JSONDecodeError:\n",
    "            # fallback: treat as plain string\n",
    "            action_input = {\"query\": raw_input}\n",
    "        return {\n",
    "            \"thought\": m_action.group(\"thought\").strip(),\n",
    "            \"action\": m_action.group(\"action\").strip(),\n",
    "            \"action_input\": action_input,\n",
    "        }\n",
    "    # If neither matches, coerce into a Thought with no Action.\n",
    "    return {\"thought\": text.strip(), \"action\": None, \"action_input\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3e868",
   "metadata": {},
   "source": [
    "**Explanation.** The agent may emit either a `Final Answer` or a `Thought/Action/Action Input` triple. We parse both and store as `Step`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fdaf6",
   "metadata": {},
   "source": [
    "### A3. The core Plan–Act–Observe loop (no tools yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3091a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3: Core loop skeleton\n",
    "\n",
    "def run_agent(goal: str, tools: Dict[str, Callable[[dict], str]], max_steps: int = 6, temperature: float = 0.2) -> str:\n",
    "    state = AgentState(goal=goal)\n",
    "    for step_idx in range(1, max_steps + 1):\n",
    "        # Build messages: system + user(goal) + scratchpad transcript\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        user_block = f\"Goal: {goal}\\n\\nScratchpad so far:\\n\"\n",
    "        for i, s in enumerate(state.scratchpad, 1):\n",
    "            user_block += f\"Step {i}\\nThought: {s.thought}\\n\"\n",
    "            if s.action:\n",
    "                user_block += f\"Action: {s.action}\\nAction Input: {json.dumps(s.action_input, ensure_ascii=False)}\\n\"\n",
    "            if s.observation:\n",
    "                user_block += f\"Observation: {s.observation}\\n\"\n",
    "            user_block += \"\\n\"\n",
    "        messages.append({\"role\": \"user\", \"content\": user_block})\n",
    "\n",
    "        reply = llm_chat(messages, temperature=temperature)\n",
    "        parsed = parse_agent_reply(reply)\n",
    "\n",
    "        # Final?\n",
    "        if \"final\" in parsed:\n",
    "            return parsed[\"final\"]\n",
    "\n",
    "        # Otherwise execute action (if any)\n",
    "        thought = parsed.get(\"thought\", \"\")\n",
    "        action = parsed.get(\"action\")\n",
    "        action_input = parsed.get(\"action_input\") or {}\n",
    "\n",
    "        observation = \"(no action taken)\"\n",
    "        if action:\n",
    "            tool = tools.get(action)\n",
    "            if tool is None:\n",
    "                observation = f\"Error: Unknown tool '{action}'. Available: {list(tools)}\"\n",
    "            else:\n",
    "                try:\n",
    "                    observation = tool(action_input)\n",
    "                except Exception as e:\n",
    "                    observation = f\"ToolError: {type(e).__name__}: {e}\"\n",
    "            if action == \"web_search\" and \"Error\" not in observation:\n",
    "                state.used_web = True\n",
    "\n",
    "        state.scratchpad.append(Step(thought=thought, action=action, action_input=action_input, observation=observation))\n",
    "\n",
    "    # Fallback if max_steps reached\n",
    "    tail_note = \"\\n\\n(Note: step limit reached; consider increasing max_steps.)\"\n",
    "    return (\"Partial result after steps: \\n\" + \"\\n\".join(\n",
    "        [f\"- {s.observation}\" for s in state.scratchpad if s.observation]) + tail_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc33a3d",
   "metadata": {},
   "source": [
    "**Explanation.** The loop builds a message with the current transcript, asks the LLM for the next move, executes the chosen action if valid, appends the observation, and repeats. When the model believes it’s done, it returns a **Final Answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e746fc6",
   "metadata": {},
   "source": [
    "## Stage B — Add Tools (Search, Calculator, Python Exec)\n",
    "*Goal:* Register three tools and simple adapters.\n",
    "\n",
    "### B1. Tool: DuckDuckGo web search (top results + snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af052004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1: Web search via DuckDuckGo\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Simple cache to avoid repeated IO in a single run\n",
    "_CACHE: Dict[str, Any] = {}\n",
    "\n",
    "def web_search_tool(args: dict) -> str:\n",
    "    \"\"\"Args: {\"query\": str, \"max_results\": int=5, \"region\": \"wt-wt\", \"safesearch\": \"moderate\"}\n",
    "    Returns a JSON string with results: [{title, href, body}].\"\"\"\n",
    "    q = args.get(\"query\") or args.get(\"q\")\n",
    "    if not q: return \"Error: provide {'query': '<text>'}\"\n",
    "    k = int(args.get(\"max_results\", 5))\n",
    "    region = args.get(\"region\", \"wt-wt\")\n",
    "    safesearch = args.get(\"safesearch\", \"moderate\")\n",
    "    cache_key = json.dumps([q, k, region, safesearch])\n",
    "    if cache_key in _CACHE:\n",
    "        return _CACHE[cache_key]\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(q, region=region, safesearch=safesearch, max_results=k))\n",
    "    payload = json.dumps(results[:k], ensure_ascii=False)\n",
    "    _CACHE[cache_key] = payload\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e07c4",
   "metadata": {},
   "source": [
    "**Explanation.** We use `duckduckgo-search` to fetch a few fresh results. The agent will summarize and include brief citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887d0b7",
   "metadata": {},
   "source": [
    "### B2. Tool: Safe calculator (AST-limited eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1adc00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2: Safe calculator with AST parsing\n",
    "import ast, math\n",
    "\n",
    "class SafeEval(ast.NodeVisitor):\n",
    "    allowed_nodes = (\n",
    "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Constant,\n",
    "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Mod, ast.Pow,\n",
    "        ast.USub, ast.UAdd, ast.FloorDiv, ast.Call, ast.Name, ast.Load\n",
    "    )\n",
    "    allowed_names = {\n",
    "        \"pi\": math.pi,\n",
    "        \"e\": math.e,\n",
    "        \"tau\": math.tau,\n",
    "        \"sqrt\": math.sqrt,\n",
    "        \"log\": math.log,\n",
    "        \"sin\": math.sin,\n",
    "        \"cos\": math.cos,\n",
    "        \"tan\": math.tan,\n",
    "    }\n",
    "\n",
    "    def visit(self, node):\n",
    "        if not isinstance(node, self.allowed_nodes):\n",
    "            raise ValueError(f\"Disallowed expression: {type(node).__name__}\")\n",
    "        return super().visit(node)\n",
    "\n",
    "    def eval(self, expr: str):\n",
    "        tree = ast.parse(expr, mode=\"eval\")\n",
    "        self.visit(tree)\n",
    "        code = compile(tree, \"<calc>\", \"eval\")\n",
    "        return eval(code, {\"__builtins__\": {}}, self.allowed_names)\n",
    "\n",
    "_calc = SafeEval()\n",
    "\n",
    "def calculator_tool(args: dict) -> str:\n",
    "    expr = str(args.get(\"expression\") or args.get(\"expr\") or \"\").strip()\n",
    "    if not expr:\n",
    "        return \"Error: provide {'expression': '<math>'}\"\n",
    "    try:\n",
    "        val = _calc.eval(expr)\n",
    "        return str(val)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea24d88",
   "metadata": {},
   "source": [
    "**Explanation.** We parse with `ast` to whitelist numeric operations and a few math functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6456c",
   "metadata": {},
   "source": [
    "### B3. Tool: Restricted Python code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def38292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B3: Restricted Python execution (no I/O, limited globals)\n",
    "import contextlib, io, random\n",
    "\n",
    "ALLOWED_GLOBALS = {\n",
    "    \"math\": math, \"random\": random, \"range\": range, \"len\": len, \"sum\": sum,\n",
    "    \"min\": min, \"max\": max, \"abs\": abs, \"enumerate\": enumerate\n",
    "}\n",
    "\n",
    "def python_exec_tool(args: dict) -> str:\n",
    "    \"\"\"Args: {\"code\": \"python code\"}\n",
    "    Executes in a restricted namespace and returns printed output.\n",
    "    \"\"\"\n",
    "    code = args.get(\"code\")\n",
    "    if not code:\n",
    "        return \"Error: provide {'code': '<python>'}\"\n",
    "    # Disallow import, open, __, exec/eval, etc.\n",
    "    forbidden = [\"import \", \"open(\", \"__\", \"exec(\", \"eval(\", \"compile(\", \"globals(\", \"locals(\"]\n",
    "    if any(tok in code for tok in forbidden):\n",
    "        return \"Error: forbidden tokens detected in code.\"\n",
    "    # Capture stdout\n",
    "    buf = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(buf):\n",
    "            exec(compile(code, \"<agent-code>\", \"exec\"), {**ALLOWED_GLOBALS}, {})\n",
    "    except Exception as e:\n",
    "        return f\"Error: {type(e).__name__}: {e}\"\n",
    "    return buf.getvalue().strip() or \"<no output>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1750d6",
   "metadata": {},
   "source": [
    "**Explanation.** We block dangerous builtins and keep the sandbox intentionally minimal—good enough for mathy demos and small simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa1e9f",
   "metadata": {},
   "source": [
    "### B4. Register tools & a router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbe1bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4: Register tools\n",
    "TOOLS: Dict[str, Callable[[dict], str]] = {\n",
    "    \"web_search\": web_search_tool,\n",
    "    \"calculator\": calculator_tool,\n",
    "    \"python\": python_exec_tool,\n",
    "}\n",
    "\n",
    "# A small description block the LLM can use (you can show this to the model as needed)\n",
    "TOOL_DESCRIPTIONS = {\n",
    "    \"web_search\": \"Search the web with DuckDuckGo. Input: {\\\"query\\\": str, \\\"max_results\\\": int}. Output: JSON list of {title, href, body}\",\n",
    "    \"calculator\": \"Evaluate math expressions safely. Input: {\\\"expression\\\": str}. Output: stringified result\",\n",
    "    \"python\": \"Run small Python snippets without I/O. Input: {\\\"code\\\": str}. Output: stdout or <no output>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfb0d8",
   "metadata": {},
   "source": [
    "**Explanation.** The agent now has three callable tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b055f26",
   "metadata": {},
   "source": [
    "## Stage C — Safety, Limits, and Answer Formatting\n",
    "*Goal:* Encourage the model to plan; inject the tool list; limit steps; and format final answers with optional citations.\n",
    "\n",
    "### C1. Add tool hints to the prompt & a planning nudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8b6bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1: Extend SYSTEM_PROMPT with tool inventory + planning guidance\n",
    "TOOL_HINTS = \"\\n\".join([f\"- {k}: {v}\" for k, v in TOOL_DESCRIPTIONS.items()])\n",
    "SYSTEM_PROMPT_WITH_TOOLS = SYSTEM_PROMPT + \"\\n\\nAvailable tools:\\n\" + TOOL_HINTS + \"\\n\\nPlanning guidance: Break the task into steps. Prefer using tools when needed; keep Thoughts brief.\"\n",
    "\n",
    "def run_agent_with_tools(goal: str, max_steps: int = 6, temperature: float = 0.2) -> str:\n",
    "    # Same as run_agent but uses the extended system prompt and returns clean Final Answer.\n",
    "    state = AgentState(goal=goal)\n",
    "    for step_idx in range(1, max_steps + 1):\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_TOOLS}]\n",
    "        user_block = f\"Goal: {goal}\\n\\nScratchpad so far:\\n\"\n",
    "        for i, s in enumerate(state.scratchpad, 1):\n",
    "            user_block += f\"Step {i}\\nThought: {s.thought}\\n\"\n",
    "            if s.action:\n",
    "                user_block += f\"Action: {s.action}\\nAction Input: {json.dumps(s.action_input, ensure_ascii=False)}\\n\"\n",
    "            if s.observation:\n",
    "                user_block += f\"Observation: {s.observation}\\n\"\n",
    "            user_block += \"\\n\"\n",
    "        messages.append({\"role\": \"user\", \"content\": user_block})\n",
    "\n",
    "        reply = llm_chat(messages, temperature=temperature)\n",
    "        parsed = parse_agent_reply(reply)\n",
    "        if \"final\" in parsed:\n",
    "            final = parsed[\"final\"].strip()\n",
    "            # Add a small note if web used but no citation-like text present\n",
    "            if state.used_web and (\"http\" not in final and \"href\" not in final):\n",
    "                final += \"\\n\\n(Sources: derived from DuckDuckGo results in the scratchpad.)\"\n",
    "            return final\n",
    "\n",
    "        thought = parsed.get(\"thought\", \"\")\n",
    "        action = parsed.get(\"action\")\n",
    "        action_input = parsed.get(\"action_input\") or {}\n",
    "\n",
    "        observation = \"(no action taken)\"\n",
    "        if action:\n",
    "            tool = TOOLS.get(action)\n",
    "            if tool is None:\n",
    "                observation = f\"Error: Unknown tool '{action}'. Available: {list(TOOLS)}\"\n",
    "            else:\n",
    "                try:\n",
    "                    observation = tool(action_input)\n",
    "                except Exception as e:\n",
    "                    observation = f\"ToolError: {type(e).__name__}: {e}\"\n",
    "            if action == \"web_search\" and \"Error\" not in observation:\n",
    "                state.used_web = True\n",
    "        state.scratchpad.append(Step(thought=thought, action=action, action_input=action_input, observation=observation))\n",
    "\n",
    "    return \"Unable to reach Final Answer within step limit. Try increasing max_steps or simplifying the goal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b843e24",
   "metadata": {},
   "source": [
    "**Explanation.** We now provide tool descriptions to the model, nudge planning behavior, and return a clean string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62a637",
   "metadata": {},
   "source": [
    "## Stage D — Tests & Demonstrations\n",
    "We’ll execute three diverse tasks. Feel free to change topics to your interests.\n",
    "\n",
    "### D1. Research Task — “Find and summarize recent articles on a topic”\n",
    "*Prompt:*  \n",
    "> \"Find and summarize recent articles (past 3 months) on the environmental impact of data centers; provide 5 bullets and 3 source links.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70680367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **2025 ESG Report: Data Centre Environmental Impact**: This report provides an in-depth look at the environmental footprint of data center providers, highlighting the need for sustainable practices. [Read more](https://dcnnmagazine.com/data-centres/2025-esg-report-data-centre-environmental-impact/)\n",
      "- **Investigating the Environmental Sustainability of Data Centers**: This study reviews sustainable practices in data center construction and maintenance, emphasizing the importance of addressing environmental impacts. [Read more](https://ajosr.org/wp-content/uploads/journal/published_paper/volume-3/issue-1/ajsr2024_Zrj3RTFV.pdf)\n",
      "- **Data Center Environmental Impact: Key Challenges**: Discusses energy use, water consumption, e-waste, and emissions as major concerns in the digital infrastructure of data centers. [Read more](https://cc-techgroup.com/data-center-environmental-impact/)\n",
      "- **The Environmental Impact of Data Centers - Simple Science**: Highlights the significant increase in data centers due to AI, raising concerns about their environmental footprint. [Read more](https://scisimple.com/en/articles/2025-05-23-the-environmental-impact-of-data-centers--a3w0lqm)\n",
      "- **Investigating the Ecological Impacts of Data Centers**: Explores the growth of data centers and the lack of sustainability considerations, warning of potential ecological costs. [Read more](https://impactclimate.mit.edu/2025/03/20/investigating-the-ecological-impacts-of-data-centers/)\n"
     ]
    }
   ],
   "source": [
    "research_goal = (\n",
    "    \"Find and summarize recent articles (past 3 months) on the environmental impact of data centers; \"\n",
    "    \"provide 5 bullets and 3 source links.\"\n",
    ")\n",
    "print(run_agent_with_tools(research_goal, max_steps=6, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f046b88",
   "metadata": {},
   "source": [
    "**Expected behavior.** The agent will call `web_search` at least once, parse snippets, and return a compact summary + links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96f903",
   "metadata": {},
   "source": [
    "### D2. Fact → Calculation Task — “World population growth rate rough calc”\n",
    "*Prompt:*  \n",
    "> \"If world population was ~7.9B in 2021 and ~8.1B in 2023, estimate the annualized growth rate using the calculator, then explain in one sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5fe2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annualized growth rate of the world population from 2021 to 2023 is approximately 1.26%. This is calculated using the formula for annual growth rate, which shows the percentage increase over the two-year period.\n"
     ]
    }
   ],
   "source": [
    "calc_goal = (\n",
    "    \"If world population was ~7.9B in 2021 and ~8.1B in 2023, estimate the annualized growth rate using the calculator, \"\n",
    "    \"then explain in one sentence.\"\n",
    ")\n",
    "print(run_agent_with_tools(calc_goal, max_steps=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a2780",
   "metadata": {},
   "source": [
    "**Expected behavior.** The agent should (ideally) call `calculator` with an expression like `(8.1/7.9)**(1/2)-1` and then report a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15378817",
   "metadata": {},
   "source": [
    "### D3. Code/Simulation Task — “Monte Carlo π”\n",
    "*Prompt:*  \n",
    "> \"Use the python tool to run a short Monte Carlo simulation to estimate π with 100_000 samples; print the estimate only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74541f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to reach Final Answer within step limit. Try increasing max_steps or simplifying the goal.\n"
     ]
    }
   ],
   "source": [
    "mc_goal = (\n",
    "    \"Use the python tool to run a short Monte Carlo simulation to estimate π with 100_000 samples; print the estimate only.\"\n",
    ")\n",
    "print(run_agent_with_tools(mc_goal, max_steps=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5c0dc",
   "metadata": {},
   "source": [
    "**Expected behavior.** The agent should choose the `python` tool and print a numeric estimate near 3.1415."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e73f2b",
   "metadata": {},
   "source": [
    "##  Notes\n",
    "- **Why Plan–Act–Observe?** It structures problem solving, leaves a traceable audit trail (scratchpad), and makes tool use explicit.\n",
    "- **Design choices:**\n",
    "  - We used a **constrained output format** to keep parsing reliable.\n",
    "  - **Tool registry** enables easy extension—add your own tools and update `TOOL_DESCRIPTIONS`.\n",
    "  - **Safety:** AST-limited calculator and restricted `exec` prevent common hazards. The Python sandbox is intentionally conservative.\n",
    "- **Limitations:**\n",
    "  - Output parsing may fail if the model deviates from the schema; we partially guard against this.\n",
    "  - Web search quality depends on DDG snippets; for production, combine with site fetching + reader.\n",
    "  - The sandbox blocks imports and file/network I/O; widen carefully if you need more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75e0ca",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "**What you learned**\n",
    "- How a Plan–Act–Observe loop coordinates an LLM with external tools.\n",
    "- How to integrate web search, a calculator, and restricted Python execution.\n",
    "- How to test an agent on research, calc, and simulation tasks.\n",
    "\n",
    "**Next steps**\n",
    "- Add a **retriever** over your documents (e.g., local PDFs) as another tool.\n",
    "- Implement **result grading** (self-check) before Final Answer.\n",
    "- Log telemetry (steps, tokens) and add UI."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd9b811e-513a-4d5e-bd74-51952ee3c66d",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "## Quick Install (pinned)\n",
    "%%bash\n",
    "python -m venv .venv && source .venv/bin/activate  # Windows: .venv\\\\Scripts\\\\activate\n",
    "pip install --upgrade pip\n",
    "pip install openai==1.40.2 duckduckgo-search==5.3.1 tiktoken==0.7.0 requests==2.32.3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0aa7f49-f01c-495a-85fc-b16a0528e14a",
   "metadata": {
    "language": "bash"
   },
   "source": [
    "## Minimal .env template (example)\n",
    "%%bash\n",
    "# If using OpenAI directly\n",
    "echo 'export OPENAI_API_KEY=\"sk-...\"' >> .env\n",
    "# If routing via LiteLLM-compatible server (optional)\n",
    "# echo 'export LITELLM_BASE_URL=\"http://localhost:4000/v1\"' >> .env\n",
    "# echo 'export LITELLM_MODEL=\"gpt-4o-mini\"' >> .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9fc29-13fc-4aa9-acce-fe0e294a7ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day 2 — Exercise 5: Agent Planning and Tool Integration (Plan–Act–Observe)"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
