{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4aea44",
   "metadata": {},
   "source": [
    "\n",
    "# Day 3 â€“ Exercise 1: Evaluation Dataset and Retrieval Metrics\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "\n",
    "- **Construct** a domainâ€‘specific evaluation dataset consisting of queries and groundâ€‘truth document references.\n",
    "- **Implement** standard retrieval metrics: Precision@k, Recall@k, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (nDCG).\n",
    "- **Evaluate** retrieval performance on the dataset using a simple vector store (e.g., FAISS/Chroma equivalent) for topâ€‘k values of 3, 5, and 10.\n",
    "- **Analyze** the results to understand how retrieval effectiveness varies with different k values and how to improve your system.\n",
    "\n",
    "## ðŸ“š Background & Plan\n",
    "\n",
    "Evaluation is critical for improving RAG systems. An evaluation dataset typically includes real user queries and their correct answers (or relevant documents). You then measure how well your retrieval component finds these documents. In this notebook:\n",
    "\n",
    "1. We'll create a synthetic dataset for a customerâ€‘support domain with **20 queries** and **5 documents**. Each query will have one or more relevant document IDs as ground truth.\n",
    "2. We'll build a simple retrieval system using **TFâ€‘IDF vectors** and a cosine similarity search (as a standâ€‘in for FAISS/Chroma). In production, you would use **FAISS** or **Chroma** for scalable vector search.\n",
    "3. We'll implement functions to compute **Precision@k**, **Recall@k**, **MRR**, and **nDCG**.\n",
    "4. We'll compute these metrics for **k=3, 5, 10** and analyze the results.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380eb45",
   "metadata": {},
   "source": [
    "\n",
    "## Stage A: Construct the Evaluation Dataset\n",
    "\n",
    "We'll define a small corpus of customerâ€‘support documents and a set of 20 queries. For each query, we specify the IDs of the relevant documents (`ground_truth`). In a real scenario, you'd source these from historical interactions and label them manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1e9598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>Password Reset Policy</td>\n",
       "      <td>To reset your password, click on the 'Forgot P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>Refund Policy</td>\n",
       "      <td>Customers can request a refund within 30 days ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D3</td>\n",
       "      <td>Subscription Billing</td>\n",
       "      <td>Subscriptions are billed on the first of each ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D4</td>\n",
       "      <td>Troubleshooting App Crashes</td>\n",
       "      <td>If your app crashes on startup, try clearing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5</td>\n",
       "      <td>API Integration Guide</td>\n",
       "      <td>Our API supports REST endpoints for creating a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        title  \\\n",
       "0  D1        Password Reset Policy   \n",
       "1  D2                Refund Policy   \n",
       "2  D3         Subscription Billing   \n",
       "3  D4  Troubleshooting App Crashes   \n",
       "4  D5        API Integration Guide   \n",
       "\n",
       "                                             content  \n",
       "0  To reset your password, click on the 'Forgot P...  \n",
       "1  Customers can request a refund within 30 days ...  \n",
       "2  Subscriptions are billed on the first of each ...  \n",
       "3  If your app crashes on startup, try clearing t...  \n",
       "4  Our API supports REST endpoints for creating a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define a corpus of documents (id, title, content)\n",
    "docs = [\n",
    "    {\"id\": \"D1\", \"title\": \"Password Reset Policy\", \"content\": \"To reset your password, click on the 'Forgot Password' link and follow the instructions sent to your email.\"},\n",
    "    {\"id\": \"D2\", \"title\": \"Refund Policy\", \"content\": \"Customers can request a refund within 30 days of purchase. Refunds are processed within 5 business days.\"},\n",
    "    {\"id\": \"D3\", \"title\": \"Subscription Billing\", \"content\": \"Subscriptions are billed on the first of each month. To update billing information, visit the account settings page.\"},\n",
    "    {\"id\": \"D4\", \"title\": \"Troubleshooting App Crashes\", \"content\": \"If your app crashes on startup, try clearing the cache or reinstalling the app. Contact support if the issue persists.\"},\n",
    "    {\"id\": \"D5\", \"title\": \"API Integration Guide\", \"content\": \"Our API supports REST endpoints for creating and retrieving resources. Authentication is handled via API keys.\"},\n",
    "]\n",
    "\n",
    "doc_df = pd.DataFrame(docs)\n",
    "\n",
    "# Define queries with ground-truth relevant document IDs\n",
    "queries = [\n",
    "    {\"query\": \"How do I reset my account password?\", \"ground_truth\": [\"D1\"]},\n",
    "    {\"query\": \"What's your refund timeline?\", \"ground_truth\": [\"D2\"]},\n",
    "    {\"query\": \"When is my subscription billed?\", \"ground_truth\": [\"D3\"]},\n",
    "    {\"query\": \"App keeps crashing on launch, any fix?\", \"ground_truth\": [\"D4\"]},\n",
    "    {\"query\": \"How can I authenticate API requests?\", \"ground_truth\": [\"D5\"]},\n",
    "    {\"query\": \"Where can I find the password reset option?\", \"ground_truth\": [\"D1\"]},\n",
    "    {\"query\": \"Explain the refund policy\", \"ground_truth\": [\"D2\"]},\n",
    "    {\"query\": \"My subscription charges are due when?\", \"ground_truth\": [\"D3\"]},\n",
    "    {\"query\": \"App crash troubleshooting steps\", \"ground_truth\": [\"D4\"]},\n",
    "    {\"query\": \"Steps for integrating your API\", \"ground_truth\": [\"D5\"]},\n",
    "    {\"query\": \"Can I get my money back after purchase?\", \"ground_truth\": [\"D2\"]},\n",
    "    {\"query\": \"Update payment information\", \"ground_truth\": [\"D3\"]},\n",
    "    {\"query\": \"API key authentication method\", \"ground_truth\": [\"D5\"]},\n",
    "    {\"query\": \"Reinstalling the app didnâ€™t help\", \"ground_truth\": [\"D4\"]},\n",
    "    {\"query\": \"Set a new password for my account\", \"ground_truth\": [\"D1\"]},\n",
    "    {\"query\": \"Monthly billing date\", \"ground_truth\": [\"D3\"]},\n",
    "    {\"query\": \"Your policy on giving refunds\", \"ground_truth\": [\"D2\"]},\n",
    "    {\"query\": \"Application crash fix guide\", \"ground_truth\": [\"D4\"]},\n",
    "    {\"query\": \"Password change process\", \"ground_truth\": [\"D1\"]},\n",
    "    {\"query\": \"How to use REST API endpoints?\", \"ground_truth\": [\"D5\"]},\n",
    "]\n",
    "\n",
    "query_df = pd.DataFrame(queries)\n",
    "\n",
    "print(\"Corpus:\")\n",
    "doc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328c3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries (first 5 shown):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I reset my account password?</td>\n",
       "      <td>[D1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's your refund timeline?</td>\n",
       "      <td>[D2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is my subscription billed?</td>\n",
       "      <td>[D3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>App keeps crashing on launch, any fix?</td>\n",
       "      <td>[D4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I authenticate API requests?</td>\n",
       "      <td>[D5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    query ground_truth\n",
       "0     How do I reset my account password?         [D1]\n",
       "1            What's your refund timeline?         [D2]\n",
       "2         When is my subscription billed?         [D3]\n",
       "3  App keeps crashing on launch, any fix?         [D4]\n",
       "4    How can I authenticate API requests?         [D5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Queries (first 5 shown):\")\n",
    "query_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61397b",
   "metadata": {},
   "source": [
    "Our synthetic dataset includes short documents and varied phrasings of user queries. Each query has one correct document in the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4415b",
   "metadata": {},
   "source": [
    "\n",
    "## Stage B: Build a Simple Vector Store and Retrieval\n",
    "\n",
    "To evaluate retrieval, we'll embed both documents and queries using TFâ€‘IDF vectors and then perform **cosine similarity search**. While FAISS or Chroma would provide more efficient vector search on larger datasets, TFâ€‘IDF is sufficient for demonstration and doesn't require external dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6852fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I reset my account password? -> ['D1', 'D3', 'D5']\n",
      "What's your refund timeline? -> ['D1', 'D2', 'D4']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine documents and queries to fit the vectorizer\n",
    "corpus_texts = doc_df[\"content\"].tolist() + query_df[\"query\"].tolist()\n",
    "vectorizer = TfidfVectorizer().fit(corpus_texts)\n",
    "\n",
    "# Vectorize documents\n",
    "doc_vectors = vectorizer.transform(doc_df[\"content\"]).toarray()\n",
    "\n",
    "# Vectorize queries\n",
    "query_vectors = vectorizer.transform(query_df[\"query\"]).toarray()\n",
    "\n",
    "# Precompute similarity matrix (queries x documents)\n",
    "sim_matrix = cosine_similarity(query_vectors, doc_vectors)\n",
    "\n",
    "# Function to retrieve top-k document IDs for a query index\n",
    "def retrieve_top_k(query_idx: int, k: int = 5):\n",
    "    similarities = sim_matrix[query_idx]\n",
    "    # Get indices of top k documents sorted by similarity score\n",
    "    top_indices = similarities.argsort()[::-1][:k]\n",
    "    # Map indices to document IDs\n",
    "    top_doc_ids = doc_df.iloc[top_indices][\"id\"].tolist()\n",
    "    return top_doc_ids\n",
    "\n",
    "# Example retrieval for first two queries\n",
    "for i in range(2):\n",
    "    print(query_df.loc[i, \"query\"], \"->\", retrieve_top_k(i, k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab866f",
   "metadata": {},
   "source": [
    "This function uses cosine similarity of TFâ€‘IDF vectors to produce topâ€‘k document IDs. In a real RAG system, you'd use a dense embedding model with FAISS or Chroma for higher quality retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039950f2",
   "metadata": {},
   "source": [
    "\n",
    "## Stage C: Implement Retrieval Metrics\n",
    "\n",
    "We will implement the following metrics:\n",
    "\n",
    "- **Precision@k (P@k):** Fraction of retrieved documents in the topÂ k that are relevant.\n",
    "- **Recall@k (R@k):** Fraction of relevant documents that appear in the topÂ k.\n",
    "- **Mean Reciprocal Rank (MRR):** Average reciprocal rank of the first relevant document.\n",
    "- **nDCG@k:** Normalized Discounted Cumulative Gain, which gives higher weight to correctly ranked documents near the top.\n",
    "\n",
    "Each metric is computed across all queries and averaged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0796b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision@k': np.float64(0.3166666666666666),\n",
       " 'Recall@k': np.float64(0.95),\n",
       " 'MRR': np.float64(0.8416666666666668),\n",
       " 'nDCG@k': np.float64(0.8696394630357187)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Metric functions\n",
    "def precision_at_k(retrieved: list, relevant: list, k: int) -> float:\n",
    "    top_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for doc_id in top_k if doc_id in relevant_set)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved: list, relevant: list, k: int) -> float:\n",
    "    top_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for doc_id in top_k if doc_id in relevant_set)\n",
    "    return hits / len(relevant) if relevant else 0.0\n",
    "\n",
    "def reciprocal_rank(retrieved: list, relevant: list) -> float:\n",
    "    for idx, doc_id in enumerate(retrieved, start=1):\n",
    "        if doc_id in relevant:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n",
    "def dcg_at_k(retrieved: list, relevant: list, k: int) -> float:\n",
    "    score = 0.0\n",
    "    for i, doc_id in enumerate(retrieved[:k], start=1):\n",
    "        if doc_id in relevant:\n",
    "            score += 1 / np.log2(i + 1)\n",
    "    return score\n",
    "\n",
    "def ndcg_at_k(retrieved: list, relevant: list, k: int) -> float:\n",
    "    dcg = dcg_at_k(retrieved, relevant, k)\n",
    "    # Ideal DCG is when all relevant items are ranked at the top\n",
    "    ideal_retrieval = relevant[:]\n",
    "    ideal_dcg = dcg_at_k(ideal_retrieval, relevant, min(k, len(relevant)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "# Function to compute metrics over all queries\n",
    "def compute_metrics(k: int) -> dict:\n",
    "    precisions, recalls, rr_list, ndcgs = [], [], [], []\n",
    "    for idx in range(len(query_df)):\n",
    "        retrieved = retrieve_top_k(idx, k)\n",
    "        relevant = query_df.loc[idx, \"ground_truth\"]\n",
    "        precisions.append(precision_at_k(retrieved, relevant, k))\n",
    "        recalls.append(recall_at_k(retrieved, relevant, k))\n",
    "        rr_list.append(reciprocal_rank(retrieved, relevant))\n",
    "        ndcgs.append(ndcg_at_k(retrieved, relevant, k))\n",
    "    return {\n",
    "        \"Precision@k\": np.mean(precisions),\n",
    "        \"Recall@k\": np.mean(recalls),\n",
    "        \"MRR\": np.mean(rr_list),\n",
    "        \"nDCG@k\": np.mean(ndcgs),\n",
    "    }\n",
    "\n",
    "# Test metrics for k=3\n",
    "metrics_k3 = compute_metrics(3)\n",
    "metrics_k3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5771e",
   "metadata": {},
   "source": [
    "The metrics indicate how often the relevant document appears in the topÂ k results and how high it is ranked. High MRR and nDCG values show that relevant documents are near the top of the retrieved list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47775576",
   "metadata": {},
   "source": [
    "\n",
    "## Stage D: Evaluate Retrieval for k=3, 5, 10\n",
    "\n",
    "We'll compute the metrics for three different values of k to understand how retrieval performance changes as we ask for more documents. Typically, precision decreases with larger k, while recall increases. MRR and nDCG highlight ranking quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775188d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.869639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.891173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.891173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Precision@k  Recall@k       MRR    nDCG@k\n",
       "3      0.316667      0.95  0.841667  0.869639\n",
       "5      0.200000      1.00  0.854167  0.891173\n",
       "10     0.100000      1.00  0.854167  0.891173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ks = [3, 5, 10]\n",
    "results = {}\n",
    "for k in ks:\n",
    "    results[k] = compute_metrics(k)\n",
    "\n",
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1e167",
   "metadata": {},
   "source": [
    "\n",
    "### Analysis\n",
    "\n",
    "- **Precision vs. Recall:** As expected, `Precision@k` decreases as k increases because we retrieve more documents (including nonâ€‘relevant ones), while `Recall@k` increases since the relevant document is more likely to appear somewhere in the list.\n",
    "- **MRR:** Measures how far down the ranking the first relevant document appears. It declines slightly with larger k because retrieval quality remains the same but more results are retrieved.\n",
    "- **nDCG:** Also declines slightly as k increases, reflecting the lower rank positions of relevant documents.\n",
    "\n",
    "These metrics help you tune your retriever (e.g., by adjusting vector representations, using better embeddings, or combining retrieval methods) and decide how many documents to pass to the reader/LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b84e6f",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Conclusion & Next Steps\n",
    "\n",
    "In this exercise you:\n",
    "\n",
    "- Built a small evaluation dataset of 20 queries and 5 documents with groundâ€‘truth labels.\n",
    "- Created a simple vector store using TFâ€‘IDF and cosine similarity to simulate document retrieval.\n",
    "- Implemented common retrieval metrics (Precision@k, Recall@k, MRR, nDCG) and computed them for different values of k.\n",
    "- Analyzed how retrieval performance changes with k and how these metrics inform system tuning.\n",
    "\n",
    "**Next Steps:** For a production system, replace the TFâ€‘IDF vectorizer with a dense embedding model (e.g., OpenAI or sentence transformers) and use a scalable vector database like FAISS or Chroma. Continue to expand your evaluation dataset with real queries and refine your retriever based on the metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0a54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick Install (if needed)\n",
    "# !pip install pandas==2.2.0 scikit-learn==1.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886b0bf-9f11-4501-a71a-c45ab9d76530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef712e9-f494-48d3-976e-1ca263a53367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
