{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f62420",
   "metadata": {},
   "source": [
    "\n",
    "# Day 2 ‚Äì Exercise 8: Advanced Multi‚ÄëStep Customer Support Workflow (Enterprise)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "In this advanced exercise you will:\n",
    "\n",
    "- **Build** a multi‚Äëstep customer support agent that performs classification, retrieval, answer generation, ticket creation, and summarization.\n",
    "- **Integrate** baseline and LLM‚Äëbased classifiers (via RealLight/LiteLLM) to understand different approaches to intent routing.\n",
    "- **Simulate** retrieval using BM25, dense, and hybrid strategies and track metadata such as document counts and sources.\n",
    "- **Capture** detailed logs (including latency and cost estimates) and compute metrics like classification accuracy, success rate, and average cost.\n",
    "- **Analyze** failure patterns and plan improvements for a production MVP.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **Intent Classification:** Compare rule‚Äëbased, logistic regression, and LLM classifiers.\n",
    "2. **Retrieval Strategies:** Route queries to BM25, dense, or hybrid retrievers and track citations.\n",
    "3. **Multi‚ÄëStep Orchestration:** Design a workflow that answers questions, creates tickets when unresolved, and summarizes interactions.\n",
    "4. **Observability & Metrics:** Log interactions with metrics such as latency and cost to inform system optimization.\n",
    "\n",
    "**Estimated Time:** 120 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a66f6b",
   "metadata": {},
   "source": [
    "\n",
    "## üîß Setup and Installation\n",
    "\n",
    "Before starting, ensure you have the following packages installed:\n",
    "\n",
    "- `pandas==2.2.0` for data handling\n",
    "- `scikit‚Äëlearn==1.4.0` for the logistic regression classifier\n",
    "- `litellm==1.39.1` and `openai==1.12.0` for RealLight (LiteLLM) integration\n",
    "\n",
    "Install them in your environment using pip:\n",
    "```bash\n",
    "pip install pandas==2.2.0 scikit-learn==1.4.0 litellm==1.39.1 openai==1.12.0\n",
    "```\n",
    "\n",
    "Also, set your `OPENAI_API_KEY` (or other provider key) as an environment variable. **Never hard‚Äëcode secrets in notebooks.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fe1d72e-525c-42a3-a54e-4901da40ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please install the elasticsearch library: pip install elasticsearch\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from elasticsearch import Elasticsearch\n",
    "\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(\n",
    "        hosts=[{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"http\"}],\n",
    "        request_timeout=30\n",
    "    )\n",
    "\n",
    "    # Quick test: ping the cluster\n",
    "    if es.ping():\n",
    "        print(\"‚úÖ Connected to Elasticsearch\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not connect to Elasticsearch\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Please install the elasticsearch library: pip install elasticsearch\")\n",
    "except Exception as e:\n",
    "    print(\"Elasticsearch connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a283ffc0-1a62-4b34-9aec-5a4162e02883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-9.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting elastic-transport<10,>=9.1.0 (from elasticsearch)\n",
      "  Downloading elastic_transport-9.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.13/site-packages (from elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.13/site-packages (from elasticsearch) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/anaconda3/lib/python3.13/site-packages (from elastic-transport<10,>=9.1.0->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from elastic-transport<10,>=9.1.0->elasticsearch) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil->elasticsearch) (1.17.0)\n",
      "Downloading elasticsearch-9.1.1-py3-none-any.whl (937 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m937.5/937.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading elastic_transport-9.1.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [elasticsearch]0m [elasticsearch]\n",
      "Successfully installed elastic-transport-9.1.0 elasticsearch-9.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aae2ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure environment variable is set (for demonstration; do not print secrets)\n",
    "import os\n",
    "os.environ.get(\"OPENAI_API_KEY\", \"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a65c3b",
   "metadata": {},
   "source": [
    "\n",
    "## Stage A: Prepare the Dataset\n",
    "\n",
    "We'll construct a dataset of **12 customer support queries** across multiple categories:\n",
    "\n",
    "- **Factual:** Questions answerable with static information (e.g., policies)\n",
    "- **Procedural:** How‚Äëto questions requiring stepwise instructions\n",
    "- **Exploratory:** Open‚Äëended or general questions\n",
    "- **Billing:** Questions about payments and charges\n",
    "- **Troubleshooting:** Issues requiring deeper investigation (often unresolved)\n",
    "\n",
    "Each entry includes a `query`, its annotated `intent`, a `resolved` flag indicating whether it can be answered directly, and a `category` for more fine‚Äëgrained analysis. In production, you'd source these from real ticket data and label them manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c33442d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>resolved</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I reset my password?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>True</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why was my account suspended?</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>False</td>\n",
       "      <td>troubleshooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the refund policy?</td>\n",
       "      <td>factual</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My order hasn't arrived yet, what should I do?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>False</td>\n",
       "      <td>troubleshooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the differences between basic and prem...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>True</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can I update my billing address?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>True</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When will I be charged for my subscription?</td>\n",
       "      <td>factual</td>\n",
       "      <td>True</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I see an unknown charge on my credit card.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>False</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steps to troubleshoot a connectivity issue.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>True</td>\n",
       "      <td>troubleshooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Provide an overview of your customer loyalty p...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do I cancel my order and get a refund?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>False</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Your site is down, I cannot access anything.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>False</td>\n",
       "      <td>troubleshooting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query       intent  resolved  \\\n",
       "0                         How do I reset my password?   procedural      True   \n",
       "1                       Why was my account suspended?  exploratory     False   \n",
       "2                          What is the refund policy?      factual      True   \n",
       "3      My order hasn't arrived yet, what should I do?   procedural     False   \n",
       "4   Explain the differences between basic and prem...  exploratory      True   \n",
       "5                How can I update my billing address?   procedural      True   \n",
       "6         When will I be charged for my subscription?      factual      True   \n",
       "7          I see an unknown charge on my credit card.  exploratory     False   \n",
       "8         Steps to troubleshoot a connectivity issue.   procedural      True   \n",
       "9   Provide an overview of your customer loyalty p...  exploratory      True   \n",
       "10         How do I cancel my order and get a refund?   procedural     False   \n",
       "11       Your site is down, I cannot access anything.  exploratory     False   \n",
       "\n",
       "           category  \n",
       "0        procedural  \n",
       "1   troubleshooting  \n",
       "2           factual  \n",
       "3   troubleshooting  \n",
       "4       exploratory  \n",
       "5           billing  \n",
       "6           billing  \n",
       "7           billing  \n",
       "8   troubleshooting  \n",
       "9           factual  \n",
       "10          billing  \n",
       "11  troubleshooting  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "support_data = [\n",
    "    {\"query\": \"How do I reset my password?\", \"intent\": \"procedural\", \"resolved\": True, \"category\": \"procedural\"},\n",
    "    {\"query\": \"Why was my account suspended?\", \"intent\": \"exploratory\", \"resolved\": False, \"category\": \"troubleshooting\"},\n",
    "    {\"query\": \"What is the refund policy?\", \"intent\": \"factual\", \"resolved\": True, \"category\": \"factual\"},\n",
    "    {\"query\": \"My order hasn't arrived yet, what should I do?\", \"intent\": \"procedural\", \"resolved\": False, \"category\": \"troubleshooting\"},\n",
    "    {\"query\": \"Explain the differences between basic and premium plans.\", \"intent\": \"exploratory\", \"resolved\": True, \"category\": \"exploratory\"},\n",
    "    {\"query\": \"How can I update my billing address?\", \"intent\": \"procedural\", \"resolved\": True, \"category\": \"billing\"},\n",
    "    {\"query\": \"When will I be charged for my subscription?\", \"intent\": \"factual\", \"resolved\": True, \"category\": \"billing\"},\n",
    "    {\"query\": \"I see an unknown charge on my credit card.\", \"intent\": \"exploratory\", \"resolved\": False, \"category\": \"billing\"},\n",
    "    {\"query\": \"Steps to troubleshoot a connectivity issue.\", \"intent\": \"procedural\", \"resolved\": True, \"category\": \"troubleshooting\"},\n",
    "    {\"query\": \"Provide an overview of your customer loyalty program.\", \"intent\": \"exploratory\", \"resolved\": True, \"category\": \"factual\"},\n",
    "    {\"query\": \"How do I cancel my order and get a refund?\", \"intent\": \"procedural\", \"resolved\": False, \"category\": \"billing\"},\n",
    "    {\"query\": \"Your site is down, I cannot access anything.\", \"intent\": \"exploratory\", \"resolved\": False, \"category\": \"troubleshooting\"},\n",
    "]\n",
    "\n",
    "support_df = pd.DataFrame(support_data)\n",
    "support_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88516d82",
   "metadata": {},
   "source": [
    "This table forms the basis for our classification and workflow. Note the `category` field which can be used for more granular routing or specialized agents in larger systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace5f12",
   "metadata": {},
   "source": [
    "\n",
    "## Stage B: Intent Classification\n",
    "\n",
    "To decide how to retrieve information and handle each query, we'll compare three classifiers:\n",
    "\n",
    "1. **Rule‚ÄëBased Classifier:** Simple keyword matching.\n",
    "2. **Logistic Regression:** Supervised learning on TF‚ÄëIDF features.\n",
    "3. **LLM Classifier via RealLight:** Few‚Äëshot classification using an LLM. (We provide a function; running it requires a valid API key and network access.)\n",
    "\n",
    "### B.1: Rule‚ÄëBased Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f566101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>pred_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I reset my password?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why was my account suspended?</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the refund policy?</td>\n",
       "      <td>factual</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My order hasn't arrived yet, what should I do?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the differences between basic and prem...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can I update my billing address?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When will I be charged for my subscription?</td>\n",
       "      <td>factual</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I see an unknown charge on my credit card.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steps to troubleshoot a connectivity issue.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Provide an overview of your customer loyalty p...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do I cancel my order and get a refund?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>billing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Your site is down, I cannot access anything.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query       intent  \\\n",
       "0                         How do I reset my password?   procedural   \n",
       "1                       Why was my account suspended?  exploratory   \n",
       "2                          What is the refund policy?      factual   \n",
       "3      My order hasn't arrived yet, what should I do?   procedural   \n",
       "4   Explain the differences between basic and prem...  exploratory   \n",
       "5                How can I update my billing address?   procedural   \n",
       "6         When will I be charged for my subscription?      factual   \n",
       "7          I see an unknown charge on my credit card.  exploratory   \n",
       "8         Steps to troubleshoot a connectivity issue.   procedural   \n",
       "9   Provide an overview of your customer loyalty p...  exploratory   \n",
       "10         How do I cancel my order and get a refund?   procedural   \n",
       "11       Your site is down, I cannot access anything.  exploratory   \n",
       "\n",
       "      pred_rule  \n",
       "0    procedural  \n",
       "1   exploratory  \n",
       "2       billing  \n",
       "3       factual  \n",
       "4   exploratory  \n",
       "5       billing  \n",
       "6       billing  \n",
       "7       billing  \n",
       "8    procedural  \n",
       "9       factual  \n",
       "10      billing  \n",
       "11  exploratory  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Simple keyword-based intent classifier\n",
    "\n",
    "def rule_based_intent(query: str) -> str:\n",
    "    query_lower = query.lower()\n",
    "    factual_kw = [\"what\", \"when\", \"policy\", \"overview\"]\n",
    "    procedural_kw = [\"how\", \"steps\", \"reset\", \"update\", \"cancel\", \"troubleshoot\"]\n",
    "    billing_kw = [\"charge\", \"billing\", \"refund\", \"subscription\"]\n",
    "    if any(kw in query_lower for kw in billing_kw):\n",
    "        return \"billing\"\n",
    "    if any(kw in query_lower for kw in procedural_kw):\n",
    "        return \"procedural\"\n",
    "    if any(kw in query_lower for kw in factual_kw):\n",
    "        return \"factual\"\n",
    "    return \"exploratory\"\n",
    "\n",
    "# Apply to dataset\n",
    "support_df[\"pred_rule\"] = support_df[\"query\"].apply(rule_based_intent)\n",
    "support_df[[\"query\", \"intent\", \"pred_rule\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800eabc",
   "metadata": {},
   "source": [
    "The rule‚Äëbased approach works well for obvious keywords but will misclassify ambiguous queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fe7f7",
   "metadata": {},
   "source": [
    "### B.2: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfaee76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " exploratory       1.00      0.50      0.67         2\n",
      "     factual       0.00      0.00      0.00         1\n",
      "  procedural       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.44      0.50      0.39         4\n",
      "weighted avg       0.58      0.50      0.46         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>pred_log_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I reset my password?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why was my account suspended?</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the refund policy?</td>\n",
       "      <td>factual</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My order hasn't arrived yet, what should I do?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the differences between basic and prem...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can I update my billing address?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When will I be charged for my subscription?</td>\n",
       "      <td>factual</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I see an unknown charge on my credit card.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steps to troubleshoot a connectivity issue.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Provide an overview of your customer loyalty p...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do I cancel my order and get a refund?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Your site is down, I cannot access anything.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query       intent  \\\n",
       "0                         How do I reset my password?   procedural   \n",
       "1                       Why was my account suspended?  exploratory   \n",
       "2                          What is the refund policy?      factual   \n",
       "3      My order hasn't arrived yet, what should I do?   procedural   \n",
       "4   Explain the differences between basic and prem...  exploratory   \n",
       "5                How can I update my billing address?   procedural   \n",
       "6         When will I be charged for my subscription?      factual   \n",
       "7          I see an unknown charge on my credit card.  exploratory   \n",
       "8         Steps to troubleshoot a connectivity issue.   procedural   \n",
       "9   Provide an overview of your customer loyalty p...  exploratory   \n",
       "10         How do I cancel my order and get a refund?   procedural   \n",
       "11       Your site is down, I cannot access anything.  exploratory   \n",
       "\n",
       "   pred_log_reg  \n",
       "0    procedural  \n",
       "1    procedural  \n",
       "2    procedural  \n",
       "3    procedural  \n",
       "4   exploratory  \n",
       "5    procedural  \n",
       "6    procedural  \n",
       "7   exploratory  \n",
       "8    procedural  \n",
       "9   exploratory  \n",
       "10   procedural  \n",
       "11  exploratory  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    support_df[\"query\"], support_df[\"intent\"], test_size=0.3, random_state=42, stratify=support_df[\"intent\"]\n",
    ")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "log_reg = LogisticRegression(max_iter=300)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = log_reg.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save full-model predictions for later use\n",
    "support_df[\"pred_log_reg\"] = log_reg.predict(vectorizer.transform(support_df[\"query\"]))\n",
    "support_df[[\"query\", \"intent\", \"pred_log_reg\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ffd90",
   "metadata": {},
   "source": [
    "The logistic regression classifier captures patterns beyond keywords but still requires labeled data and may struggle with rare intents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d303ab7",
   "metadata": {},
   "source": [
    "### B.3: LLM Classifier via RealLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "961beab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Import litellm if available\n",
    "try:\n",
    "    import litellm  # type: ignore\n",
    "except ImportError:\n",
    "    litellm = None\n",
    "\n",
    "\n",
    "def llm_classify_intents(\n",
    "    queries: List[str], model: str = \"gpt-3.5-turbo\", temperature: float = 0.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Classify intents using RealLight; returns a list of predicted intents.\n",
    "\n",
    "    Allowed intents:\n",
    "    - factual\n",
    "    - procedural\n",
    "    - exploratory\n",
    "    - billing\n",
    "    - troubleshooting\n",
    "    \"\"\"\n",
    "    if litellm is None:\n",
    "        print(\"litellm not installed; returning 'exploratory' for all queries.\")\n",
    "        return [\"exploratory\"] * len(queries)\n",
    "\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Classify the user's intent into one of: \"\n",
    "                    \"factual, procedural, exploratory, billing, troubleshooting.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Query: {q}\\nIntent:\"   # ‚úÖ explicit newline\n",
    "            },\n",
    "        ]\n",
    "        try:\n",
    "            resp = litellm.completion(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=1,\n",
    "            )\n",
    "            intent = resp.choices[0].message.content.strip().lower()\n",
    "        except Exception as e:\n",
    "            print(f\"LLM error for '{q}': {e}\")\n",
    "            intent = \"exploratory\"\n",
    "        results.append(intent)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example (uncomment if API key is set)\n",
    "# support_df[\"pred_llm\"] = llm_classify_intents(support_df[\"query\"].tolist())\n",
    "# support_df[[\"query\", \"intent\", \"pred_llm\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da615bfc",
   "metadata": {},
   "source": [
    "LLM classification can capture subtle nuances but incurs latency and cost. We avoid executing it here; provide your API key and uncomment the example to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6da8cc",
   "metadata": {},
   "source": [
    "\n",
    "## Stage C: Retrieval Strategies\n",
    "\n",
    "To answer queries, we'll simulate three retrieval strategies:\n",
    "\n",
    "- **BM25 Retrieval:** Keyword search (fast for factual queries)\n",
    "- **Dense Vector Retrieval:** Semantic search (good for exploratory queries)\n",
    "- **Hybrid Retrieval:** Combines BM25 and dense results (useful for procedural or troubleshooting queries)\n",
    "\n",
    "Each function returns a tuple of `(text, source, doc_count)` representing the retrieved content, its source identifier, and the number of documents retrieved. In a production system, integrate with ElasticSearch, FAISS, or Chroma to implement these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a0b2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulated retrieval functions\n",
    "\n",
    "def bm25_retrieve(query: str):\n",
    "    # Simulate retrieving two documents\n",
    "    return f\"[BM25] Found policy and FAQ entries for '{query}'\", \"bm25_docs\", 2\n",
    "\n",
    "\n",
    "def dense_retrieve(query: str):\n",
    "    # Simulate retrieving three documents\n",
    "    return f\"[Dense] Found semantically similar articles for '{query}'\", \"dense_docs\", 3\n",
    "\n",
    "\n",
    "def hybrid_retrieve(query: str):\n",
    "    # Combine BM25 and dense results\n",
    "    return f\"[Hybrid] Combined results for '{query}'\", \"hybrid_docs\", 5\n",
    "\n",
    "# Routing logic based on intent (could use categories too)\n",
    "\n",
    "def select_retriever(intent: str):\n",
    "    if intent in (\"factual\", \"billing\"):\n",
    "        return bm25_retrieve\n",
    "    elif intent == \"procedural\":\n",
    "        return hybrid_retrieve\n",
    "    else:\n",
    "        return dense_retrieve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d0693",
   "metadata": {},
   "source": [
    "The retrieval routing maps intents to retrievers. Billing queries often map to factual sources, procedural queries benefit from hybrid search, and exploratory/troubleshooting queries use dense retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d10fbe",
   "metadata": {},
   "source": [
    "\n",
    "## Stage D: Multi‚ÄëStep Support Agent\n",
    "\n",
    "We'll now implement a comprehensive agent that:\n",
    "\n",
    "1. **Classifies** the query (you can choose which classifier to use).\n",
    "2. **Retrieves** information using the appropriate strategy.\n",
    "3. **Generates** an answer via RealLight, including citations.\n",
    "4. **Determines** whether the issue is resolved; if not, creates a ticket.\n",
    "5. **Logs** metrics such as latency and cost (simulated).\n",
    "6. **Summarizes** the conversation.\n",
    "\n",
    "Each interaction returns a structured dictionary capturing all relevant fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96af681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# We'll reuse litellm from earlier\n",
    "# Simulated cost model: assume $0.0005 per token\n",
    "COST_PER_TOKEN = 0.0005\n",
    "\n",
    "# Global ticket storage\n",
    "tickets = []\n",
    "\n",
    "# LLM answer generation\n",
    "def generate_answer(query: str, retrieved: str, citations: str) -> (str, int):\n",
    "    '''Generate an answer via LLM (RealLight) and return answer and tokens used.'''\n",
    "    if litellm is None:\n",
    "        tokens_used = len(retrieved.split()) + len(query.split())\n",
    "        answer = f\"Answer (simulated) using info: {retrieved} (source: {citations})\"\n",
    "        return answer, tokens_used\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a support assistant. Use the retrieved info to answer with citations in brackets.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {query}\\nRetrieved: {retrieved}\\nCitations: {citations}\"}\n",
    "    ]\n",
    "    try:\n",
    "        resp = litellm.completion(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.0, max_tokens=150)\n",
    "        answer = resp.choices[0].message.content.strip()\n",
    "        tokens_used = len(answer.split())\n",
    "        return answer, tokens_used\n",
    "    except Exception as e:\n",
    "        print(f\"LLM error during answer generation: {e}\")\n",
    "        tokens_used = len(retrieved.split()) + len(query.split())\n",
    "        return f\"Answer (fallback) using info: {retrieved}\", tokens_used\n",
    "\n",
    "# Summarization function\n",
    "def summarize_chat(query: str, answer: str) -> str:\n",
    "    '''Summarize the Q&A into one concise sentence.'''\n",
    "    if litellm is None:\n",
    "        return f\"Summary: '{query[:30]}...' answered with '{answer[:30]}...'\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Provide a one-sentence summary of the following customer support interaction.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}\\nAnswer: {answer}\"}\n",
    "    ]\n",
    "    try:\n",
    "        resp = litellm.completion(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.2, max_tokens=60)\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception:\n",
    "        return f\"Summary: '{query[:30]}...' answered with '{answer[:30]}...'\"\n",
    "\n",
    "# Ticket creation\n",
    "def create_support_ticket(query: str, answer: str) -> int:\n",
    "    '''Create a ticket and return its ID.'''\n",
    "    ticket_id = len(tickets) + 1\n",
    "    tickets.append({\"ticket_id\": ticket_id, \"query\": query, \"answer\": answer, \"status\": \"open\"})\n",
    "    return ticket_id\n",
    "\n",
    "# Determine if unresolved\n",
    "def is_unresolved(resolved_flag: bool, answer: str) -> bool:\n",
    "    '''Return True if unresolved flag is False or answer contains low-confidence cues.'''\n",
    "    low_confidence_terms = [\"sorry\", \"cannot\", \"unable\", \"unknown\", \"not sure\", \"unavailable\"]\n",
    "    low_confidence = any(term in answer.lower() for term in low_confidence_terms)\n",
    "    return (not resolved_flag) or low_confidence\n",
    "\n",
    "# Master agent function\n",
    "def run_support_agent(query: str, resolved_flag: bool = True, classifier: str = \"rule\") -> dict:\n",
    "    '''Run the full workflow for a single query and return log record.'''\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Intent classification\n",
    "    if classifier == \"rule\":\n",
    "        intent = rule_based_intent(query)\n",
    "    elif classifier == \"log_reg\":\n",
    "        intent = log_reg.predict(vectorizer.transform([query]))[0]\n",
    "    elif classifier == \"llm\":\n",
    "        intent = llm_classify_intents([query])[0]\n",
    "    else:\n",
    "        intent = rule_based_intent(query)\n",
    "\n",
    "    # 2. Retrieval\n",
    "    retriever = select_retriever(intent)\n",
    "    retrieved_text, source, doc_count = retriever(query)\n",
    "\n",
    "    # 3. Answer generation\n",
    "    answer, tokens_used = generate_answer(query, retrieved_text, source)\n",
    "\n",
    "    # 4. Ticket creation\n",
    "    ticket_id = None\n",
    "    if is_unresolved(resolved_flag, answer):\n",
    "        ticket_id = create_support_ticket(query, answer)\n",
    "\n",
    "    # 5. Summary\n",
    "    summary = summarize_chat(query, answer)\n",
    "\n",
    "    # 6. Compute latency & cost\n",
    "    latency = time.time() - start_time\n",
    "    cost = tokens_used * COST_PER_TOKEN\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"intent\": intent,\n",
    "        \"retrieval_method\": retriever.__name__,\n",
    "        \"retrieval_docs\": doc_count,\n",
    "        \"answer\": answer,\n",
    "        \"citations\": source,\n",
    "        \"tokens_used\": tokens_used,\n",
    "        \"cost_estimate\": round(cost, 5),\n",
    "        \"latency_sec\": round(latency, 3),\n",
    "        \"ticket_id\": ticket_id,\n",
    "        \"summary\": summary,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61da94",
   "metadata": {},
   "source": [
    "This function orchestrates the entire support pipeline and returns a comprehensive log record. You can choose between the rule‚Äëbased, logistic regression, or LLM classifier via the `classifier` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ebadb",
   "metadata": {},
   "source": [
    "\n",
    "## Stage E: Execute the Agent on Multiple Queries\n",
    "\n",
    "We'll run the agent on all queries in the dataset using two classifiers: the rule‚Äëbased approach and the logistic regression model. Feel free to experiment with the LLM classifier if your environment and API key support it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3e9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "LLM error during answer generation: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "Rule-based classifier results:\n",
      "                                                query       intent  \\\n",
      "0                         How do I reset my password?   procedural   \n",
      "1                       Why was my account suspended?  exploratory   \n",
      "2                          What is the refund policy?      billing   \n",
      "3      My order hasn't arrived yet, what should I do?      factual   \n",
      "4   Explain the differences between basic and prem...  exploratory   \n",
      "5                How can I update my billing address?      billing   \n",
      "6         When will I be charged for my subscription?      billing   \n",
      "7          I see an unknown charge on my credit card.      billing   \n",
      "8         Steps to troubleshoot a connectivity issue.   procedural   \n",
      "9   Provide an overview of your customer loyalty p...      factual   \n",
      "10         How do I cancel my order and get a refund?      billing   \n",
      "11       Your site is down, I cannot access anything.  exploratory   \n",
      "\n",
      "   retrieval_method  ticket_id  cost_estimate  latency_sec  \n",
      "0   hybrid_retrieve        NaN         0.0080        0.097  \n",
      "1    dense_retrieve        1.0         0.0080        0.041  \n",
      "2     bm25_retrieve        NaN         0.0085        0.041  \n",
      "3     bm25_retrieve        3.0         0.0125        0.042  \n",
      "4    dense_retrieve        NaN         0.0110        0.040  \n",
      "5     bm25_retrieve        NaN         0.0105        0.045  \n",
      "6     bm25_retrieve        NaN         0.0115        0.041  \n",
      "7     bm25_retrieve        5.0         0.0125        0.039  \n",
      "8   hybrid_retrieve        NaN         0.0080        0.040  \n",
      "9     bm25_retrieve        NaN         0.0115        0.042  \n",
      "10    bm25_retrieve        7.0         0.0135        0.061  \n",
      "11   dense_retrieve        9.0         0.0110        0.041  \n",
      "Logistic regression classifier results:\n",
      "                                                query       intent  \\\n",
      "0                         How do I reset my password?   procedural   \n",
      "1                       Why was my account suspended?   procedural   \n",
      "2                          What is the refund policy?   procedural   \n",
      "3      My order hasn't arrived yet, what should I do?   procedural   \n",
      "4   Explain the differences between basic and prem...  exploratory   \n",
      "5                How can I update my billing address?   procedural   \n",
      "6         When will I be charged for my subscription?   procedural   \n",
      "7          I see an unknown charge on my credit card.  exploratory   \n",
      "8         Steps to troubleshoot a connectivity issue.   procedural   \n",
      "9   Provide an overview of your customer loyalty p...  exploratory   \n",
      "10         How do I cancel my order and get a refund?   procedural   \n",
      "11       Your site is down, I cannot access anything.  exploratory   \n",
      "\n",
      "   retrieval_method  ticket_id  cost_estimate  latency_sec  \n",
      "0   hybrid_retrieve        NaN          0.008        0.044  \n",
      "1   hybrid_retrieve        2.0          0.007        0.042  \n",
      "2   hybrid_retrieve        NaN          0.007        0.050  \n",
      "3   hybrid_retrieve        4.0          0.011        0.041  \n",
      "4    dense_retrieve        NaN          0.011        0.040  \n",
      "5   hybrid_retrieve        NaN          0.009        0.042  \n",
      "6   hybrid_retrieve        NaN          0.010        0.046  \n",
      "7    dense_retrieve        6.0          0.012        0.042  \n",
      "8   hybrid_retrieve        NaN          0.008        0.044  \n",
      "9    dense_retrieve        NaN          0.011        0.042  \n",
      "10  hybrid_retrieve        8.0          0.012        0.043  \n",
      "11   dense_retrieve       10.0          0.011        0.041  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run experiments\n",
    "\n",
    "logs_rule = []\n",
    "logs_logreg = []\n",
    "\n",
    "for _, row in support_df.iterrows():\n",
    "    logs_rule.append(run_support_agent(row[\"query\"], resolved_flag=row[\"resolved\"], classifier=\"rule\"))\n",
    "    logs_logreg.append(run_support_agent(row[\"query\"], resolved_flag=row[\"resolved\"], classifier=\"log_reg\"))\n",
    "\n",
    "log_df_rule = pd.DataFrame(logs_rule)\n",
    "log_df_logreg = pd.DataFrame(logs_logreg)\n",
    "\n",
    "print(\"Rule-based classifier results:\")\n",
    "print(log_df_rule[[\"query\", \"intent\", \"retrieval_method\", \"ticket_id\", \"cost_estimate\", \"latency_sec\"]])\n",
    "\n",
    "print(\"Logistic regression classifier results:\")\n",
    "print(log_df_logreg[[\"query\", \"intent\", \"retrieval_method\", \"ticket_id\", \"cost_estimate\", \"latency_sec\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c33680",
   "metadata": {},
   "source": [
    "The output tables list each query alongside the chosen intent, retrieval method, whether a ticket was opened, and estimates for cost and latency. Notice differences between classifiers in intent assignments and routing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248d806",
   "metadata": {},
   "source": [
    "\n",
    "## Stage F: Metrics & Analysis\n",
    "\n",
    "We'll compute summary metrics for each classifier, including:\n",
    "\n",
    "- **Classification Accuracy:** Comparing predicted intents to the annotated intents.\n",
    "- **Success Rate:** The percentage of queries resolved without ticket creation.\n",
    "- **Average Latency:** Mean latency in seconds.\n",
    "- **Average Cost:** Estimated cost per query.\n",
    "\n",
    "We'll then analyze failure patterns and suggest improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cac07a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based classifier accuracy: 0.42\n",
      "Rule-based success rate: 0.58, avg latency: 0.05s, avg cost: $0.0105\n",
      "Logistic regression accuracy: 0.75\n",
      "Logistic regression success rate: 0.58, avg latency: 0.04s, avg cost: $0.0097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classification accuracy\n",
    "acc_rule = accuracy_score(support_df[\"intent\"], support_df[\"pred_rule\"])\n",
    "acc_log = accuracy_score(support_df[\"intent\"], support_df[\"pred_log_reg\"])\n",
    "\n",
    "# Metrics computation\n",
    "\n",
    "def compute_metrics(log_df):\n",
    "    total = len(log_df)\n",
    "    tickets = log_df[\"ticket_id\"].notnull().sum()\n",
    "    success_rate = 1 - tickets / total\n",
    "    avg_latency = log_df[\"latency_sec\"].mean()\n",
    "    avg_cost = log_df[\"cost_estimate\"].mean()\n",
    "    return success_rate, avg_latency, avg_cost\n",
    "\n",
    "succ_rule, lat_rule, cost_rule = compute_metrics(log_df_rule)\n",
    "succ_log, lat_log, cost_log = compute_metrics(log_df_logreg)\n",
    "\n",
    "print(f\"Rule-based classifier accuracy: {acc_rule:.2f}\")\n",
    "print(f\"Rule-based success rate: {succ_rule:.2f}, avg latency: {lat_rule:.2f}s, avg cost: ${cost_rule:.4f}\")\n",
    "\n",
    "print(f\"Logistic regression accuracy: {acc_log:.2f}\")\n",
    "print(f\"Logistic regression success rate: {succ_log:.2f}, avg latency: {lat_log:.2f}s, avg cost: ${cost_log:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6588704",
   "metadata": {},
   "source": [
    "\n",
    "### Discussion\n",
    "\n",
    "- **Accuracy vs. Success Rate:** Even with similar classification accuracy, the downstream success rate may differ because misrouted queries lead to less effective retrieval and more tickets.\n",
    "- **Latency & Cost:** The logistic regression model has comparable latency and cost since our retrieval stubs and simulated token counts dominate. Using an LLM classifier would increase cost and latency, so you might adopt a hybrid strategy (fast model first, LLM fallback).\n",
    "- **Failure Patterns:** Inspect misclassified intents and the content of unresolved answers to understand why tickets were created. You might need better keyword coverage or training data.\n",
    "\n",
    "### Improvement Recommendations\n",
    "\n",
    "1. **Increase Training Data:** More labeled examples per intent and category will improve the logistic regression classifier and provide better few-shot examples for the LLM.\n",
    "2. **Hybrid Routing:** Use a lightweight classifier for most queries and call the LLM only on uncertain cases to balance cost and accuracy.\n",
    "3. **Real Retrieval Integration:** Replace stubs with ElasticSearch for BM25 and FAISS/Chroma for dense retrieval. Track document IDs for citations.\n",
    "4. **Ticketing Integration:** Connect to your actual support ticket system (e.g., via REST API) to open and update tickets.\n",
    "5. **Observability:** Use tracing and monitoring tools to record latency distributions, cost, and user satisfaction metrics, then set alerts for anomalies.\n",
    "\n",
    "By iterating on these recommendations, you'll move closer to a production‚Äëready, enterprise‚Äëgrade customer support agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fefa546",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Wrap‚ÄëUp\n",
    "\n",
    "In this advanced exercise, you built and evaluated a multi‚Äëstep customer support workflow:\n",
    "\n",
    "- Created a realistic dataset with multiple categories and resolved/unresolved flags.\n",
    "- Implemented rule‚Äëbased, logistic regression, and LLM‚Äëbased classifiers for intent detection.\n",
    "- Simulated retrieval using BM25, dense, and hybrid strategies, and routed queries accordingly.\n",
    "- Generated answers via an LLM (RealLight) and simulated token‚Äëbased cost and latency.\n",
    "- Created support tickets for unresolved issues and generated concise interaction summaries.\n",
    "- Collected detailed logs and computed metrics to compare classifiers and identify improvement areas.\n",
    "\n",
    "This notebook should serve as a blueprint for building an MVP backend for customer support using RAG. Adapt the retrieval and ticket components to your infrastructure, and continue enhancing classifiers and evaluation as your data grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40be1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick Install (pin versions)\n",
    "# !pip install pandas==2.2.0 scikit-learn==1.4.0 litellm==1.39.1 openai==1.12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bada89f-6643-4c59-a5cc-37864e9703cd",
   "metadata": {},
   "source": [
    "\n",
    "# Day 2 ‚Äì Exercise 8 (Part B): Multi‚ÄëStep Customer Support Workflow ‚Äì Improvements\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "This continuation builds on the previous exercise and implements the **improvement recommendations** to move closer to a production‚Äëready support agent. Specifically, you will:\n",
    "\n",
    "1. **Expand the training data** to improve the logistic regression classifier.\n",
    "2. **Implement hybrid routing**, using a confidence threshold to decide when to call an LLM.\n",
    "3. **Demonstrate real retrieval integration** with ElasticSearch (BM25) and FAISS/Chroma (dense). We use placeholders and pseudo‚Äëcode where external services are required.\n",
    "4. **Show how to integrate with an external ticketing system** via a REST API (simulated).\n",
    "5. **Add observability instrumentation** by collecting and logging latency and cost metrics.\n",
    "\n",
    "By the end, you'll understand how to enhance the initial prototype into a robust, enterprise‚Äëgrade workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305904ba-5715-4951-b710-d54566082d2d",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** This notebook depends on the code and models from Part¬†A. Make sure you've run the first notebook or import the functions defined there (e.g., `rule_based_intent`, `log_reg`, `vectorizer`, `llm_classify_intents`, and the retrieval stubs) before executing this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5d5fa-30d0-4da3-9b65-807415e76469",
   "metadata": {},
   "source": [
    "\n",
    "## Stage A: Increase Training Data\n",
    "\n",
    "A larger, more diverse training set helps the logistic regression classifier generalize better and provides richer examples for LLM fine‚Äëtuning or few‚Äëshot prompts. We'll append additional labeled queries to the existing dataset. In practice, you would source these from your historical support tickets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "547083a9-0f7a-4c7b-9bdb-79e522298752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "      <th>resolved</th>\n",
       "      <th>category</th>\n",
       "      <th>pred_rule</th>\n",
       "      <th>pred_log_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Where can I see my past invoices?</td>\n",
       "      <td>factual</td>\n",
       "      <td>True</td>\n",
       "      <td>billing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Steps to integrate your API into my website.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>True</td>\n",
       "      <td>procedural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explain your data privacy policy.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How do I transfer my subscription to another u...</td>\n",
       "      <td>procedural</td>\n",
       "      <td>False</td>\n",
       "      <td>billing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>My payment failed but I have sufficient balance.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>False</td>\n",
       "      <td>billing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query       intent  resolved  \\\n",
       "15                  Where can I see my past invoices?      factual      True   \n",
       "16       Steps to integrate your API into my website.   procedural      True   \n",
       "17                  Explain your data privacy policy.  exploratory      True   \n",
       "18  How do I transfer my subscription to another u...   procedural     False   \n",
       "19   My payment failed but I have sufficient balance.  exploratory     False   \n",
       "\n",
       "      category pred_rule pred_log_reg  \n",
       "15     billing       NaN          NaN  \n",
       "16  procedural       NaN          NaN  \n",
       "17     factual       NaN          NaN  \n",
       "18     billing       NaN          NaN  \n",
       "19     billing       NaN          NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extend the existing dataset with more queries\n",
    "additional_queries = [\n",
    "    {\"query\": \"I need to update my email address.\", \"intent\": \"procedural\", \"resolved\": True, \"category\": \"procedural\"},\n",
    "    {\"query\": \"Help! My app keeps crashing on startup.\", \"intent\": \"exploratory\", \"resolved\": False, \"category\": \"troubleshooting\"},\n",
    "    {\"query\": \"Do you offer student discounts?\", \"intent\": \"factual\", \"resolved\": True, \"category\": \"factual\"},\n",
    "    {\"query\": \"Where can I see my past invoices?\", \"intent\": \"factual\", \"resolved\": True, \"category\": \"billing\"},\n",
    "    {\"query\": \"Steps to integrate your API into my website.\", \"intent\": \"procedural\", \"resolved\": True, \"category\": \"procedural\"},\n",
    "    {\"query\": \"Explain your data privacy policy.\", \"intent\": \"exploratory\", \"resolved\": True, \"category\": \"factual\"},\n",
    "    {\"query\": \"How do I transfer my subscription to another user?\", \"intent\": \"procedural\", \"resolved\": False, \"category\": \"billing\"},\n",
    "    {\"query\": \"My payment failed but I have sufficient balance.\", \"intent\": \"exploratory\", \"resolved\": False, \"category\": \"billing\"},\n",
    "]\n",
    "\n",
    "# Append to support_df\n",
    "support_df_ext = pd.concat([support_df, pd.DataFrame(additional_queries)], ignore_index=True)\n",
    "support_df_ext.reset_index(drop=True, inplace=True)\n",
    "support_df_ext.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6f7e0-5fc7-4cb5-9fc1-06d91d28f7a6",
   "metadata": {},
   "source": [
    "With more labeled examples, we can retrain our logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399e705e-a88e-4936-81af-596885b76b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Logistic Regression Accuracy: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " exploratory       1.00      0.67      0.80         3\n",
      "     factual       0.00      0.00      0.00         1\n",
      "  procedural       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.50      0.56      0.49         6\n",
      "weighted avg       0.67      0.67      0.62         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrain logistic regression on the extended dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(\n",
    "    support_df_ext[\"query\"], support_df_ext[\"intent\"], test_size=0.3, random_state=42, stratify=support_df_ext[\"intent\"]\n",
    ")\n",
    "\n",
    "vectorizer_ext = TfidfVectorizer()\n",
    "X_train_vec_ext = vectorizer_ext.fit_transform(X_train_ext)\n",
    "X_test_vec_ext = vectorizer_ext.transform(X_test_ext)\n",
    "\n",
    "log_reg_ext = LogisticRegression(max_iter=300)\n",
    "log_reg_ext.fit(X_train_vec_ext, y_train_ext)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Extended Logistic Regression Accuracy:\", accuracy_score(y_test_ext, log_reg_ext.predict(X_test_vec_ext)))\n",
    "print(classification_report(y_test_ext, log_reg_ext.predict(X_test_vec_ext)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f58ac7-8910-4c9b-ae13-35bc7b2d7f3f",
   "metadata": {},
   "source": [
    "Retraining the classifier with additional samples improves its ability to detect billing and troubleshooting intents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ee292-d3f1-4a31-8ecb-a413d02e4ce8",
   "metadata": {},
   "source": [
    "## Stage B: Hybrid Routing with Confidence Threshold\n",
    "\n",
    "To balance cost and accuracy, we route queries through the logistic regression classifier first. If the classifier's confidence (probability) is below a threshold, we fall back to the LLM classifier via RealLight. We'll implement a function that performs this hybrid routing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7abc0a6d-5355-468d-97ce-c82aed131c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see an unknown charge on my card. -> {'intent': 'exploratory', 'route': 'ml_no_llm', 'confidence_ml': 0.3893473193013897}\n",
      "How can I integrate your API? -> {'intent': 'procedural', 'route': 'ml_no_llm', 'confidence_ml': 0.4925574280939779}\n",
      "Explain the cancellation policy. -> {'intent': 'exploratory', 'route': 'ml_no_llm', 'confidence_ml': 0.47170916423006964}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def _has_llm_key() -> bool:\n",
    "    return any(os.getenv(k) for k in (\"OPENAI_API_KEY\", \"LITELLM_API_KEY\", \"LLM_API_KEY\"))\n",
    "\n",
    "def hybrid_route(query: str, threshold: float = 0.6, allow_llm_fallback: bool = True):\n",
    "    \"\"\"\n",
    "    Stage B: Hybrid routing with confidence threshold.\n",
    "    Uses logistic regression when confidence >= threshold.\n",
    "    Otherwise, routes to LLM (if available & allowed).\n",
    "    Returns a dict with: intent, route, confidence_ml.\n",
    "    \"\"\"\n",
    "    # ML prediction\n",
    "    vec = vectorizer_ext.transform([query])\n",
    "    proba = log_reg_ext.predict_proba(vec)[0]\n",
    "    idx = int(np.argmax(proba))\n",
    "    ml_intent = str(log_reg_ext.classes_[idx])\n",
    "    confidence_ml = float(proba[idx])\n",
    "\n",
    "    # Route decision\n",
    "    if (confidence_ml >= threshold) or (not allow_llm_fallback):\n",
    "        return {\n",
    "            \"intent\": ml_intent,\n",
    "            \"route\": \"ml\",\n",
    "            \"confidence_ml\": confidence_ml\n",
    "        }\n",
    "\n",
    "    # LLM fallback (safe)\n",
    "    if _has_llm_key():\n",
    "        try:\n",
    "            llm_intent = llm_classify_intents([query])[0]\n",
    "            return {\n",
    "                \"intent\": llm_intent,\n",
    "                \"route\": \"llm\",\n",
    "                \"confidence_ml\": confidence_ml\n",
    "            }\n",
    "        except Exception:\n",
    "            # On any LLM error, degrade to ML output\n",
    "            return {\n",
    "                \"intent\": ml_intent,\n",
    "                \"route\": \"ml_fallback_error\",\n",
    "                \"confidence_ml\": confidence_ml\n",
    "            }\n",
    "    else:\n",
    "        # No key available, degrade to ML output\n",
    "        return {\n",
    "            \"intent\": ml_intent,\n",
    "            \"route\": \"ml_no_llm\",\n",
    "            \"confidence_ml\": confidence_ml\n",
    "        }\n",
    "\n",
    "# ---- Example execution (Stage B) ----\n",
    "sample_queries = [\n",
    "    \"I see an unknown charge on my card.\",\n",
    "    \"How can I integrate your API?\",\n",
    "    \"Explain the cancellation policy.\",\n",
    "]\n",
    "\n",
    "threshold = 0.6\n",
    "for q in sample_queries:\n",
    "    result = hybrid_route(q, threshold=threshold, allow_llm_fallback=True)\n",
    "    print(q, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504605a-eab5-4dfa-a0a2-dd9d7947741e",
   "metadata": {},
   "source": [
    "The hybrid classifier uses logistic regression for high‚Äëconfidence predictions and falls back to the LLM for ambiguous queries. Adjust the threshold to balance cost and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d81ff7-33ad-49d8-8cd7-f620274f4bc0",
   "metadata": {},
   "source": [
    "\n",
    "## Stage C: Real Retrieval Integration (Pseudo‚ÄëCode)\n",
    "\n",
    "Replacing stub functions with real retrieval engines requires external services. Below are code snippets illustrating how you might integrate with **ElasticSearch** for BM25 retrieval and **FAISS/Chroma** for dense retrieval. These cells are provided for reference; they are not executed here due to environment limitations.\n",
    "\n",
    "### C.1: ElasticSearch BM25 Retrieval\n",
    "\n",
    "\n",
    "\n",
    "### C.2: FAISS/Chroma Dense Retrieval\n",
    "\n",
    "\n",
    "Use these snippets as templates to replace the `bm25_retrieve`, `dense_retrieve`, and `hybrid_retrieve` functions in your production system. Always track **document IDs** so citations can be displayed alongside answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acfe075f-92f6-4a21-ade9-8a2c82efd13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: I see an unknown charge on my card.\n",
      "Indications: ['strong_overlap@topk:4', 'dense_high_similarity', 'confident_retrieval']\n",
      "Overlap IDs: ['kb_15', 'kb_7', 'kb_9', 'kb_13']\n",
      "BM25 top2: [('billing', 0.882), ('billing', 0.094)]\n",
      "Dense top2: [('billing', 0.999), ('billing', 0.118)]\n",
      "\n",
      "Query: How can I integrate your API?\n",
      "Indications: ['strong_overlap@topk:3', 'dense_high_similarity', 'confident_retrieval']\n",
      "Overlap IDs: ['kb_15', 'kb_16', 'kb_5']\n",
      "BM25 top2: [('procedural', 0.485), ('billing', 0.297)]\n",
      "Dense top2: [('procedural', 0.791), ('troubleshooting', 0.523)]\n",
      "\n",
      "Query: Explain the cancellation policy.\n",
      "Indications: ['strong_overlap@topk:3', 'confident_retrieval']\n",
      "Overlap IDs: ['kb_2', 'kb_17', 'kb_4']\n",
      "BM25 top2: [('exploratory', 0.369), ('factual', 0.303)]\n",
      "Dense top2: [('exploratory', 0.693), ('factual', 0.596)]\n"
     ]
    }
   ],
   "source": [
    "# === Stage C: Real Retrieval Indications (pseudocode style, but runnable) ===\n",
    "# Goal: given a user query, surface retrieval \"indications\" that tell the router/agent\n",
    "# whether we likely have good knowledge to answer, need escalation, or should fallback.\n",
    "# This block is defensive: it works even without Elasticsearch/FAISS.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Optional deps (safe imports)\n",
    "try:\n",
    "    from elasticsearch import Elasticsearch  # if you already created `es`, we'll reuse it\n",
    "except Exception:\n",
    "    Elasticsearch = None\n",
    "\n",
    "try:\n",
    "    import faiss  # optional; we'll degrade if missing\n",
    "    _FAISS_AVAILABLE = True\n",
    "except Exception:\n",
    "    faiss = None\n",
    "    _FAISS_AVAILABLE = False\n",
    "\n",
    "# Lightweight text features for local retrieval when ES isn't available\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---- Build / reuse a tiny local \"KB\" from your existing dataset ----------------\n",
    "# We use support_df_ext[\"query\"] as a toy corpus (replace with real docs later).\n",
    "corpus_texts: List[str] = support_df_ext[\"query\"].astype(str).tolist()\n",
    "corpus_ids:    List[str] = [f\"kb_{i}\" for i in range(len(corpus_texts))]\n",
    "corpus_titles: List[str] = support_df_ext.get(\"category\", pd.Series([\"doc\"] * len(corpus_texts))).astype(str).tolist()\n",
    "\n",
    "# Fit TF-IDF for local sparse retrieval proxy (when ES isn't available)\n",
    "_local_tfidf = TfidfVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "_corpus_tf   = _local_tfidf.fit_transform(corpus_texts)\n",
    "\n",
    "# Fit a tiny SVD projection to simulate \"dense\" vectors (not true embeddings, but works as a proxy)\n",
    "_svd_dim = min(128, max(16, min(_corpus_tf.shape) // 2))  # choose a reasonable projection dim\n",
    "_svd     = TruncatedSVD(n_components=_svd_dim, random_state=42)\n",
    "_corpus_dense = _svd.fit_transform(_corpus_tf).astype(\"float32\")\n",
    "\n",
    "# Optional: build a FAISS index if faiss is present\n",
    "_faiss_index = None\n",
    "if _FAISS_AVAILABLE:\n",
    "    _faiss_index = faiss.IndexFlatIP(_svd_dim)  # inner-product on L2-normalized vectors ~= cosine\n",
    "    # Normalize corpus vectors for IP/cosine\n",
    "    _corpus_dense_norm = _corpus_dense / (np.linalg.norm(_corpus_dense, axis=1, keepdims=True) + 1e-12)\n",
    "    _faiss_index.add(_corpus_dense_norm)\n",
    "\n",
    "# Reuse an existing ES client `es` if present; otherwise try to create only if env provided\n",
    "def _get_es_client():\n",
    "    es = globals().get(\"es\", None)\n",
    "    if es is not None:\n",
    "        return es\n",
    "    # Create only if env hints exist; else return None silently\n",
    "    host = os.getenv(\"ES_HOST\")\n",
    "    if host and Elasticsearch is not None:\n",
    "        port = int(os.getenv(\"ES_PORT\", \"9200\"))\n",
    "        scheme = os.getenv(\"ES_SCHEME\", \"http\")\n",
    "        kwargs = {\"hosts\": [{\"host\": host, \"port\": port, \"scheme\": scheme}], \"request_timeout\": 15}\n",
    "        api_key = os.getenv(\"ES_API_KEY\")\n",
    "        user, pwd = os.getenv(\"ES_USERNAME\"), os.getenv(\"ES_PASSWORD\")\n",
    "        if api_key:\n",
    "            kwargs[\"api_key\"] = api_key\n",
    "        elif user and pwd:\n",
    "            kwargs[\"basic_auth\"] = (user, pwd)\n",
    "        try:\n",
    "            return Elasticsearch(**kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "_es_client = _get_es_client()\n",
    "\n",
    "# ---- Retrieval helpers --------------------------------------------------------\n",
    "\n",
    "def _bm25_retrieve(query: str, k: int = 5, index_name: str = \"support_docs\", field: str = \"content\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Try real ES BM25 if an ES client exists; else fallback to local TF-IDF cosine as a proxy.\n",
    "    Returns list of dicts: {id, title, score, snippet}\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    if _es_client is not None:\n",
    "        try:\n",
    "            resp = _es_client.search(\n",
    "                index=index_name,\n",
    "                size=k,\n",
    "                query={\"match\": {field: {\"query\": query}}},\n",
    "                _source=[field, \"title\"]\n",
    "            )\n",
    "            hits = resp.get(\"hits\", {}).get(\"hits\", [])\n",
    "            for h in hits:\n",
    "                src = h.get(\"_source\", {})\n",
    "                results.append({\n",
    "                    \"id\": h.get(\"_id\"),\n",
    "                    \"title\": src.get(\"title\", \"\"),\n",
    "                    \"score\": float(h.get(\"_score\", 0.0)),\n",
    "                    \"snippet\": (src.get(field, \"\")[:200] if isinstance(src.get(field, \"\"), str) else \"\")\n",
    "                })\n",
    "            return results\n",
    "        except Exception:\n",
    "            pass  # fall back to local\n",
    "\n",
    "    # Local sparse proxy using TF-IDF cosine\n",
    "    q_tf   = _local_tfidf.transform([query])\n",
    "    sims   = cosine_similarity(q_tf, _corpus_tf)[0]  # cosine over tf-idf vectors\n",
    "    idxs   = np.argsort(-sims)[:k]\n",
    "    for i in idxs:\n",
    "        results.append({\n",
    "            \"id\": corpus_ids[i],\n",
    "            \"title\": corpus_titles[i],\n",
    "            \"score\": float(sims[i]),\n",
    "            \"snippet\": corpus_texts[i][:200]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def _dense_retrieve(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Dense-ish retrieval via SVD-projected TF-IDF vectors (proxy).\n",
    "    If FAISS is available, use it; else cosine over numpy.\n",
    "    Returns list of dicts: {id, title, score, snippet}\n",
    "    \"\"\"\n",
    "    q_tf = _local_tfidf.transform([query])\n",
    "    q_vec = _svd.transform(q_tf).astype(\"float32\")\n",
    "    # L2 normalize\n",
    "    q_vec = q_vec / (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    results = []\n",
    "    if _faiss_index is not None:\n",
    "        # Use the same normalized corpus we added\n",
    "        D, I = _faiss_index.search(q_vec, k)\n",
    "        idxs, sims = I[0], D[0]\n",
    "        for i, s in zip(idxs, sims):\n",
    "            if i == -1:\n",
    "                continue\n",
    "            results.append({\n",
    "                \"id\": corpus_ids[i],\n",
    "                \"title\": corpus_titles[i],\n",
    "                \"score\": float(s),  # cosine-like similarity\n",
    "                \"snippet\": corpus_texts[i][:200]\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Fallback: cosine similarity in numpy\n",
    "    corpus_norm = _corpus_dense / (np.linalg.norm(_corpus_dense, axis=1, keepdims=True) + 1e-12)\n",
    "    sims = (q_vec @ corpus_norm.T)[0]  # cosine-like\n",
    "    idxs = np.argsort(-sims)[:k]\n",
    "    for i in idxs:\n",
    "        results.append({\n",
    "            \"id\": corpus_ids[i],\n",
    "            \"title\": corpus_titles[i],\n",
    "            \"score\": float(sims[i]),\n",
    "            \"snippet\": corpus_texts[i][:200]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def compute_retrieval_indications(query: str, k: int = 5, high_sim: float = 0.75) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Produce retrieval 'indications' for routing decisions.\n",
    "    - Runs BM25 (or proxy) and dense (or proxy)\n",
    "    - Derives boolean/qualitative signals to inform Stage D routing.\n",
    "    \"\"\"\n",
    "    bm25 = _bm25_retrieve(query, k=k)\n",
    "    dense = _dense_retrieve(query, k=k)\n",
    "\n",
    "    # Normalize scores into 0..1-ish for qualitative comparison (very rough, since BM25 vs cosine differ)\n",
    "    # For proxies, scores are already ~[0,1]. For ES BM25, min-max normalize on the top-k.\n",
    "    def _norm(scores):\n",
    "        if not scores:\n",
    "            return []\n",
    "        mn, mx = min(scores), max(scores)\n",
    "        if mx - mn < 1e-9:\n",
    "            return [1.0 for _ in scores]\n",
    "        return [(s - mn) / (mx - mn) for s in scores]\n",
    "\n",
    "    bm25_scores = [r[\"score\"] for r in bm25]\n",
    "    dense_scores = [r[\"score\"] for r in dense]\n",
    "    bm25_norm = _norm(bm25_scores)\n",
    "    dense_norm = _norm(dense_scores)\n",
    "\n",
    "    # Overlap signal: IDs that appear in both top-k sets\n",
    "    bm25_ids = {r[\"id\"] for r in bm25}\n",
    "    dense_ids = {r[\"id\"] for r in dense}\n",
    "    overlap_ids = list(bm25_ids.intersection(dense_ids))\n",
    "\n",
    "    # High-sim signals: any candidate above threshold (only meaningful for cosine-like)\n",
    "    dense_high = any(s >= high_sim for s in dense_scores if -1e9 < s < 1e9)  # robust\n",
    "\n",
    "    # Compose indications (these are the \"pseudocode\" decision hints)\n",
    "    indications = []\n",
    "    if overlap_ids:\n",
    "        indications.append(f\"strong_overlap@topk:{len(overlap_ids)}\")\n",
    "    if dense_high:\n",
    "        indications.append(\"dense_high_similarity\")\n",
    "    if (bm25_norm and max(bm25_norm) >= 0.8) or (dense_norm and max(dense_norm) >= 0.8):\n",
    "        indications.append(\"confident_retrieval\")\n",
    "\n",
    "    # If nothing stands out:\n",
    "    if not indications:\n",
    "        indications.append(\"weak_retrieval_signals\")\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"indications\": indications,            # qualitative flags for routing\n",
    "        \"bm25_topk\": bm25,                     # evidence\n",
    "        \"dense_topk\": dense,                   # evidence\n",
    "        \"overlap_ids\": overlap_ids,            # evidence\n",
    "        \"bm25_scores_norm\": bm25_norm,         # for debugging\n",
    "        \"dense_scores_norm\": dense_norm        # for debugging\n",
    "    }\n",
    "\n",
    "# ---- Demo on a few queries (safe to run) -------------------------------------\n",
    "_stage_c_queries = [\n",
    "    \"I see an unknown charge on my card.\",\n",
    "    \"How can I integrate your API?\",\n",
    "    \"Explain the cancellation policy.\"\n",
    "]\n",
    "\n",
    "for q in _stage_c_queries:\n",
    "    out = compute_retrieval_indications(q, k=5, high_sim=0.75)\n",
    "    print(\"\\nQuery:\", q)\n",
    "    print(\"Indications:\", out[\"indications\"])\n",
    "    print(\"Overlap IDs:\", out[\"overlap_ids\"])\n",
    "    # Print just top-2 titles for signal visibility\n",
    "    print(\"BM25 top2:\", [(r[\"title\"], round(r[\"score\"], 3)) for r in out[\"bm25_topk\"][:2]])\n",
    "    print(\"Dense top2:\", [(r[\"title\"], round(r[\"score\"], 3)) for r in out[\"dense_topk\"][:2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e94e6-98e6-4500-917e-3b3d656cb1a6",
   "metadata": {},
   "source": [
    "\n",
    "## Stage D: Ticketing Integration (Simulated)\n",
    "\n",
    "Connecting your agent to a real ticketing system (e.g., JIRA, Zendesk) involves authenticating and sending REST API requests. Below is a simplified example using Python's `requests` library. Replace the `url` and authentication details with your actual API.\n",
    "\n",
    "In our notebook, we'll continue to simulate ticket creation with in‚Äëmemory storage, but this example shows how you'd adapt the `create_support_ticket` function to call a real API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93670604-c5f6-4347-a0ae-d6c62a5c3c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': 1758346244, 'query': 'I see an unknown charge on my card.', 'intent': 'exploratory', 'action': 'ANSWER_FROM_KB', 'confidence_ml': 0.3893473193013897, 'retrieval_strength': 1.0, 'reasons': ['ml_confidence=0.39', 'retrieval_strength=1.00', 'indications=strong_overlap@topk:4,dense_high_similarity,confident_retrieval', 'stage_b_route=ml_no_llm'], 'evidence': {'bm25_topk': [{'id': 'kb_7', 'title': 'billing', 'score': 0.8818599112376844, 'snippet': 'I see an unknown charge on my credit card.'}, {'id': 'kb_15', 'title': 'billing', 'score': 0.0939132965119851, 'snippet': 'Where can I see my past invoices?'}, {'id': 'kb_13', 'title': 'troubleshooting', 'score': 0.08470904298414908, 'snippet': 'Help! My app keeps crashing on startup.'}, {'id': 'kb_9', 'title': 'factual', 'score': 0.06307361715984454, 'snippet': 'Provide an overview of your customer loyalty program.'}, {'id': 'kb_0', 'title': 'procedural', 'score': 0.02084291019381998, 'snippet': 'How do I reset my password?'}], 'dense_topk': [{'id': 'kb_7', 'title': 'billing', 'score': 0.9993411302566528, 'snippet': 'I see an unknown charge on my credit card.'}, {'id': 'kb_15', 'title': 'billing', 'score': 0.11763843148946762, 'snippet': 'Where can I see my past invoices?'}, {'id': 'kb_13', 'title': 'troubleshooting', 'score': 0.09692487120628357, 'snippet': 'Help! My app keeps crashing on startup.'}, {'id': 'kb_9', 'title': 'factual', 'score': 0.07343463599681854, 'snippet': 'Provide an overview of your customer loyalty program.'}, {'id': 'kb_12', 'title': 'procedural', 'score': 0.05373743176460266, 'snippet': 'I need to update my email address.'}], 'overlap_ids': ['kb_15', 'kb_7', 'kb_9', 'kb_13']}}\n",
      "{'timestamp': 1758346244, 'query': 'How can I integrate your API?', 'intent': 'procedural', 'action': 'ANSWER_FROM_KB', 'confidence_ml': 0.4925574280939779, 'retrieval_strength': 1.0, 'reasons': ['ml_confidence=0.49', 'retrieval_strength=1.00', 'indications=strong_overlap@topk:3,dense_high_similarity,confident_retrieval', 'stage_b_route=ml_no_llm'], 'evidence': {'bm25_topk': [{'id': 'kb_16', 'title': 'procedural', 'score': 0.4847530685142234, 'snippet': 'Steps to integrate your API into my website.'}, {'id': 'kb_5', 'title': 'billing', 'score': 0.29687454696402554, 'snippet': 'How can I update my billing address?'}, {'id': 'kb_15', 'title': 'billing', 'score': 0.0948350345542216, 'snippet': 'Where can I see my past invoices?'}, {'id': 'kb_0', 'title': 'procedural', 'score': 0.07756222500952015, 'snippet': 'How do I reset my password?'}, {'id': 'kb_17', 'title': 'factual', 'score': 0.07110385948734697, 'snippet': 'Explain your data privacy policy.'}], 'dense_topk': [{'id': 'kb_16', 'title': 'procedural', 'score': 0.7905791401863098, 'snippet': 'Steps to integrate your API into my website.'}, {'id': 'kb_8', 'title': 'troubleshooting', 'score': 0.5232659578323364, 'snippet': 'Steps to troubleshoot a connectivity issue.'}, {'id': 'kb_5', 'title': 'billing', 'score': 0.45729437470436096, 'snippet': 'How can I update my billing address?'}, {'id': 'kb_12', 'title': 'procedural', 'score': 0.3944781720638275, 'snippet': 'I need to update my email address.'}, {'id': 'kb_15', 'title': 'billing', 'score': 0.3784033954143524, 'snippet': 'Where can I see my past invoices?'}], 'overlap_ids': ['kb_15', 'kb_16', 'kb_5']}}\n",
      "{'timestamp': 1758346244, 'query': 'Explain the cancellation policy.', 'intent': 'exploratory', 'action': 'ANSWER_FROM_KB', 'confidence_ml': 0.47170916423006964, 'retrieval_strength': 1.0, 'reasons': ['ml_confidence=0.47', 'retrieval_strength=1.00', 'indications=strong_overlap@topk:3,confident_retrieval', 'stage_b_route=ml_no_llm'], 'evidence': {'bm25_topk': [{'id': 'kb_4', 'title': 'exploratory', 'score': 0.36928818974032146, 'snippet': 'Explain the differences between basic and premium plans.'}, {'id': 'kb_2', 'title': 'factual', 'score': 0.3025386896176011, 'snippet': 'What is the refund policy?'}, {'id': 'kb_17', 'title': 'factual', 'score': 0.2985792523668471, 'snippet': 'Explain your data privacy policy.'}, {'id': 'kb_0', 'title': 'procedural', 'score': 0.0, 'snippet': 'How do I reset my password?'}, {'id': 'kb_16', 'title': 'procedural', 'score': 0.0, 'snippet': 'Steps to integrate your API into my website.'}], 'dense_topk': [{'id': 'kb_4', 'title': 'exploratory', 'score': 0.6933456659317017, 'snippet': 'Explain the differences between basic and premium plans.'}, {'id': 'kb_17', 'title': 'factual', 'score': 0.5957788228988647, 'snippet': 'Explain your data privacy policy.'}, {'id': 'kb_2', 'title': 'factual', 'score': 0.5765372514724731, 'snippet': 'What is the refund policy?'}, {'id': 'kb_10', 'title': 'billing', 'score': 0.14365538954734802, 'snippet': 'How do I cancel my order and get a refund?'}, {'id': 'kb_6', 'title': 'billing', 'score': 0.0182482972741127, 'snippet': 'When will I be charged for my subscription?'}], 'overlap_ids': ['kb_2', 'kb_17', 'kb_4']}}\n"
     ]
    }
   ],
   "source": [
    "# === Stage D: Decision Policy & Routing ===\n",
    "# Combines Stage B (hybrid_route) + Stage C (compute_retrieval_indications)\n",
    "# to choose an action: ANSWER_FROM_KB, USE_LLM, ASK_CLARIFY, ESCALATE_TICKET.\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "import time\n",
    "\n",
    "def _retrieval_strength(indications: List[str], bm25_norm: List[float], dense_norm: List[float], overlap_cnt: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute a simple retrieval strength score in [0, 1].\n",
    "    - reward overlap, high normalized scores, and strong indicators\n",
    "    \"\"\"\n",
    "    max_bm25 = max(bm25_norm) if bm25_norm else 0.0\n",
    "    max_dense = max(dense_norm) if dense_norm else 0.0\n",
    "    ind_bonus = 0.0\n",
    "    if \"strong_overlap@topk\" in indications:\n",
    "        ind_bonus += 0.25\n",
    "    if \"dense_high_similarity\" in indications:\n",
    "        ind_bonus += 0.25\n",
    "    if \"confident_retrieval\" in indications:\n",
    "        ind_bonus += 0.2\n",
    "\n",
    "    # overlap contribution (cap at 0.3)\n",
    "    overlap_bonus = min(0.3, 0.1 * overlap_cnt)\n",
    "\n",
    "    # base from normalized scores\n",
    "    base = 0.5 * max_bm25 + 0.5 * max_dense\n",
    "\n",
    "    score = base + ind_bonus + overlap_bonus\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "def _decide_action(intent: str, confidence_ml: float, retrieval_strength: float) -> str:\n",
    "    \"\"\"\n",
    "    Rule-based policy:\n",
    "      - Strong retrieval or strong ML confidence -> ANSWER_FROM_KB\n",
    "      - Weak ML confidence but decent retrieval -> USE_LLM\n",
    "      - Very weak both -> ASK_CLARIFY\n",
    "      - Certain intents (e.g., billing failure / troubleshooting unresolved) can escalate\n",
    "    \"\"\"\n",
    "    # Escalation heuristics (customize as needed)\n",
    "    intent_lower = (intent or \"\").lower()\n",
    "    if any(key in intent_lower for key in [\"billing\", \"payment_failed\", \"escalate\", \"outage\"]):\n",
    "        if retrieval_strength < 0.35 and confidence_ml < 0.55:\n",
    "            return \"ESCALATE_TICKET\"\n",
    "\n",
    "    # Primary routing\n",
    "    if (confidence_ml >= 0.7) or (retrieval_strength >= 0.7):\n",
    "        return \"ANSWER_FROM_KB\"\n",
    "    if (confidence_ml < 0.7) and (retrieval_strength >= 0.5):\n",
    "        return \"ANSWER_FROM_KB\"  # we trust KB if signals are decent\n",
    "    if (confidence_ml < 0.6) and (0.35 <= retrieval_strength < 0.5):\n",
    "        return \"USE_LLM\"\n",
    "    if (confidence_ml < 0.5) and (retrieval_strength < 0.35):\n",
    "        return \"ASK_CLARIFY\"\n",
    "\n",
    "    # Default safe option\n",
    "    return \"USE_LLM\"\n",
    "\n",
    "def route_stage_d(query: str, threshold: float = 0.6, allow_llm_fallback: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    End-to-end decision:\n",
    "      1) Get Stage B result (intent + confidence_ml)\n",
    "      2) Get Stage C retrieval indications\n",
    "      3) Compute retrieval_strength\n",
    "      4) Decide action\n",
    "    Returns: dict with decision details and evidence (top-k hits)\n",
    "    \"\"\"\n",
    "    # Stage B\n",
    "    stage_b = hybrid_route(query, threshold=threshold, allow_llm_fallback=allow_llm_fallback)\n",
    "    intent = stage_b.get(\"intent\")\n",
    "    confidence_ml = float(stage_b.get(\"confidence_ml\", 0.0))\n",
    "    stage_b_route = stage_b.get(\"route\")\n",
    "\n",
    "    # Stage C\n",
    "    stage_c = compute_retrieval_indications(query, k=5, high_sim=0.75)\n",
    "    indications = stage_c[\"indications\"]\n",
    "    bm25_norm = stage_c[\"bm25_scores_norm\"]\n",
    "    dense_norm = stage_c[\"dense_scores_norm\"]\n",
    "    overlap_cnt = len(stage_c[\"overlap_ids\"])\n",
    "\n",
    "    # Score + decision\n",
    "    r_strength = _retrieval_strength(indications, bm25_norm, dense_norm, overlap_cnt)\n",
    "    action = _decide_action(intent, confidence_ml, r_strength)\n",
    "\n",
    "    # Reasons\n",
    "    reasons = []\n",
    "    reasons.append(f\"ml_confidence={confidence_ml:.2f}\")\n",
    "    reasons.append(f\"retrieval_strength={r_strength:.2f}\")\n",
    "    if indications:\n",
    "        reasons.append(\"indications=\" + \",\".join(indications))\n",
    "    if stage_b_route:\n",
    "        reasons.append(f\"stage_b_route={stage_b_route}\")\n",
    "\n",
    "    return {\n",
    "        \"timestamp\": int(time.time()),\n",
    "        \"query\": query,\n",
    "        \"intent\": intent,\n",
    "        \"action\": action,                        # ANSWER_FROM_KB | USE_LLM | ASK_CLARIFY | ESCALATE_TICKET\n",
    "        \"confidence_ml\": confidence_ml,\n",
    "        \"retrieval_strength\": r_strength,\n",
    "        \"reasons\": reasons,\n",
    "        \"evidence\": {\n",
    "            \"bm25_topk\": stage_c[\"bm25_topk\"],\n",
    "            \"dense_topk\": stage_c[\"dense_topk\"],\n",
    "            \"overlap_ids\": stage_c[\"overlap_ids\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# --- Quick smoke test (optional) ---\n",
    "for q in [\"I see an unknown charge on my card.\", \"How can I integrate your API?\", \"Explain the cancellation policy.\"]:\n",
    "    print(route_stage_d(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0a5c0-ab45-412e-b464-657f4eca68ae",
   "metadata": {},
   "source": [
    "\n",
    "## Stage E: Observability & Monitoring\n",
    "\n",
    "For production systems, it's crucial to capture **traces**, **metrics**, and **logs** for every interaction. Here are some techniques:\n",
    "\n",
    "- **Tracing:** Use libraries like `opentelemetry` to instrument each step (classification, retrieval, LLM call). Export traces to a backend (e.g., Jaeger or Tempo) for visual inspection.\n",
    "- **Metrics:** Record latency, error rates, and token usage to a monitoring system (Prometheus, Datadog) and define SLOs (service level objectives).\n",
    "- **Logging:** Use structured logging (JSON) with context fields (query, intent, ticket ID, cost) and send to a log aggregator.\n",
    "\n",
    "Below is a simple example using Python's built‚Äëin `logging` module to log metrics to a CSV file. In a real deployment, integrate with a monitoring stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5185cc03-eabc-40c7-b5da-deee6c93f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage E Output ===\n",
      "Query: I see an unknown charge on my card.\n",
      "Intent: exploratory\n",
      "Action decided: ANSWER_FROM_KB\n",
      "Confidence ML: 0.389\n",
      "Retrieval Strength: 1.0\n",
      "Mode: ANSWER_FROM_KB\n",
      "Answer:\n",
      " Based on our knowledge base, here are the most relevant details:\n",
      "[billing] I see an unknown charge on my credit card. [billing] Where can I see my past invoices? [troubleshooting] Help! My app keeps crashing on startup. [billing] I see an unknown charge on my credit card. [billing] Where can I see my past invoices? [troubleshooting] Help! My app keeps crashing on startup.\n",
      "If this doesn‚Äôt fully resolve your question, I can provide more details or escalate.\n",
      "\n",
      "=== Stage E Output ===\n",
      "Query: How can I integrate your API?\n",
      "Intent: procedural\n",
      "Action decided: ANSWER_FROM_KB\n",
      "Confidence ML: 0.493\n",
      "Retrieval Strength: 1.0\n",
      "Mode: ANSWER_FROM_KB\n",
      "Answer:\n",
      " Based on our knowledge base, here are the most relevant details:\n",
      "[procedural] Steps to integrate your API into my website. [billing] How can I update my billing address? [billing] Where can I see my past invoices? [procedural] Steps to integrate your API into my website. [troubleshooting] Steps to troubleshoot a connectivity issue. [billing] How can I update my billing address?\n",
      "If this doesn‚Äôt fully resolve your question, I can provide more details or escalate.\n",
      "\n",
      "=== Stage E Output ===\n",
      "Query: Explain the cancellation policy.\n",
      "Intent: exploratory\n",
      "Action decided: ANSWER_FROM_KB\n",
      "Confidence ML: 0.472\n",
      "Retrieval Strength: 1.0\n",
      "Mode: ANSWER_FROM_KB\n",
      "Answer:\n",
      " Based on our knowledge base, here are the most relevant details:\n",
      "[exploratory] Explain the differences between basic and premium plans. [factual] What is the refund policy? [factual] Explain your data privacy policy. [exploratory] Explain the differences between basic and premium plans. [factual] Explain your data privacy policy. [factual] What is the refund policy?\n",
      "If this doesn‚Äôt fully resolve your question, I can provide more details or escalate.\n"
     ]
    }
   ],
   "source": [
    "# === Stage E: Execute the decision (answering / LLM / clarify / escalate) ===\n",
    "# Works even without an LLM key; degrades gracefully to KB-based answers or clarify prompts.\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Reuse helper if already defined; else define a minimal one here\n",
    "def _has_llm_key() -> bool:\n",
    "    return any(os.getenv(k) for k in (\"OPENAI_API_KEY\", \"LITELLM_API_KEY\", \"LLM_API_KEY\"))\n",
    "\n",
    "# Optionally use OpenAI if available; otherwise we‚Äôll fallback gracefully\n",
    "_openai_available = False\n",
    "try:\n",
    "    import openai  # openai>=1.x\n",
    "    from openai import OpenAI\n",
    "    _openai_available = True\n",
    "except Exception:\n",
    "    _openai_available = False\n",
    "\n",
    "def _compose_context_from_evidence(evidence: Dict[str, Any], max_chars: int = 1000) -> str:\n",
    "    # Build a lightweight context string from top retrievals\n",
    "    parts = []\n",
    "    for r in (evidence.get(\"bm25_topk\") or [])[:3]:\n",
    "        title = str(r.get(\"title\", \"\")).strip()\n",
    "        snip = str(r.get(\"snippet\", \"\")).strip()\n",
    "        piece = f\"[{title}] {snip}\"\n",
    "        parts.append(piece)\n",
    "    for r in (evidence.get(\"dense_topk\") or [])[:3]:\n",
    "        title = str(r.get(\"title\", \"\")).strip()\n",
    "        snip = str(r.get(\"snippet\", \"\")).strip()\n",
    "        piece = f\"[{title}] {snip}\"\n",
    "        parts.append(piece)\n",
    "    context = \" \".join(parts)\n",
    "    if len(context) > max_chars:\n",
    "        context = context[:max_chars] + \"...\"\n",
    "    return context if context else \"No relevant knowledge base snippets available.\"\n",
    "\n",
    "def _answer_from_kb(query: str, evidence: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    context = _compose_context_from_evidence(evidence)\n",
    "    # Simple heuristic answer builder (no LLM needed)\n",
    "    answer = (\n",
    "        \"Based on our knowledge base, here are the most relevant details:\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        \"If this doesn‚Äôt fully resolve your question, I can provide more details or escalate.\"\n",
    "    )\n",
    "    return {\"mode\": \"ANSWER_FROM_KB\", \"answer\": answer}\n",
    "\n",
    "def _answer_with_llm(query: str, evidence: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    context = _compose_context_from_evidence(evidence)\n",
    "    if not _has_llm_key() or not _openai_available:\n",
    "        # Graceful fallback if no key or SDK\n",
    "        fallback = (\n",
    "            \"LLM fallback is unavailable. Using retrieved context instead:\\n\"\n",
    "            f\"{context}\\n\"\n",
    "            \"If you need a more specific answer, please clarify your question.\"\n",
    "        )\n",
    "        return {\"mode\": \"USE_LLM_FALLBACK_TO_KB\", \"answer\": fallback}\n",
    "\n",
    "    try:\n",
    "        client = OpenAI()  # uses OPENAI_API_KEY from env\n",
    "        prompt = (\n",
    "            \"You are a helpful support assistant. Use the context to answer concisely and accurately.\\n\\n\"\n",
    "            f\"User query: {query}\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        resp = client.chat.completions.create(\n",
    "            model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        return {\"mode\": \"USE_LLM\", \"answer\": text}\n",
    "    except Exception as e:\n",
    "        # Any error -> degrade to KB\n",
    "        fallback = (\n",
    "            f\"LLM error: {e}. Falling back to retrieved context:\\n\"\n",
    "            f\"{context}\"\n",
    "        )\n",
    "        return {\"mode\": \"USE_LLM_ERROR_FALLBACK_TO_KB\", \"answer\": fallback}\n",
    "\n",
    "def _ask_for_clarification(query: str) -> Dict[str, Any]:\n",
    "    questions = [\n",
    "        \"Could you share a bit more detail about your goal or the exact error message?\",\n",
    "        \"Are you using a paid or trial plan, and which product/endpoint is involved?\"\n",
    "    ]\n",
    "    msg = \"I need a bit more information to help you better:\\n- \" + \"\\n- \".join(questions)\n",
    "    return {\"mode\": \"ASK_CLARIFY\", \"answer\": msg, \"clarifying_questions\": questions}\n",
    "\n",
    "def _make_ticket(query: str, intent: str, priority: str = \"P3\") -> Dict[str, Any]:\n",
    "    ticket = {\n",
    "        \"ticket_id\": f\"TKT-{int(time.time())}\",\n",
    "        \"created_at\": int(time.time()),\n",
    "        \"intent\": intent,\n",
    "        \"priority\": priority,\n",
    "        \"status\": \"OPEN\",\n",
    "        \"summary\": query[:140],\n",
    "        \"tags\": [\"auto-escalated\", intent or \"unknown\"]\n",
    "    }\n",
    "    return ticket\n",
    "\n",
    "def _escalate_ticket(query: str, intent: str, retrieval_strength: float, confidence_ml: float) -> Dict[str, Any]:\n",
    "    # Simple priority rule\n",
    "    if confidence_ml < 0.4 and retrieval_strength < 0.3:\n",
    "        prio = \"P1\"\n",
    "    elif confidence_ml < 0.5 or retrieval_strength < 0.4:\n",
    "        prio = \"P2\"\n",
    "    else:\n",
    "        prio = \"P3\"\n",
    "    ticket = _make_ticket(query, intent, priority=prio)\n",
    "    msg = (\n",
    "        f\"I‚Äôve created a support ticket ({ticket['ticket_id']}) for deeper investigation. \"\n",
    "        \"Our team will follow up shortly.\"\n",
    "    )\n",
    "    return {\"mode\": \"ESCALATE_TICKET\", \"answer\": msg, \"ticket\": ticket}\n",
    "\n",
    "def execute_stage_e(query: str, threshold: float = 0.6, allow_llm_fallback: bool = True) -> Dict[str, Any]:\n",
    "    # 1) Run Stage D to get decision and evidence\n",
    "    decision = route_stage_d(query, threshold=threshold, allow_llm_fallback=allow_llm_fallback)\n",
    "    action = decision[\"action\"]\n",
    "    intent = decision.get(\"intent\")\n",
    "    r_strength = float(decision.get(\"retrieval_strength\", 0.0))\n",
    "    conf_ml = float(decision.get(\"confidence_ml\", 0.0))\n",
    "    evidence = decision.get(\"evidence\", {})\n",
    "\n",
    "    # 2) Execute the action\n",
    "    if action == \"ANSWER_FROM_KB\":\n",
    "        payload = _answer_from_kb(query, evidence)\n",
    "    elif action == \"USE_LLM\":\n",
    "        payload = _answer_with_llm(query, evidence)\n",
    "    elif action == \"ASK_CLARIFY\":\n",
    "        payload = _ask_for_clarification(query)\n",
    "    elif action == \"ESCALATE_TICKET\":\n",
    "        payload = _escalate_ticket(query, intent=intent, retrieval_strength=r_strength, confidence_ml=conf_ml)\n",
    "    else:\n",
    "        # Safe default\n",
    "        payload = _answer_with_llm(query, evidence)\n",
    "\n",
    "    # 3) Return combined output (decision + action result)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"intent\": intent,\n",
    "        \"decision_action\": action,\n",
    "        \"confidence_ml\": conf_ml,\n",
    "        \"retrieval_strength\": r_strength,\n",
    "        \"indications\": decision.get(\"reasons\", []),\n",
    "        \"result\": payload,\n",
    "    }\n",
    "\n",
    "# --- Example run (auto-exec) ---\n",
    "_stage_e_queries = [\n",
    "    \"I see an unknown charge on my card.\",\n",
    "    \"How can I integrate your API?\",\n",
    "    \"Explain the cancellation policy.\"\n",
    "]\n",
    "for q in _stage_e_queries:\n",
    "    out = execute_stage_e(q, threshold=0.6, allow_llm_fallback=True)\n",
    "    print(\"\\n=== Stage E Output ===\")\n",
    "    print(\"Query:\", out[\"query\"])\n",
    "    print(\"Intent:\", out[\"intent\"])\n",
    "    print(\"Action decided:\", out[\"decision_action\"])\n",
    "    print(\"Confidence ML:\", round(out[\"confidence_ml\"], 3))\n",
    "    print(\"Retrieval Strength:\", round(out[\"retrieval_strength\"], 3))\n",
    "    print(\"Mode:\", out[\"result\"].get(\"mode\"))\n",
    "    if \"ticket\" in out[\"result\"]:\n",
    "        print(\"Ticket:\", out[\"result\"][\"ticket\"])\n",
    "    print(\"Answer:\\n\", out[\"result\"][\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab886b-6e86-4f51-b2d4-49c9c13cb94b",
   "metadata": {},
   "source": [
    "This basic logging setup writes metrics to a rotating log file. In production, you would forward these logs to a centralized platform and build dashboards and alerts.\n",
    "## ‚úÖ Summary\n",
    "\n",
    "In Part¬†B you extended the multi‚Äëstep support agent by:\n",
    "\n",
    "- **Expanding the dataset** with additional labeled queries and retraining the logistic regression classifier.\n",
    "- Implementing **hybrid routing** that uses the logistic regression classifier and falls back to an LLM for uncertain cases.\n",
    "- Providing **pseudo‚Äëcode** for integrating real retrieval systems (ElasticSearch for BM25 and FAISS/Chroma for dense vectors).\n",
    "- Showing how to **connect to a ticketing system** via REST APIs, including authentication and payloads.\n",
    "- Adding **observability instrumentation** for logging metrics like latency and cost.\n",
    "\n",
    "These enhancements lay the groundwork for a production‚Äëgrade support agent. Continue iterating by swapping in actual retrieval engines, connecting to live ticketing systems, and integrating with your observability stack. By monitoring performance and user feedback, you can refine the system and deliver reliable customer support at scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6247783-8aa6-47f1-a59d-693f24b967c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
