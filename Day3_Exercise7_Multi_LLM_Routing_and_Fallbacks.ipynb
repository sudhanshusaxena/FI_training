{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Day 3 ‚Äî Exercise 7: Multi-LLM Routing and Fallbacks\n",
    "## Practical Hands-on Implementation with Intelligent Model Selection\n",
    "\n",
    "### ‚úÖ Objectives:\n",
    "- Build intelligent LLM routing system based on query complexity\n",
    "- Implement dynamic model selection with fallbacks\n",
    "- Create cost optimization and performance tracking\n",
    "- Demonstrate working LLM routing with real-time interaction\n",
    "- Show practical enterprise applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "‚úÖ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-core\n",
    "!pip install -q gradio\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API Key configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "print(\"‚úÖ OpenAI API Key configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Multi-LLM Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_9412/1710044252.py:10: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-LLM Router initialized!\n",
      "üìä Available models: 3\n",
      "üìä Models: ['fast_model', 'balanced_model', 'quality_model']\n",
      "üìä Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "class MultiLLMRouter:\n",
    "    def __init__(self):\n",
    "        # Define different LLM configurations (simulating different models)\n",
    "        self.models = {\n",
    "            \"fast_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n",
    "                \"cost_per_token\": 0.0001,\n",
    "                \"speed\": 0.5,  # seconds\n",
    "                \"quality\": 0.7,\n",
    "                \"use_case\": \"Simple queries, quick responses\"\n",
    "            },\n",
    "            \"balanced_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.5, max_tokens=200),\n",
    "                \"cost_per_token\": 0.0002,\n",
    "                \"speed\": 1.0,  # seconds\n",
    "                \"quality\": 0.8,\n",
    "                \"use_case\": \"Medium complexity, balanced performance\"\n",
    "            },\n",
    "            \"quality_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.7, max_tokens=500),\n",
    "                \"cost_per_token\": 0.0005,\n",
    "                \"speed\": 2.0,  # seconds\n",
    "                \"quality\": 0.9,\n",
    "                \"use_case\": \"Complex queries, high quality responses\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.routing_history = []\n",
    "        self.cost_tracker = {\"total_cost\": 0, \"requests\": 0}\n",
    "    \n",
    "    def analyze_query_complexity(self, query: str) -> dict:\n",
    "        \"\"\"Analyze query complexity to determine best model.\"\"\"\n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Length factor\n",
    "        if len(query) > 100:\n",
    "            complexity_score += 0.3\n",
    "        elif len(query) > 50:\n",
    "            complexity_score += 0.1\n",
    "        \n",
    "        # Complexity keywords\n",
    "        complex_keywords = [\"analyze\", \"compare\", \"evaluate\", \"complex\", \"detailed\", \"comprehensive\", \"research\"]\n",
    "        if any(keyword in query.lower() for keyword in complex_keywords):\n",
    "            complexity_score += 0.4\n",
    "        \n",
    "        # Question complexity\n",
    "        if \"?\" in query:\n",
    "            complexity_score += 0.2\n",
    "        \n",
    "        # Technical terms\n",
    "        technical_terms = [\"algorithm\", \"architecture\", \"framework\", \"methodology\", \"implementation\"]\n",
    "        if any(term in query.lower() for term in technical_terms):\n",
    "            complexity_score += 0.3\n",
    "        \n",
    "        return {\n",
    "            \"score\": complexity_score,\n",
    "            \"category\": \"simple\" if complexity_score < 0.3 else \"medium\" if complexity_score < 0.6 else \"complex\"\n",
    "        }\n",
    "    \n",
    "    def select_model(self, complexity: dict) -> str:\n",
    "        \"\"\"Select best model based on complexity analysis.\"\"\"\n",
    "        if complexity[\"category\"] == \"simple\":\n",
    "            return \"fast_model\"\n",
    "        elif complexity[\"category\"] == \"medium\":\n",
    "            return \"balanced_model\"\n",
    "        else:\n",
    "            return \"quality_model\"\n",
    "    \n",
    "    def route_query(self, query: str) -> dict:\n",
    "        \"\"\"Route query to appropriate LLM with fallback.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        complexity = self.analyze_query_complexity(query)\n",
    "        \n",
    "        # Select primary model\n",
    "        primary_model = self.select_model(complexity)\n",
    "        \n",
    "        try:\n",
    "            # Try primary model first\n",
    "            model_config = self.models[primary_model]\n",
    "            \n",
    "            # Simulate model response with occasional failures\n",
    "            if random.random() < 0.1:  # 10% failure rate for demo\n",
    "                raise Exception(f\"Primary model {primary_model} temporarily unavailable\")\n",
    "            \n",
    "            # Get response from primary model\n",
    "            response = model_config[\"llm\"].invoke(query)\n",
    "            \n",
    "            # Calculate costs and metrics\n",
    "            estimated_tokens = len(query.split()) + len(response.split())\n",
    "            cost = estimated_tokens * model_config[\"cost_per_token\"]\n",
    "            \n",
    "            # Update cost tracker\n",
    "            self.cost_tracker[\"total_cost\"] += cost\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Log routing decision\n",
    "            routing_info = {\n",
    "                \"query\": query,\n",
    "                \"complexity\": complexity,\n",
    "                \"selected_model\": primary_model,\n",
    "                \"fallback_used\": False,\n",
    "                \"response\": response,\n",
    "                \"cost\": cost,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            self.routing_history.append(routing_info)\n",
    "            \n",
    "            return routing_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to alternative model\n",
    "            print(f\"‚ö†Ô∏è Primary model failed: {str(e)}\")\n",
    "            \n",
    "            # Select fallback model\n",
    "            if primary_model == \"fast_model\":\n",
    "                fallback_model = \"balanced_model\"\n",
    "            elif primary_model == \"balanced_model\":\n",
    "                fallback_model = \"quality_model\"\n",
    "            else:\n",
    "                fallback_model = \"fast_model\"\n",
    "            \n",
    "            try:\n",
    "                fallback_config = self.models[fallback_model]\n",
    "                response = fallback_config[\"llm\"].invoke(query)\n",
    "                \n",
    "                estimated_tokens = len(query.split()) + len(response.split())\n",
    "                cost = estimated_tokens * fallback_config[\"cost_per_token\"]\n",
    "                \n",
    "                self.cost_tracker[\"total_cost\"] += cost\n",
    "                self.cost_tracker[\"requests\"] += 1\n",
    "                \n",
    "                routing_info = {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": fallback_model,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": response,\n",
    "                    \"cost\": cost,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                \n",
    "                self.routing_history.append(routing_info)\n",
    "                \n",
    "                return routing_info\n",
    "                \n",
    "            except Exception as e2:\n",
    "                # Final fallback - return error message\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": None,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": f\"All models are currently unavailable. Error: {str(e2)}\",\n",
    "                    \"cost\": 0,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"error\": True\n",
    "                }\n",
    "    \n",
    "    def get_routing_stats(self):\n",
    "        \"\"\"Get routing statistics.\"\"\"\n",
    "        if not self.routing_history:\n",
    "            return \"No queries processed yet\"\n",
    "        \n",
    "        total_queries = len(self.routing_history)\n",
    "        successful_queries = len([r for r in self.routing_history if not r.get(\"error\", False)])\n",
    "        fallback_usage = len([r for r in self.routing_history if r.get(\"fallback_used\", False)])\n",
    "        \n",
    "        model_usage = {}\n",
    "        for routing in self.routing_history:\n",
    "            model = routing.get(\"selected_model\", \"unknown\")\n",
    "            model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"successful_queries\": successful_queries,\n",
    "            \"success_rate\": successful_queries / total_queries * 100,\n",
    "            \"fallback_usage\": fallback_usage,\n",
    "            \"fallback_rate\": fallback_usage / total_queries * 100,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"total_cost\": self.cost_tracker[\"total_cost\"],\n",
    "            \"avg_cost_per_query\": self.cost_tracker[\"total_cost\"] / total_queries\n",
    "        }\n",
    "\n",
    "# Initialize router\n",
    "router = MultiLLMRouter()\n",
    "\n",
    "print(\"‚úÖ Multi-LLM Router initialized!\")\n",
    "print(f\"üìä Available models: {len(router.models)}\")\n",
    "print(f\"üìä Models: {list(router.models.keys())}\")\n",
    "print(f\"üìä Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test LLM Routing System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ TESTING LLM ROUTING SYSTEM:\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello ---\n",
      "Complexity: simple (score: 0.00)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: , I am looking for a reliable and experienced writer who can write high-quality articles and web con...\n",
      "Cost: $0.0086\n",
      "Response Time: 1.90s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 2: What is machine learning? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Machine learning is a subset of artificial intelligence that involves the use of algorithms and st...\n",
      "Cost: $0.0094\n",
      "Response Time: 1.84s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 3: Analyze the comprehensive methodology for implemen... ---\n",
      "Complexity: complex (score: 1.00)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Distributed machine learning algorithms are becoming increasingly popular due to their ability to ...\n",
      "Cost: $0.2215\n",
      "Response Time: 4.81s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 4: How are you? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "I am an AI language model created by OpenAI, so I do not have the ability to feel emotions. But th...\n",
      "Cost: $0.0035\n",
      "Response Time: 1.33s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 5: Compare and evaluate different deep learning frame... ---\n",
      "Complexity: complex (score: 0.80)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Deep learning frameworks have become an essential tool for natural language processing tasks, as t...\n",
      "Cost: $0.2135\n",
      "Response Time: 4.40s\n",
      "‚úÖ Success\n",
      "\n",
      "üìä ROUTING STATISTICS:\n",
      "============================================================\n",
      "Total Queries: 5\n",
      "Success Rate: 100.0%\n",
      "Fallback Rate: 0.0%\n",
      "Total Cost: $0.4565\n",
      "Average Cost per Query: $0.0913\n",
      "Model Usage: {'fast_model': 3, 'quality_model': 2}\n"
     ]
    }
   ],
   "source": [
    "# Test routing system with different query complexities\n",
    "test_queries = [\n",
    "    \"Hello\",  # Simple\n",
    "    \"What is machine learning?\",  # Medium\n",
    "    \"Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments\",  # Complex\n",
    "    \"How are you?\",  # Simple\n",
    "    \"Compare and evaluate different deep learning frameworks for natural language processing tasks\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"üîÑ TESTING LLM ROUTING SYSTEM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query[:50]}{'...' if len(query) > 50 else ''} ---\")\n",
    "    \n",
    "    result = router.route_query(query)\n",
    "    \n",
    "    print(f\"Complexity: {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\")\n",
    "    print(f\"Selected Model: {result['selected_model']}\")\n",
    "    print(f\"Fallback Used: {result['fallback_used']}\")\n",
    "    print(f\"Response: {result['response'][:100]}...\")\n",
    "    print(f\"Cost: ${result['cost']:.4f}\")\n",
    "    print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"‚ùå Error: {result.get('error')}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Success\")\n",
    "\n",
    "# Show routing statistics\n",
    "print(f\"\\nüìä ROUTING STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "stats = router.get_routing_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
    "print(f\"Fallback Rate: {stats['fallback_rate']:.1f}%\")\n",
    "print(f\"Total Cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"Average Cost per Query: ${stats['avg_cost_per_query']:.4f}\")\n",
    "print(f\"Model Usage: {stats['model_usage']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interactive LLM Routing Demo with Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive LLM Router ready!\n",
      "üìä Router: MultiLLMRouter\n",
      "üìä Models: 3 available\n",
      "üìä Routing: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create interactive LLM routing system\n",
    "class InteractiveLLMRouter:\n",
    "    def __init__(self):\n",
    "        self.router = router\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_query(self, query, history):\n",
    "        \"\"\"Process query through LLM routing system.\"\"\"\n",
    "        if not query.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Get routed response\n",
    "        result = self.router.route_query(query)\n",
    "        \n",
    "        # Format response for display\n",
    "        if result.get('error'):\n",
    "            response = f\"‚ùå **Error:** {result['response']}\"\n",
    "        else:\n",
    "            response = f\"\"\"**LLM Response:**\n",
    "{result['response']}\n",
    "\n",
    "**Routing Details:**\n",
    "‚Ä¢ **Complexity:** {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\n",
    "‚Ä¢ **Model Used:** {result['selected_model']}\n",
    "‚Ä¢ **Fallback Used:** {'Yes' if result['fallback_used'] else 'No'}\n",
    "‚Ä¢ **Cost:** ${result['cost']:.4f}\n",
    "‚Ä¢ **Response Time:** {result['response_time']:.2f}s\"\"\"\n",
    "        \n",
    "        # Update history\n",
    "        history.append([query, response])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    def get_system_stats(self):\n",
    "        \"\"\"Get current system statistics.\"\"\"\n",
    "        stats = self.router.get_routing_stats()\n",
    "        if isinstance(stats, str):\n",
    "            return \"üìä LLM Router: Ready for queries\"\n",
    "        \n",
    "        return f\"üìä LLM Router: {stats['total_queries']} queries | {stats['success_rate']:.1f}% success | ${stats['total_cost']:.4f} total cost\"\n",
    "\n",
    "# Initialize interactive system\n",
    "interactive_router = InteractiveLLMRouter()\n",
    "\n",
    "print(\"‚úÖ Interactive LLM Router ready!\")\n",
    "print(f\"üìä Router: {type(router).__name__}\")\n",
    "print(f\"üìä Models: {len(router.models)} available\")\n",
    "print(f\"üìä Routing: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LLM Routing Demo ready!\n",
      "üéØ Launch the demo to see intelligent model selection in action!\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://626f3709add6ccb707.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://626f3709add6ccb707.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://626f3709add6ccb707.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"LLM Routing Demo\") as demo:\n",
    "    gr.Markdown(\"# üöÄ Multi-LLM Routing Demo - See Intelligent Model Selection!\")\n",
    "    gr.Markdown(\"**This demo shows how queries are intelligently routed to different LLM models based on complexity!**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label=\"LLM-Routed Chat\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Query\", placeholder=\"Try: 'Hello' or 'Analyze machine learning algorithms'\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Route to LLM\")\n",
    "                clear_btn = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            system_stats = gr.Textbox(label=\"System Statistics\", value=interactive_router.get_system_stats(), interactive=False)\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### üéØ Try These Queries:\")\n",
    "            gr.Markdown(\"‚Ä¢ `Hello` - Simple query ‚Üí Fast Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `What is AI?` - Medium complexity ‚Üí Balanced Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `Analyze machine learning algorithms` - Complex ‚Üí Quality Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `Compare deep learning frameworks` - Complex ‚Üí Quality Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `How are you?` - Simple ‚Üí Fast Model\")\n",
    "            \n",
    "            gr.Markdown(\"### ü§ñ Available Models:\")\n",
    "            gr.Markdown(\"‚Ä¢ **‚ö° Fast Model** - Quick responses, low cost\")\n",
    "            gr.Markdown(\"‚Ä¢ **‚öñÔ∏è Balanced Model** - Good performance, moderate cost\")\n",
    "            gr.Markdown(\"‚Ä¢ **üéØ Quality Model** - High quality, higher cost\")\n",
    "            \n",
    "            gr.Markdown(\"### üß† Routing Logic:\")\n",
    "            gr.Markdown(\"‚Ä¢ **Query Length** - Longer queries = higher complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Keywords** - Technical terms increase complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Question Types** - Questions get medium complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Fallback System** - Automatic failover if primary model fails\")\n",
    "            \n",
    "            gr.Markdown(\"### üìä Features:\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Intelligent model selection\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Automatic fallbacks\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Cost optimization\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Performance tracking\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Complexity analysis\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Real-time routing\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def submit_query(query, history):\n",
    "        if query.strip():\n",
    "            new_history, _ = interactive_router.process_query(query, history or [])\n",
    "            return new_history, \"\", interactive_router.get_system_stats()\n",
    "        return history, \"\", interactive_router.get_system_stats()\n",
    "    \n",
    "    def clear_chat():\n",
    "        return [], interactive_router.get_system_stats()\n",
    "    \n",
    "    # Connect events\n",
    "    msg.submit(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    send_btn.click(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, system_stats])\n",
    "\n",
    "print(\"üöÄ LLM Routing Demo ready!\")\n",
    "print(\"üéØ Launch the demo to see intelligent model selection in action!\")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary - What We've Built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ MULTI-LLM ROUTING EXERCISE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "‚úÖ What We've Demonstrated:\n",
      "‚Ä¢ Intelligent LLM routing based on query complexity\n",
      "‚Ä¢ Dynamic model selection with fallback strategies\n",
      "‚Ä¢ Cost optimization and performance tracking\n",
      "‚Ä¢ Real-time routing decisions\n",
      "‚Ä¢ Interactive demo with Gradio\n",
      "‚Ä¢ Real API integration with OpenAI\n",
      "\n",
      "üöÄ Key Learning Outcomes:\n",
      "‚Ä¢ Query complexity analysis enables smart routing\n",
      "‚Ä¢ Fallback systems ensure high availability\n",
      "‚Ä¢ Cost optimization through model selection\n",
      "‚Ä¢ Performance tracking improves system efficiency\n",
      "‚Ä¢ Real API integration with OpenAI\n",
      "‚Ä¢ Practical hands-on implementation\n",
      "\n",
      "üéØ Production-Ready Features:\n",
      "‚Ä¢ Multi-model LLM routing system\n",
      "‚Ä¢ Complexity-based model selection\n",
      "‚Ä¢ Automatic fallback mechanisms\n",
      "‚Ä¢ Cost and performance tracking\n",
      "‚Ä¢ Real-time routing decisions\n",
      "‚Ä¢ Interactive user interface\n",
      "\n",
      "üìä System Statistics:\n",
      "‚Ä¢ Total queries: 5\n",
      "‚Ä¢ Success rate: 100.0%\n",
      "‚Ä¢ Fallback rate: 0.0%\n",
      "‚Ä¢ Total cost: $0.4565\n",
      "‚Ä¢ Available models: 3\n",
      "‚Ä¢ Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "print(\"üéâ MULTI-LLM ROUTING EXERCISE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ What We've Demonstrated:\")\n",
    "print(\"‚Ä¢ Intelligent LLM routing based on query complexity\")\n",
    "print(\"‚Ä¢ Dynamic model selection with fallback strategies\")\n",
    "print(\"‚Ä¢ Cost optimization and performance tracking\")\n",
    "print(\"‚Ä¢ Real-time routing decisions\")\n",
    "print(\"‚Ä¢ Interactive demo with Gradio\")\n",
    "print(\"‚Ä¢ Real API integration with OpenAI\")\n",
    "\n",
    "print(\"\\nüöÄ Key Learning Outcomes:\")\n",
    "print(\"‚Ä¢ Query complexity analysis enables smart routing\")\n",
    "print(\"‚Ä¢ Fallback systems ensure high availability\")\n",
    "print(\"‚Ä¢ Cost optimization through model selection\")\n",
    "print(\"‚Ä¢ Performance tracking improves system efficiency\")\n",
    "print(\"‚Ä¢ Real API integration with OpenAI\")\n",
    "print(\"‚Ä¢ Practical hands-on implementation\")\n",
    "\n",
    "print(\"\\nüéØ Production-Ready Features:\")\n",
    "print(\"‚Ä¢ Multi-model LLM routing system\")\n",
    "print(\"‚Ä¢ Complexity-based model selection\")\n",
    "print(\"‚Ä¢ Automatic fallback mechanisms\")\n",
    "print(\"‚Ä¢ Cost and performance tracking\")\n",
    "print(\"‚Ä¢ Real-time routing decisions\")\n",
    "print(\"‚Ä¢ Interactive user interface\")\n",
    "\n",
    "print(\"\\nüìä System Statistics:\")\n",
    "stats = router.get_routing_stats()\n",
    "if isinstance(stats, dict):\n",
    "    print(f\"‚Ä¢ Total queries: {stats['total_queries']}\")\n",
    "    print(f\"‚Ä¢ Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"‚Ä¢ Fallback rate: {stats['fallback_rate']:.1f}%\")\n",
    "    print(f\"‚Ä¢ Total cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"‚Ä¢ Available models: {len(router.models)}\")\n",
    "    print(f\"‚Ä¢ Routing strategy: Complexity-based with fallbacks\")\n",
    "else:\n",
    "    print(\"‚Ä¢ System ready for queries\")\n",
    "    print(f\"‚Ä¢ Available models: {len(router.models)}\")\n",
    "    print(f\"‚Ä¢ Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
