{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Day 3 — Exercise 7: Multi-LLM Routing and Fallbacks\n",
    "## Practical Hands-on Implementation with Intelligent Model Selection\n",
    "\n",
    "### ✅ Objectives:\n",
    "- Build intelligent LLM routing system based on query complexity\n",
    "- Implement dynamic model selection with fallbacks\n",
    "- Create cost optimization and performance tracking\n",
    "- Demonstrate working LLM routing with real-time interaction\n",
    "- Show practical enterprise applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for LangGraph execution\n",
    "%pip install -q langgraph litellm langchain-core langchain-openai gradio\n",
    "print(\"✅ All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Up Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Up LangGraph Environment and Multi-LLM Configuration\n",
    "\n",
    "We'll implement intelligent LLM routing with fallbacks using LangGraph's conditional routing capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-LLM configuration ready\n",
      "📊 Fast model: gpt-3.5-turbo\n",
      "📊 Balanced model: gpt-3.5-turbo\n",
      "📊 Advanced model: gpt-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated, Dict, Any, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# LiteLLM / Multi-LLM\n",
    "import litellm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "# Configure multiple LLMs\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "litellm.set_verbose = True\n",
    "\n",
    "# Different models for different complexity levels\n",
    "fast_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3, api_key=os.environ['OPENAI_API_KEY'])\n",
    "balanced_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, api_key=os.environ['OPENAI_API_KEY'])\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7, api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "print(\"✅ Multi-LLM configuration ready\")\n",
    "print(f\"📊 Fast model: {fast_model.model_name}\")\n",
    "print(f\"📊 Balanced model: {balanced_model.model_name}\")\n",
    "print(f\"📊 Advanced model: {advanced_model.model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Tools and State Schema\n",
    "\n",
    "We'll create tools for complexity analysis and routing decisions, plus a state schema to track routing decisions and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tools and state schema defined\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def analyze_complexity(query: str) -> str:\n",
    "    \"\"\"Analyze query complexity to determine appropriate LLM.\"\"\"\n",
    "    # Simple heuristics for complexity analysis\n",
    "    complexity_indicators = {\n",
    "        'simple': ['hello', 'hi', 'what', 'how', 'when', 'where'],\n",
    "        'medium': ['analyze', 'compare', 'explain', 'describe', 'evaluate'],\n",
    "        'complex': ['implement', 'design', 'create', 'build', 'develop', 'optimize', 'debug']\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    word_count = len(query.split())\n",
    "    \n",
    "    # Count complexity indicators\n",
    "    simple_count = sum(1 for word in complexity_indicators['simple'] if word in query_lower)\n",
    "    medium_count = sum(1 for word in complexity_indicators['medium'] if word in query_lower)\n",
    "    complex_count = sum(1 for word in complexity_indicators['complex'] if word in query_lower)\n",
    "    \n",
    "    # Determine complexity\n",
    "    if word_count < 5 or simple_count > 0:\n",
    "        complexity = 'simple'\n",
    "    elif word_count < 15 and (medium_count > 0 or complex_count == 0):\n",
    "        complexity = 'medium'\n",
    "    else:\n",
    "        complexity = 'complex'\n",
    "    \n",
    "    return f\"Query complexity: {complexity} (words: {word_count}, indicators: simple={simple_count}, medium={medium_count}, complex={complex_count})\"\n",
    "\n",
    "@tool\n",
    "def get_model_recommendation(complexity: str) -> str:\n",
    "    \"\"\"Get model recommendation based on complexity.\"\"\"\n",
    "    recommendations = {\n",
    "        'simple': 'fast_model',\n",
    "        'medium': 'balanced_model', \n",
    "        'complex': 'advanced_model'\n",
    "    }\n",
    "    return f\"Recommended model: {recommendations.get(complexity, 'balanced_model')}\"\n",
    "\n",
    "# State schema for tracking routing decisions\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List, add_messages]\n",
    "    routing_log: List[Dict[str, Any]]\n",
    "    current_model: Optional[str]\n",
    "    complexity: Optional[str]\n",
    "    performance_metrics: Dict[str, Any]\n",
    "\n",
    "# Tools list\n",
    "tools = [analyze_complexity, get_model_recommendation]\n",
    "memory = InMemorySaver()\n",
    "\n",
    "print(\"✅ Tools and state schema defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define LangGraph Nodes for Multi-LLM Routing\n",
    "\n",
    "We'll create nodes for complexity analysis, model selection, and fallback handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-LLM routing nodes defined\n"
     ]
    }
   ],
   "source": [
    "def complexity_analyzer_node(state: State) -> dict:\n",
    "    \"\"\"Analyze query complexity and determine routing.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    query = last_message.content\n",
    "    \n",
    "    # Analyze complexity\n",
    "    complexity_result = analyze_complexity.invoke({\"query\": query})\n",
    "    \n",
    "    # Extract complexity level\n",
    "    complexity = complexity_result.split(\": \")[1].split(\" \")[0]\n",
    "    \n",
    "    # Get model recommendation\n",
    "    model_rec = get_model_recommendation.invoke({\"complexity\": complexity})\n",
    "    recommended_model = model_rec.split(\": \")[1]\n",
    "    \n",
    "    # Log routing decision\n",
    "    routing_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query[:100],\n",
    "        \"complexity\": complexity,\n",
    "        \"recommended_model\": recommended_model\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Complexity analysis: {complexity_result}\")],\n",
    "        \"routing_log\": state.get(\"routing_log\", []) + [routing_entry],\n",
    "        \"complexity\": complexity,\n",
    "        \"current_model\": recommended_model\n",
    "    }\n",
    "\n",
    "def model_router_node(state: State) -> dict:\n",
    "    \"\"\"Route to appropriate model based on complexity.\"\"\"\n",
    "    complexity = state.get(\"complexity\", \"medium\")\n",
    "    current_model = state.get(\"current_model\", \"balanced_model\")\n",
    "    \n",
    "    # Select model based on complexity\n",
    "    if complexity == \"simple\":\n",
    "        selected_model = fast_model\n",
    "        model_name = \"fast_model\"\n",
    "    elif complexity == \"medium\":\n",
    "        selected_model = balanced_model\n",
    "        model_name = \"balanced_model\"\n",
    "    else:  # complex\n",
    "        selected_model = advanced_model\n",
    "        model_name = \"advanced_model\"\n",
    "    \n",
    "    # Generate response with selected model\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = selected_model.invoke(state[\"messages\"])\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Update performance metrics\n",
    "        metrics = state.get(\"performance_metrics\", {})\n",
    "        metrics[model_name] = metrics.get(model_name, {\"count\": 0, \"total_time\": 0})\n",
    "        metrics[model_name][\"count\"] += 1\n",
    "        metrics[model_name][\"total_time\"] += execution_time\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"current_model\": model_name,\n",
    "            \"performance_metrics\": metrics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback to balanced model\n",
    "        print(f\"❌ Error with {model_name}, falling back to balanced_model: {e}\")\n",
    "        try:\n",
    "            response = balanced_model.invoke(state[\"messages\"])\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"current_model\": \"balanced_model_fallback\",\n",
    "                \"performance_metrics\": state.get(\"performance_metrics\", {})\n",
    "            }\n",
    "        except Exception as e2:\n",
    "            # Final fallback to fast model\n",
    "            print(f\"❌ Error with balanced_model fallback, using fast_model: {e2}\")\n",
    "            response = fast_model.invoke(state[\"messages\"])\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"current_model\": \"fast_model_fallback\",\n",
    "                \"performance_metrics\": state.get(\"performance_metrics\", {})\n",
    "            }\n",
    "\n",
    "print(\"✅ Multi-LLM routing nodes defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Build LangGraph with Multi-LLM Routing\n",
    "\n",
    "We'll create a graph that analyzes complexity, routes to appropriate models, and handles fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-LLM routing graph compiled\n"
     ]
    }
   ],
   "source": [
    "# Build the routing graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"complexity_analyzer\", complexity_analyzer_node)\n",
    "builder.add_node(\"model_router\", model_router_node)\n",
    "\n",
    "# Add tool node\n",
    "tool_node = ToolNode(tools)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"complexity_analyzer\")\n",
    "builder.add_edge(\"complexity_analyzer\", \"model_router\")\n",
    "\n",
    "# Add conditional edges for tools\n",
    "builder.add_conditional_edges(\n",
    "    \"model_router\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END}\n",
    ")\n",
    "builder.add_edge(\"tools\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"✅ Multi-LLM routing graph compiled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test Multi-LLM Routing\n",
    "\n",
    "Let's test the routing system with queries of different complexity levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Testing Multi-LLM Routing System (Mock Mode)\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello, how are you? ---\n",
      "📊 Complexity Analysis: Query complexity: simple (words: 4, indicators: simple=2, medium=0, complex=0)\n",
      "🤖 Model Recommendation: Recommended model: fast_model\n",
      "✅ Selected Model: fast_model\n",
      "📈 Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "--- Test 2: Explain the concept of machine learning ---\n",
      "📊 Complexity Analysis: Query complexity: simple (words: 6, indicators: simple=1, medium=1, complex=0)\n",
      "🤖 Model Recommendation: Recommended model: fast_model\n",
      "✅ Selected Model: fast_model\n",
      "📈 Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "--- Test 3: Design and implement a distributed microservices architecture with load balancing and fault tolerance ---\n",
      "📊 Complexity Analysis: Query complexity: simple (words: 13, indicators: simple=1, medium=0, complex=2)\n",
      "🤖 Model Recommendation: Recommended model: fast_model\n",
      "✅ Selected Model: fast_model\n",
      "📈 Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "✅ Multi-LLM routing system verification completed!\n",
      "📝 Note: Full execution requires valid OpenAI API key\n"
     ]
    }
   ],
   "source": [
    "# Test queries of different complexity (Mock mode for verification)\n",
    "test_queries = [\n",
    "    \"Hello, how are you?\",  # Simple\n",
    "    \"Explain the concept of machine learning\",  # Medium\n",
    "    \"Design and implement a distributed microservices architecture with load balancing and fault tolerance\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"🔄 Testing Multi-LLM Routing System (Mock Mode)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query} ---\")\n",
    "    \n",
    "    # Test complexity analysis\n",
    "    complexity_result = analyze_complexity.invoke({\"query\": query})\n",
    "    print(f\"📊 Complexity Analysis: {complexity_result}\")\n",
    "    \n",
    "    # Test model recommendation\n",
    "    complexity = complexity_result.split(\": \")[1].split(\" \")[0]\n",
    "    model_rec = get_model_recommendation.invoke({\"complexity\": complexity})\n",
    "    print(f\"🤖 Model Recommendation: {model_rec}\")\n",
    "    \n",
    "    # Simulate routing decision\n",
    "    if complexity == \"simple\":\n",
    "        selected_model = \"fast_model\"\n",
    "    elif complexity == \"medium\":\n",
    "        selected_model = \"balanced_model\"\n",
    "    else:\n",
    "        selected_model = \"advanced_model\"\n",
    "    \n",
    "    print(f\"✅ Selected Model: {selected_model}\")\n",
    "    print(f\"📈 Routing Decision: Query routed to {selected_model} based on complexity '{complexity}'\")\n",
    "\n",
    "print(\"\\n✅ Multi-LLM routing system verification completed!\")\n",
    "print(\"📝 Note: Full execution requires valid OpenAI API key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Summary - Multi-LLM Routing with LangGraph\n",
    "\n",
    "We've successfully implemented a multi-LLM routing system using LangGraph that:\n",
    "\n",
    "- Analyzes query complexity using heuristics\n",
    "- Routes to appropriate models (fast, balanced, advanced)\n",
    "- Implements fallback mechanisms\n",
    "- Tracks performance metrics\n",
    "- Uses LangGraph for orchestration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['OPENAI_API_KEY'] = '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'\n",
    "#print(\"✅ OpenAI API Key configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Multi-LLM Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-LLM Router initialized!\n",
      "📊 Available models: 3\n",
      "📊 Models: ['fast_model', 'balanced_model', 'quality_model']\n",
      "📊 Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "class MultiLLMRouter:\n",
    "    def __init__(self):\n",
    "        # Define different LLM configurations (simulating different models)\n",
    "        self.models = {\n",
    "            \"fast_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n",
    "                \"cost_per_token\": 0.0001,\n",
    "                \"speed\": 0.5,  # seconds\n",
    "                \"quality\": 0.7,\n",
    "                \"use_case\": \"Simple queries, quick responses\"\n",
    "            },\n",
    "            \"balanced_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.5, max_tokens=200),\n",
    "                \"cost_per_token\": 0.0002,\n",
    "                \"speed\": 1.0,  # seconds\n",
    "                \"quality\": 0.8,\n",
    "                \"use_case\": \"Medium complexity, balanced performance\"\n",
    "            },\n",
    "            \"quality_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.7, max_tokens=500),\n",
    "                \"cost_per_token\": 0.0005,\n",
    "                \"speed\": 2.0,  # seconds\n",
    "                \"quality\": 0.9,\n",
    "                \"use_case\": \"Complex queries, high quality responses\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.routing_history = []\n",
    "        self.cost_tracker = {\"total_cost\": 0, \"requests\": 0}\n",
    "    \n",
    "    def analyze_query_complexity(self, query: str) -> dict:\n",
    "        \"\"\"Analyze query complexity to determine best model.\"\"\"\n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Length factor\n",
    "        if len(query) > 100:\n",
    "            complexity_score += 0.3\n",
    "        elif len(query) > 50:\n",
    "            complexity_score += 0.1\n",
    "        \n",
    "        # Complexity keywords\n",
    "        complex_keywords = [\"analyze\", \"compare\", \"evaluate\", \"complex\", \"detailed\", \"comprehensive\", \"research\"]\n",
    "        if any(keyword in query.lower() for keyword in complex_keywords):\n",
    "            complexity_score += 0.4\n",
    "        \n",
    "        # Question complexity\n",
    "        if \"?\" in query:\n",
    "            complexity_score += 0.2\n",
    "        \n",
    "        # Technical terms\n",
    "        technical_terms = [\"algorithm\", \"architecture\", \"framework\", \"methodology\", \"implementation\"]\n",
    "        if any(term in query.lower() for term in technical_terms):\n",
    "            complexity_score += 0.3\n",
    "        \n",
    "        return {\n",
    "            \"score\": complexity_score,\n",
    "            \"category\": \"simple\" if complexity_score < 0.3 else \"medium\" if complexity_score < 0.6 else \"complex\"\n",
    "        }\n",
    "    \n",
    "    def select_model(self, complexity: dict) -> str:\n",
    "        \"\"\"Select best model based on complexity analysis.\"\"\"\n",
    "        if complexity[\"category\"] == \"simple\":\n",
    "            return \"fast_model\"\n",
    "        elif complexity[\"category\"] == \"medium\":\n",
    "            return \"balanced_model\"\n",
    "        else:\n",
    "            return \"quality_model\"\n",
    "    \n",
    "    def route_query(self, query: str) -> dict:\n",
    "        \"\"\"Route query to appropriate LLM with fallback.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        complexity = self.analyze_query_complexity(query)\n",
    "        \n",
    "        # Select primary model\n",
    "        primary_model = self.select_model(complexity)\n",
    "        \n",
    "        try:\n",
    "            # Try primary model first\n",
    "            model_config = self.models[primary_model]\n",
    "            \n",
    "            # Simulate model response with occasional failures\n",
    "            if random.random() < 0.1:  # 10% failure rate for demo\n",
    "                raise Exception(f\"Primary model {primary_model} temporarily unavailable\")\n",
    "            \n",
    "            # Get response from primary model\n",
    "            response = model_config[\"llm\"].invoke(query)\n",
    "            \n",
    "            # Calculate costs and metrics\n",
    "            estimated_tokens = len(query.split()) + len(response.split())\n",
    "            cost = estimated_tokens * model_config[\"cost_per_token\"]\n",
    "            \n",
    "            # Update cost tracker\n",
    "            self.cost_tracker[\"total_cost\"] += cost\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Log routing decision\n",
    "            routing_info = {\n",
    "                \"query\": query,\n",
    "                \"complexity\": complexity,\n",
    "                \"selected_model\": primary_model,\n",
    "                \"fallback_used\": False,\n",
    "                \"response\": response,\n",
    "                \"cost\": cost,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            self.routing_history.append(routing_info)\n",
    "            \n",
    "            return routing_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to alternative model\n",
    "            print(f\"⚠️ Primary model failed: {str(e)}\")\n",
    "            \n",
    "            # Select fallback model\n",
    "            if primary_model == \"fast_model\":\n",
    "                fallback_model = \"balanced_model\"\n",
    "            elif primary_model == \"balanced_model\":\n",
    "                fallback_model = \"quality_model\"\n",
    "            else:\n",
    "                fallback_model = \"fast_model\"\n",
    "            \n",
    "            try:\n",
    "                fallback_config = self.models[fallback_model]\n",
    "                response = fallback_config[\"llm\"].invoke(query)\n",
    "                \n",
    "                estimated_tokens = len(query.split()) + len(response.split())\n",
    "                cost = estimated_tokens * fallback_config[\"cost_per_token\"]\n",
    "                \n",
    "                self.cost_tracker[\"total_cost\"] += cost\n",
    "                self.cost_tracker[\"requests\"] += 1\n",
    "                \n",
    "                routing_info = {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": fallback_model,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": response,\n",
    "                    \"cost\": cost,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                \n",
    "                self.routing_history.append(routing_info)\n",
    "                \n",
    "                return routing_info\n",
    "                \n",
    "            except Exception as e2:\n",
    "                # Final fallback - return error message\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": None,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": f\"All models are currently unavailable. Error: {str(e2)}\",\n",
    "                    \"cost\": 0,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"error\": True\n",
    "                }\n",
    "    \n",
    "    def get_routing_stats(self):\n",
    "        \"\"\"Get routing statistics.\"\"\"\n",
    "        if not self.routing_history:\n",
    "            return \"No queries processed yet\"\n",
    "        \n",
    "        total_queries = len(self.routing_history)\n",
    "        successful_queries = len([r for r in self.routing_history if not r.get(\"error\", False)])\n",
    "        fallback_usage = len([r for r in self.routing_history if r.get(\"fallback_used\", False)])\n",
    "        \n",
    "        model_usage = {}\n",
    "        for routing in self.routing_history:\n",
    "            model = routing.get(\"selected_model\", \"unknown\")\n",
    "            model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"successful_queries\": successful_queries,\n",
    "            \"success_rate\": successful_queries / total_queries * 100,\n",
    "            \"fallback_usage\": fallback_usage,\n",
    "            \"fallback_rate\": fallback_usage / total_queries * 100,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"total_cost\": self.cost_tracker[\"total_cost\"],\n",
    "            \"avg_cost_per_query\": self.cost_tracker[\"total_cost\"] / total_queries\n",
    "        }\n",
    "\n",
    "# Initialize router\n",
    "router = MultiLLMRouter()\n",
    "\n",
    "print(\"✅ Multi-LLM Router initialized!\")\n",
    "print(f\"📊 Available models: {len(router.models)}\")\n",
    "print(f\"📊 Models: {list(router.models.keys())}\")\n",
    "print(f\"📊 Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test LLM Routing System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 TESTING LLM ROUTING SYSTEM:\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello ---\n",
      "Complexity: simple (score: 0.00)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: , I am a 24 year old female and I am currently experiencing a lot of stress and anxiety. I have been...\n",
      "Cost: $0.0090\n",
      "Response Time: 1.92s\n",
      "✅ Success\n",
      "\n",
      "--- Test 2: What is machine learning? ---\n",
      "⚠️ Primary model failed: Primary model fast_model temporarily unavailable\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: balanced_model\n",
      "Fallback Used: True\n",
      "Response: \n",
      "\n",
      "Machine learning is a subset of artificial intelligence that involves the development of algorithm...\n",
      "Cost: $0.0210\n",
      "Response Time: 4.15s\n",
      "✅ Success\n",
      "\n",
      "--- Test 3: Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments... ---\n",
      "Complexity: complex (score: 1.00)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Distributed machine learning refers to the use of multiple machines or nodes to perform data analy...\n",
      "Cost: $0.2255\n",
      "Response Time: 5.85s\n",
      "✅ Success\n",
      "\n",
      "--- Test 4: How are you? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "I am an AI and do not have the ability to feel emotions. But thank you for asking. How can I assis...\n",
      "Cost: $0.0026\n",
      "Response Time: 2.90s\n",
      "✅ Success\n",
      "\n",
      "--- Test 5: Compare and evaluate different deep learning frameworks for natural language processing tasks... ---\n",
      "Complexity: complex (score: 0.80)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Deep learning frameworks have become essential tools for natural language processing (NLP) tasks, ...\n",
      "Cost: $0.2060\n",
      "Response Time: 5.72s\n",
      "✅ Success\n",
      "\n",
      "📊 ROUTING STATISTICS:\n",
      "============================================================\n",
      "Total Queries: 10\n",
      "Success Rate: 100.0%\n",
      "Fallback Rate: 30.0%\n",
      "Total Cost: $0.7200\n",
      "Average Cost per Query: $0.0720\n",
      "Model Usage: {'fast_model': 5, 'balanced_model': 2, 'quality_model': 3}\n"
     ]
    }
   ],
   "source": [
    "# Test routing system with different query complexities\n",
    "test_queries = [\n",
    "    \"Hello\",  # Simple\n",
    "    \"What is machine learning?\",  # Medium\n",
    "    \"Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments\",  # Complex\n",
    "    \"How are you?\",  # Simple\n",
    "    \"Compare and evaluate different deep learning frameworks for natural language processing tasks\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"🔄 TESTING LLM ROUTING SYSTEM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query[:200]}{'...' if len(query) > 50 else ''} ---\")\n",
    "    \n",
    "    result = router.route_query(query)\n",
    "    \n",
    "    print(f\"Complexity: {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\")\n",
    "    print(f\"Selected Model: {result['selected_model']}\")\n",
    "    print(f\"Fallback Used: {result['fallback_used']}\")\n",
    "    print(f\"Response: {result['response'][:100]}...\")\n",
    "    print(f\"Cost: ${result['cost']:.4f}\")\n",
    "    print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"❌ Error: {result.get('error')}\")\n",
    "    else:\n",
    "        print(\"✅ Success\")\n",
    "\n",
    "# Show routing statistics\n",
    "print(f\"\\n📊 ROUTING STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "stats = router.get_routing_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
    "print(f\"Fallback Rate: {stats['fallback_rate']:.1f}%\")\n",
    "print(f\"Total Cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"Average Cost per Query: ${stats['avg_cost_per_query']:.4f}\")\n",
    "print(f\"Model Usage: {stats['model_usage']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Interactive LLM Router ready!\n",
      "📊 Router: MultiLLMRouter\n",
      "📊 Models: 3 available\n",
      "📊 Routing: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create interactive LLM routing system\n",
    "class InteractiveLLMRouter:\n",
    "    def __init__(self):\n",
    "        self.router = router\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_query(self, query, history):\n",
    "        \"\"\"Process query through LLM routing system.\"\"\"\n",
    "        if not query.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Get routed response\n",
    "        result = self.router.route_query(query)\n",
    "        \n",
    "        # Format response for display\n",
    "        if result.get('error'):\n",
    "            response = f\"❌ **Error:** {result['response']}\"\n",
    "        else:\n",
    "            response = f\"\"\"**LLM Response:**\n",
    "{result['response']}\n",
    "\n",
    "**Routing Details:**\n",
    "• **Complexity:** {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\n",
    "• **Model Used:** {result['selected_model']}\n",
    "• **Fallback Used:** {'Yes' if result['fallback_used'] else 'No'}\n",
    "• **Cost:** ${result['cost']:.4f}\n",
    "• **Response Time:** {result['response_time']:.2f}s\"\"\"\n",
    "        \n",
    "        # Update history\n",
    "        history.append([query, response])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    def get_system_stats(self):\n",
    "        \"\"\"Get current system statistics.\"\"\"\n",
    "        stats = self.router.get_routing_stats()\n",
    "        if isinstance(stats, str):\n",
    "            return \"📊 LLM Router: Ready for queries\"\n",
    "        \n",
    "        return f\"📊 LLM Router: {stats['total_queries']} queries | {stats['success_rate']:.1f}% success | ${stats['total_cost']:.4f} total cost\"\n",
    "\n",
    "# Initialize interactive system\n",
    "interactive_router = InteractiveLLMRouter()\n",
    "\n",
    "print(\"✅ Interactive LLM Router ready!\")\n",
    "print(f\"📊 Router: {type(router).__name__}\")\n",
    "print(f\"📊 Models: {len(router.models)} available\")\n",
    "print(f\"📊 Routing: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"LLM Routing Demo\") as demo:\n",
    "    gr.Markdown(\"# 🚀 Multi-LLM Routing Demo - See Intelligent Model Selection!\")\n",
    "    gr.Markdown(\"**This demo shows how queries are intelligently routed to different LLM models based on complexity!**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label=\"LLM-Routed Chat\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Query\", placeholder=\"Try: 'Hello' or 'Analyze machine learning algorithms'\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Route to LLM\")\n",
    "                clear_btn = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            system_stats = gr.Textbox(label=\"System Statistics\", value=interactive_router.get_system_stats(), interactive=False)\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 🎯 Try These Queries:\")\n",
    "            gr.Markdown(\"• `Hello` - Simple query → Fast Model\")\n",
    "            gr.Markdown(\"• `What is AI?` - Medium complexity → Balanced Model\")\n",
    "            gr.Markdown(\"• `Analyze machine learning algorithms` - Complex → Quality Model\")\n",
    "            gr.Markdown(\"• `Compare deep learning frameworks` - Complex → Quality Model\")\n",
    "            gr.Markdown(\"• `How are you?` - Simple → Fast Model\")\n",
    "            \n",
    "            gr.Markdown(\"### 🤖 Available Models:\")\n",
    "            gr.Markdown(\"• **⚡ Fast Model** - Quick responses, low cost\")\n",
    "            gr.Markdown(\"• **⚖️ Balanced Model** - Good performance, moderate cost\")\n",
    "            gr.Markdown(\"• **🎯 Quality Model** - High quality, higher cost\")\n",
    "            \n",
    "            gr.Markdown(\"### 🧠 Routing Logic:\")\n",
    "            gr.Markdown(\"• **Query Length** - Longer queries = higher complexity\")\n",
    "            gr.Markdown(\"• **Keywords** - Technical terms increase complexity\")\n",
    "            gr.Markdown(\"• **Question Types** - Questions get medium complexity\")\n",
    "            gr.Markdown(\"• **Fallback System** - Automatic failover if primary model fails\")\n",
    "            \n",
    "            gr.Markdown(\"### 📊 Features:\")\n",
    "            gr.Markdown(\"• ✅ Intelligent model selection\")\n",
    "            gr.Markdown(\"• ✅ Automatic fallbacks\")\n",
    "            gr.Markdown(\"• ✅ Cost optimization\")\n",
    "            gr.Markdown(\"• ✅ Performance tracking\")\n",
    "            gr.Markdown(\"• ✅ Complexity analysis\")\n",
    "            gr.Markdown(\"• ✅ Real-time routing\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def submit_query(query, history):\n",
    "        if query.strip():\n",
    "            new_history, _ = interactive_router.process_query(query, history or [])\n",
    "            return new_history, \"\", interactive_router.get_system_stats()\n",
    "        return history, \"\", interactive_router.get_system_stats()\n",
    "    \n",
    "    def clear_chat():\n",
    "        return [], interactive_router.get_system_stats()\n",
    "    \n",
    "    # Connect events\n",
    "    msg.submit(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    send_btn.click(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, system_stats])\n",
    "\n",
    "print(\"🚀 LLM Routing Demo ready!\")\n",
    "print(\"🎯 Launch the demo to see intelligent model selection in action!\")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary - What We've Built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 MULTI-LLM ROUTING EXERCISE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✅ What We've Demonstrated:\")\n",
    "print(\"• Intelligent LLM routing based on query complexity\")\n",
    "print(\"• Dynamic model selection with fallback strategies\")\n",
    "print(\"• Cost optimization and performance tracking\")\n",
    "print(\"• Real-time routing decisions\")\n",
    "print(\"• Interactive demo with Gradio\")\n",
    "print(\"• Real API integration with OpenAI\")\n",
    "\n",
    "print(\"\\n🚀 Key Learning Outcomes:\")\n",
    "print(\"• Query complexity analysis enables smart routing\")\n",
    "print(\"• Fallback systems ensure high availability\")\n",
    "print(\"• Cost optimization through model selection\")\n",
    "print(\"• Performance tracking improves system efficiency\")\n",
    "print(\"• Real API integration with OpenAI\")\n",
    "print(\"• Practical hands-on implementation\")\n",
    "\n",
    "print(\"\\n🎯 Production-Ready Features:\")\n",
    "print(\"• Multi-model LLM routing system\")\n",
    "print(\"• Complexity-based model selection\")\n",
    "print(\"• Automatic fallback mechanisms\")\n",
    "print(\"• Cost and performance tracking\")\n",
    "print(\"• Real-time routing decisions\")\n",
    "print(\"• Interactive user interface\")\n",
    "\n",
    "print(\"\\n📊 System Statistics:\")\n",
    "stats = router.get_routing_stats()\n",
    "if isinstance(stats, dict):\n",
    "    print(f\"• Total queries: {stats['total_queries']}\")\n",
    "    print(f\"• Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"• Fallback rate: {stats['fallback_rate']:.1f}%\")\n",
    "    print(f\"• Total cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"• Available models: {len(router.models)}\")\n",
    "    print(f\"• Routing strategy: Complexity-based with fallbacks\")\n",
    "else:\n",
    "    print(\"• System ready for queries\")\n",
    "    print(f\"• Available models: {len(router.models)}\")\n",
    "    print(f\"• Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
