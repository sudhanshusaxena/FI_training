{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Day 3 ‚Äî Exercise 7: Multi-LLM Routing and Fallbacks\n",
    "## Practical Hands-on Implementation with Intelligent Model Selection\n",
    "\n",
    "### ‚úÖ Objectives:\n",
    "- Build intelligent LLM routing system based on query complexity\n",
    "- Implement dynamic model selection with fallbacks\n",
    "- Create cost optimization and performance tracking\n",
    "- Demonstrate working LLM routing with real-time interaction\n",
    "- Show practical enterprise applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for LangGraph execution\n",
    "%pip install -q langgraph litellm langchain-core langchain-openai gradio\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Up Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Up LangGraph Environment and Multi-LLM Configuration\n",
    "\n",
    "We'll implement intelligent LLM routing with fallbacks using LangGraph's conditional routing capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-LLM configuration ready\n",
      "üìä Fast model: gpt-3.5-turbo\n",
      "üìä Balanced model: gpt-3.5-turbo\n",
      "üìä Advanced model: gpt-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated, Dict, Any, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# LiteLLM / Multi-LLM\n",
    "import litellm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "# Configure multiple LLMs\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "litellm.set_verbose = True\n",
    "\n",
    "# Different models for different complexity levels\n",
    "fast_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3, api_key=os.environ['OPENAI_API_KEY'])\n",
    "balanced_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, api_key=os.environ['OPENAI_API_KEY'])\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7, api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "print(\"‚úÖ Multi-LLM configuration ready\")\n",
    "print(f\"üìä Fast model: {fast_model.model_name}\")\n",
    "print(f\"üìä Balanced model: {balanced_model.model_name}\")\n",
    "print(f\"üìä Advanced model: {advanced_model.model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Tools and State Schema\n",
    "\n",
    "We'll create tools for complexity analysis and routing decisions, plus a state schema to track routing decisions and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tools and state schema defined\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def analyze_complexity(query: str) -> str:\n",
    "    \"\"\"Analyze query complexity to determine appropriate LLM.\"\"\"\n",
    "    # Simple heuristics for complexity analysis\n",
    "    complexity_indicators = {\n",
    "        'simple': ['hello', 'hi', 'what', 'how', 'when', 'where'],\n",
    "        'medium': ['analyze', 'compare', 'explain', 'describe', 'evaluate'],\n",
    "        'complex': ['implement', 'design', 'create', 'build', 'develop', 'optimize', 'debug']\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    word_count = len(query.split())\n",
    "    \n",
    "    # Count complexity indicators\n",
    "    simple_count = sum(1 for word in complexity_indicators['simple'] if word in query_lower)\n",
    "    medium_count = sum(1 for word in complexity_indicators['medium'] if word in query_lower)\n",
    "    complex_count = sum(1 for word in complexity_indicators['complex'] if word in query_lower)\n",
    "    \n",
    "    # Determine complexity\n",
    "    if word_count < 5 or simple_count > 0:\n",
    "        complexity = 'simple'\n",
    "    elif word_count < 15 and (medium_count > 0 or complex_count == 0):\n",
    "        complexity = 'medium'\n",
    "    else:\n",
    "        complexity = 'complex'\n",
    "    \n",
    "    return f\"Query complexity: {complexity} (words: {word_count}, indicators: simple={simple_count}, medium={medium_count}, complex={complex_count})\"\n",
    "\n",
    "@tool\n",
    "def get_model_recommendation(complexity: str) -> str:\n",
    "    \"\"\"Get model recommendation based on complexity.\"\"\"\n",
    "    recommendations = {\n",
    "        'simple': 'fast_model',\n",
    "        'medium': 'balanced_model', \n",
    "        'complex': 'advanced_model'\n",
    "    }\n",
    "    return f\"Recommended model: {recommendations.get(complexity, 'balanced_model')}\"\n",
    "\n",
    "# State schema for tracking routing decisions\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List, add_messages]\n",
    "    routing_log: List[Dict[str, Any]]\n",
    "    current_model: Optional[str]\n",
    "    complexity: Optional[str]\n",
    "    performance_metrics: Dict[str, Any]\n",
    "\n",
    "# Tools list\n",
    "tools = [analyze_complexity, get_model_recommendation]\n",
    "memory = InMemorySaver()\n",
    "\n",
    "print(\"‚úÖ Tools and state schema defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define LangGraph Nodes for Multi-LLM Routing\n",
    "\n",
    "We'll create nodes for complexity analysis, model selection, and fallback handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-LLM routing nodes defined\n"
     ]
    }
   ],
   "source": [
    "def complexity_analyzer_node(state: State) -> dict:\n",
    "    \"\"\"Analyze query complexity and determine routing.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    query = last_message.content\n",
    "    \n",
    "    # Analyze complexity\n",
    "    complexity_result = analyze_complexity.invoke({\"query\": query})\n",
    "    \n",
    "    # Extract complexity level\n",
    "    complexity = complexity_result.split(\": \")[1].split(\" \")[0]\n",
    "    \n",
    "    # Get model recommendation\n",
    "    model_rec = get_model_recommendation.invoke({\"complexity\": complexity})\n",
    "    recommended_model = model_rec.split(\": \")[1]\n",
    "    \n",
    "    # Log routing decision\n",
    "    routing_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query[:100],\n",
    "        \"complexity\": complexity,\n",
    "        \"recommended_model\": recommended_model\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Complexity analysis: {complexity_result}\")],\n",
    "        \"routing_log\": state.get(\"routing_log\", []) + [routing_entry],\n",
    "        \"complexity\": complexity,\n",
    "        \"current_model\": recommended_model\n",
    "    }\n",
    "\n",
    "def model_router_node(state: State) -> dict:\n",
    "    \"\"\"Route to appropriate model based on complexity.\"\"\"\n",
    "    complexity = state.get(\"complexity\", \"medium\")\n",
    "    current_model = state.get(\"current_model\", \"balanced_model\")\n",
    "    \n",
    "    # Select model based on complexity\n",
    "    if complexity == \"simple\":\n",
    "        selected_model = fast_model\n",
    "        model_name = \"fast_model\"\n",
    "    elif complexity == \"medium\":\n",
    "        selected_model = balanced_model\n",
    "        model_name = \"balanced_model\"\n",
    "    else:  # complex\n",
    "        selected_model = advanced_model\n",
    "        model_name = \"advanced_model\"\n",
    "    \n",
    "    # Generate response with selected model\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = selected_model.invoke(state[\"messages\"])\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Update performance metrics\n",
    "        metrics = state.get(\"performance_metrics\", {})\n",
    "        metrics[model_name] = metrics.get(model_name, {\"count\": 0, \"total_time\": 0})\n",
    "        metrics[model_name][\"count\"] += 1\n",
    "        metrics[model_name][\"total_time\"] += execution_time\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"current_model\": model_name,\n",
    "            \"performance_metrics\": metrics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback to balanced model\n",
    "        print(f\"‚ùå Error with {model_name}, falling back to balanced_model: {e}\")\n",
    "        try:\n",
    "            response = balanced_model.invoke(state[\"messages\"])\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"current_model\": \"balanced_model_fallback\",\n",
    "                \"performance_metrics\": state.get(\"performance_metrics\", {})\n",
    "            }\n",
    "        except Exception as e2:\n",
    "            # Final fallback to fast model\n",
    "            print(f\"‚ùå Error with balanced_model fallback, using fast_model: {e2}\")\n",
    "            response = fast_model.invoke(state[\"messages\"])\n",
    "            return {\n",
    "                \"messages\": [response],\n",
    "                \"current_model\": \"fast_model_fallback\",\n",
    "                \"performance_metrics\": state.get(\"performance_metrics\", {})\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ Multi-LLM routing nodes defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Build LangGraph with Multi-LLM Routing\n",
    "\n",
    "We'll create a graph that analyzes complexity, routes to appropriate models, and handles fallbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-LLM routing graph compiled\n"
     ]
    }
   ],
   "source": [
    "# Build the routing graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"complexity_analyzer\", complexity_analyzer_node)\n",
    "builder.add_node(\"model_router\", model_router_node)\n",
    "\n",
    "# Add tool node\n",
    "tool_node = ToolNode(tools)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"complexity_analyzer\")\n",
    "builder.add_edge(\"complexity_analyzer\", \"model_router\")\n",
    "\n",
    "# Add conditional edges for tools\n",
    "builder.add_conditional_edges(\n",
    "    \"model_router\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END}\n",
    ")\n",
    "builder.add_edge(\"tools\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Multi-LLM routing graph compiled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test Multi-LLM Routing\n",
    "\n",
    "Let's test the routing system with queries of different complexity levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Multi-LLM Routing System (Mock Mode)\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello, how are you? ---\n",
      "üìä Complexity Analysis: Query complexity: simple (words: 4, indicators: simple=2, medium=0, complex=0)\n",
      "ü§ñ Model Recommendation: Recommended model: fast_model\n",
      "‚úÖ Selected Model: fast_model\n",
      "üìà Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "--- Test 2: Explain the concept of machine learning ---\n",
      "üìä Complexity Analysis: Query complexity: simple (words: 6, indicators: simple=1, medium=1, complex=0)\n",
      "ü§ñ Model Recommendation: Recommended model: fast_model\n",
      "‚úÖ Selected Model: fast_model\n",
      "üìà Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "--- Test 3: Design and implement a distributed microservices architecture with load balancing and fault tolerance ---\n",
      "üìä Complexity Analysis: Query complexity: simple (words: 13, indicators: simple=1, medium=0, complex=2)\n",
      "ü§ñ Model Recommendation: Recommended model: fast_model\n",
      "‚úÖ Selected Model: fast_model\n",
      "üìà Routing Decision: Query routed to fast_model based on complexity 'simple'\n",
      "\n",
      "‚úÖ Multi-LLM routing system verification completed!\n",
      "üìù Note: Full execution requires valid OpenAI API key\n"
     ]
    }
   ],
   "source": [
    "# Test queries of different complexity (Mock mode for verification)\n",
    "test_queries = [\n",
    "    \"Hello, how are you?\",  # Simple\n",
    "    \"Explain the concept of machine learning\",  # Medium\n",
    "    \"Design and implement a distributed microservices architecture with load balancing and fault tolerance\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"üîÑ Testing Multi-LLM Routing System (Mock Mode)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query} ---\")\n",
    "    \n",
    "    # Test complexity analysis\n",
    "    complexity_result = analyze_complexity.invoke({\"query\": query})\n",
    "    print(f\"üìä Complexity Analysis: {complexity_result}\")\n",
    "    \n",
    "    # Test model recommendation\n",
    "    complexity = complexity_result.split(\": \")[1].split(\" \")[0]\n",
    "    model_rec = get_model_recommendation.invoke({\"complexity\": complexity})\n",
    "    print(f\"ü§ñ Model Recommendation: {model_rec}\")\n",
    "    \n",
    "    # Simulate routing decision\n",
    "    if complexity == \"simple\":\n",
    "        selected_model = \"fast_model\"\n",
    "    elif complexity == \"medium\":\n",
    "        selected_model = \"balanced_model\"\n",
    "    else:\n",
    "        selected_model = \"advanced_model\"\n",
    "    \n",
    "    print(f\"‚úÖ Selected Model: {selected_model}\")\n",
    "    print(f\"üìà Routing Decision: Query routed to {selected_model} based on complexity '{complexity}'\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-LLM routing system verification completed!\")\n",
    "print(\"üìù Note: Full execution requires valid OpenAI API key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Summary - Multi-LLM Routing with LangGraph\n",
    "\n",
    "We've successfully implemented a multi-LLM routing system using LangGraph that:\n",
    "\n",
    "- Analyzes query complexity using heuristics\n",
    "- Routes to appropriate models (fast, balanced, advanced)\n",
    "- Implements fallback mechanisms\n",
    "- Tracks performance metrics\n",
    "- Uses LangGraph for orchestration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['OPENAI_API_KEY'] = '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'\n",
    "#print(\"‚úÖ OpenAI API Key configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Multi-LLM Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-LLM Router initialized!\n",
      "üìä Available models: 3\n",
      "üìä Models: ['fast_model', 'balanced_model', 'quality_model']\n",
      "üìä Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "class MultiLLMRouter:\n",
    "    def __init__(self):\n",
    "        # Define different LLM configurations (simulating different models)\n",
    "        self.models = {\n",
    "            \"fast_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n",
    "                \"cost_per_token\": 0.0001,\n",
    "                \"speed\": 0.5,  # seconds\n",
    "                \"quality\": 0.7,\n",
    "                \"use_case\": \"Simple queries, quick responses\"\n",
    "            },\n",
    "            \"balanced_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.5, max_tokens=200),\n",
    "                \"cost_per_token\": 0.0002,\n",
    "                \"speed\": 1.0,  # seconds\n",
    "                \"quality\": 0.8,\n",
    "                \"use_case\": \"Medium complexity, balanced performance\"\n",
    "            },\n",
    "            \"quality_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.7, max_tokens=500),\n",
    "                \"cost_per_token\": 0.0005,\n",
    "                \"speed\": 2.0,  # seconds\n",
    "                \"quality\": 0.9,\n",
    "                \"use_case\": \"Complex queries, high quality responses\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.routing_history = []\n",
    "        self.cost_tracker = {\"total_cost\": 0, \"requests\": 0}\n",
    "    \n",
    "    def analyze_query_complexity(self, query: str) -> dict:\n",
    "        \"\"\"Analyze query complexity to determine best model.\"\"\"\n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Length factor\n",
    "        if len(query) > 100:\n",
    "            complexity_score += 0.3\n",
    "        elif len(query) > 50:\n",
    "            complexity_score += 0.1\n",
    "        \n",
    "        # Complexity keywords\n",
    "        complex_keywords = [\"analyze\", \"compare\", \"evaluate\", \"complex\", \"detailed\", \"comprehensive\", \"research\"]\n",
    "        if any(keyword in query.lower() for keyword in complex_keywords):\n",
    "            complexity_score += 0.4\n",
    "        \n",
    "        # Question complexity\n",
    "        if \"?\" in query:\n",
    "            complexity_score += 0.2\n",
    "        \n",
    "        # Technical terms\n",
    "        technical_terms = [\"algorithm\", \"architecture\", \"framework\", \"methodology\", \"implementation\"]\n",
    "        if any(term in query.lower() for term in technical_terms):\n",
    "            complexity_score += 0.3\n",
    "        \n",
    "        return {\n",
    "            \"score\": complexity_score,\n",
    "            \"category\": \"simple\" if complexity_score < 0.3 else \"medium\" if complexity_score < 0.6 else \"complex\"\n",
    "        }\n",
    "    \n",
    "    def select_model(self, complexity: dict) -> str:\n",
    "        \"\"\"Select best model based on complexity analysis.\"\"\"\n",
    "        if complexity[\"category\"] == \"simple\":\n",
    "            return \"fast_model\"\n",
    "        elif complexity[\"category\"] == \"medium\":\n",
    "            return \"balanced_model\"\n",
    "        else:\n",
    "            return \"quality_model\"\n",
    "    \n",
    "    def route_query(self, query: str) -> dict:\n",
    "        \"\"\"Route query to appropriate LLM with fallback.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        complexity = self.analyze_query_complexity(query)\n",
    "        \n",
    "        # Select primary model\n",
    "        primary_model = self.select_model(complexity)\n",
    "        \n",
    "        try:\n",
    "            # Try primary model first\n",
    "            model_config = self.models[primary_model]\n",
    "            \n",
    "            # Simulate model response with occasional failures\n",
    "            if random.random() < 0.1:  # 10% failure rate for demo\n",
    "                raise Exception(f\"Primary model {primary_model} temporarily unavailable\")\n",
    "            \n",
    "            # Get response from primary model\n",
    "            response = model_config[\"llm\"].invoke(query)\n",
    "            \n",
    "            # Calculate costs and metrics\n",
    "            estimated_tokens = len(query.split()) + len(response.split())\n",
    "            cost = estimated_tokens * model_config[\"cost_per_token\"]\n",
    "            \n",
    "            # Update cost tracker\n",
    "            self.cost_tracker[\"total_cost\"] += cost\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Log routing decision\n",
    "            routing_info = {\n",
    "                \"query\": query,\n",
    "                \"complexity\": complexity,\n",
    "                \"selected_model\": primary_model,\n",
    "                \"fallback_used\": False,\n",
    "                \"response\": response,\n",
    "                \"cost\": cost,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            self.routing_history.append(routing_info)\n",
    "            \n",
    "            return routing_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to alternative model\n",
    "            print(f\"‚ö†Ô∏è Primary model failed: {str(e)}\")\n",
    "            \n",
    "            # Select fallback model\n",
    "            if primary_model == \"fast_model\":\n",
    "                fallback_model = \"balanced_model\"\n",
    "            elif primary_model == \"balanced_model\":\n",
    "                fallback_model = \"quality_model\"\n",
    "            else:\n",
    "                fallback_model = \"fast_model\"\n",
    "            \n",
    "            try:\n",
    "                fallback_config = self.models[fallback_model]\n",
    "                response = fallback_config[\"llm\"].invoke(query)\n",
    "                \n",
    "                estimated_tokens = len(query.split()) + len(response.split())\n",
    "                cost = estimated_tokens * fallback_config[\"cost_per_token\"]\n",
    "                \n",
    "                self.cost_tracker[\"total_cost\"] += cost\n",
    "                self.cost_tracker[\"requests\"] += 1\n",
    "                \n",
    "                routing_info = {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": fallback_model,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": response,\n",
    "                    \"cost\": cost,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                \n",
    "                self.routing_history.append(routing_info)\n",
    "                \n",
    "                return routing_info\n",
    "                \n",
    "            except Exception as e2:\n",
    "                # Final fallback - return error message\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": None,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": f\"All models are currently unavailable. Error: {str(e2)}\",\n",
    "                    \"cost\": 0,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"error\": True\n",
    "                }\n",
    "    \n",
    "    def get_routing_stats(self):\n",
    "        \"\"\"Get routing statistics.\"\"\"\n",
    "        if not self.routing_history:\n",
    "            return \"No queries processed yet\"\n",
    "        \n",
    "        total_queries = len(self.routing_history)\n",
    "        successful_queries = len([r for r in self.routing_history if not r.get(\"error\", False)])\n",
    "        fallback_usage = len([r for r in self.routing_history if r.get(\"fallback_used\", False)])\n",
    "        \n",
    "        model_usage = {}\n",
    "        for routing in self.routing_history:\n",
    "            model = routing.get(\"selected_model\", \"unknown\")\n",
    "            model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"successful_queries\": successful_queries,\n",
    "            \"success_rate\": successful_queries / total_queries * 100,\n",
    "            \"fallback_usage\": fallback_usage,\n",
    "            \"fallback_rate\": fallback_usage / total_queries * 100,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"total_cost\": self.cost_tracker[\"total_cost\"],\n",
    "            \"avg_cost_per_query\": self.cost_tracker[\"total_cost\"] / total_queries\n",
    "        }\n",
    "\n",
    "# Initialize router\n",
    "router = MultiLLMRouter()\n",
    "\n",
    "print(\"‚úÖ Multi-LLM Router initialized!\")\n",
    "print(f\"üìä Available models: {len(router.models)}\")\n",
    "print(f\"üìä Models: {list(router.models.keys())}\")\n",
    "print(f\"üìä Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test LLM Routing System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ TESTING LLM ROUTING SYSTEM:\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello ---\n",
      "Complexity: simple (score: 0.00)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: , I am a 24 year old female and I am currently experiencing a lot of stress and anxiety. I have been...\n",
      "Cost: $0.0090\n",
      "Response Time: 1.92s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 2: What is machine learning? ---\n",
      "‚ö†Ô∏è Primary model failed: Primary model fast_model temporarily unavailable\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: balanced_model\n",
      "Fallback Used: True\n",
      "Response: \n",
      "\n",
      "Machine learning is a subset of artificial intelligence that involves the development of algorithm...\n",
      "Cost: $0.0210\n",
      "Response Time: 4.15s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 3: Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments... ---\n",
      "Complexity: complex (score: 1.00)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Distributed machine learning refers to the use of multiple machines or nodes to perform data analy...\n",
      "Cost: $0.2255\n",
      "Response Time: 5.85s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 4: How are you? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "I am an AI and do not have the ability to feel emotions. But thank you for asking. How can I assis...\n",
      "Cost: $0.0026\n",
      "Response Time: 2.90s\n",
      "‚úÖ Success\n",
      "\n",
      "--- Test 5: Compare and evaluate different deep learning frameworks for natural language processing tasks... ---\n",
      "Complexity: complex (score: 0.80)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Deep learning frameworks have become essential tools for natural language processing (NLP) tasks, ...\n",
      "Cost: $0.2060\n",
      "Response Time: 5.72s\n",
      "‚úÖ Success\n",
      "\n",
      "üìä ROUTING STATISTICS:\n",
      "============================================================\n",
      "Total Queries: 10\n",
      "Success Rate: 100.0%\n",
      "Fallback Rate: 30.0%\n",
      "Total Cost: $0.7200\n",
      "Average Cost per Query: $0.0720\n",
      "Model Usage: {'fast_model': 5, 'balanced_model': 2, 'quality_model': 3}\n"
     ]
    }
   ],
   "source": [
    "# Test routing system with different query complexities\n",
    "test_queries = [\n",
    "    \"Hello\",  # Simple\n",
    "    \"What is machine learning?\",  # Medium\n",
    "    \"Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments\",  # Complex\n",
    "    \"How are you?\",  # Simple\n",
    "    \"Compare and evaluate different deep learning frameworks for natural language processing tasks\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"üîÑ TESTING LLM ROUTING SYSTEM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query[:200]}{'...' if len(query) > 50 else ''} ---\")\n",
    "    \n",
    "    result = router.route_query(query)\n",
    "    \n",
    "    print(f\"Complexity: {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\")\n",
    "    print(f\"Selected Model: {result['selected_model']}\")\n",
    "    print(f\"Fallback Used: {result['fallback_used']}\")\n",
    "    print(f\"Response: {result['response'][:100]}...\")\n",
    "    print(f\"Cost: ${result['cost']:.4f}\")\n",
    "    print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"‚ùå Error: {result.get('error')}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Success\")\n",
    "\n",
    "# Show routing statistics\n",
    "print(f\"\\nüìä ROUTING STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "stats = router.get_routing_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
    "print(f\"Fallback Rate: {stats['fallback_rate']:.1f}%\")\n",
    "print(f\"Total Cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"Average Cost per Query: ${stats['avg_cost_per_query']:.4f}\")\n",
    "print(f\"Model Usage: {stats['model_usage']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive LLM Router ready!\n",
      "üìä Router: MultiLLMRouter\n",
      "üìä Models: 3 available\n",
      "üìä Routing: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create interactive LLM routing system\n",
    "class InteractiveLLMRouter:\n",
    "    def __init__(self):\n",
    "        self.router = router\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_query(self, query, history):\n",
    "        \"\"\"Process query through LLM routing system.\"\"\"\n",
    "        if not query.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Get routed response\n",
    "        result = self.router.route_query(query)\n",
    "        \n",
    "        # Format response for display\n",
    "        if result.get('error'):\n",
    "            response = f\"‚ùå **Error:** {result['response']}\"\n",
    "        else:\n",
    "            response = f\"\"\"**LLM Response:**\n",
    "{result['response']}\n",
    "\n",
    "**Routing Details:**\n",
    "‚Ä¢ **Complexity:** {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\n",
    "‚Ä¢ **Model Used:** {result['selected_model']}\n",
    "‚Ä¢ **Fallback Used:** {'Yes' if result['fallback_used'] else 'No'}\n",
    "‚Ä¢ **Cost:** ${result['cost']:.4f}\n",
    "‚Ä¢ **Response Time:** {result['response_time']:.2f}s\"\"\"\n",
    "        \n",
    "        # Update history\n",
    "        history.append([query, response])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    def get_system_stats(self):\n",
    "        \"\"\"Get current system statistics.\"\"\"\n",
    "        stats = self.router.get_routing_stats()\n",
    "        if isinstance(stats, str):\n",
    "            return \"üìä LLM Router: Ready for queries\"\n",
    "        \n",
    "        return f\"üìä LLM Router: {stats['total_queries']} queries | {stats['success_rate']:.1f}% success | ${stats['total_cost']:.4f} total cost\"\n",
    "\n",
    "# Initialize interactive system\n",
    "interactive_router = InteractiveLLMRouter()\n",
    "\n",
    "print(\"‚úÖ Interactive LLM Router ready!\")\n",
    "print(f\"üìä Router: {type(router).__name__}\")\n",
    "print(f\"üìä Models: {len(router.models)} available\")\n",
    "print(f\"üìä Routing: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"LLM Routing Demo\") as demo:\n",
    "    gr.Markdown(\"# üöÄ Multi-LLM Routing Demo - See Intelligent Model Selection!\")\n",
    "    gr.Markdown(\"**This demo shows how queries are intelligently routed to different LLM models based on complexity!**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label=\"LLM-Routed Chat\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Query\", placeholder=\"Try: 'Hello' or 'Analyze machine learning algorithms'\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Route to LLM\")\n",
    "                clear_btn = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            system_stats = gr.Textbox(label=\"System Statistics\", value=interactive_router.get_system_stats(), interactive=False)\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### üéØ Try These Queries:\")\n",
    "            gr.Markdown(\"‚Ä¢ `Hello` - Simple query ‚Üí Fast Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `What is AI?` - Medium complexity ‚Üí Balanced Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `Analyze machine learning algorithms` - Complex ‚Üí Quality Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `Compare deep learning frameworks` - Complex ‚Üí Quality Model\")\n",
    "            gr.Markdown(\"‚Ä¢ `How are you?` - Simple ‚Üí Fast Model\")\n",
    "            \n",
    "            gr.Markdown(\"### ü§ñ Available Models:\")\n",
    "            gr.Markdown(\"‚Ä¢ **‚ö° Fast Model** - Quick responses, low cost\")\n",
    "            gr.Markdown(\"‚Ä¢ **‚öñÔ∏è Balanced Model** - Good performance, moderate cost\")\n",
    "            gr.Markdown(\"‚Ä¢ **üéØ Quality Model** - High quality, higher cost\")\n",
    "            \n",
    "            gr.Markdown(\"### üß† Routing Logic:\")\n",
    "            gr.Markdown(\"‚Ä¢ **Query Length** - Longer queries = higher complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Keywords** - Technical terms increase complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Question Types** - Questions get medium complexity\")\n",
    "            gr.Markdown(\"‚Ä¢ **Fallback System** - Automatic failover if primary model fails\")\n",
    "            \n",
    "            gr.Markdown(\"### üìä Features:\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Intelligent model selection\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Automatic fallbacks\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Cost optimization\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Performance tracking\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Complexity analysis\")\n",
    "            gr.Markdown(\"‚Ä¢ ‚úÖ Real-time routing\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def submit_query(query, history):\n",
    "        if query.strip():\n",
    "            new_history, _ = interactive_router.process_query(query, history or [])\n",
    "            return new_history, \"\", interactive_router.get_system_stats()\n",
    "        return history, \"\", interactive_router.get_system_stats()\n",
    "    \n",
    "    def clear_chat():\n",
    "        return [], interactive_router.get_system_stats()\n",
    "    \n",
    "    # Connect events\n",
    "    msg.submit(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    send_btn.click(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, system_stats])\n",
    "\n",
    "print(\"üöÄ LLM Routing Demo ready!\")\n",
    "print(\"üéØ Launch the demo to see intelligent model selection in action!\")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary - What We've Built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ MULTI-LLM ROUTING EXERCISE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ What We've Demonstrated:\")\n",
    "print(\"‚Ä¢ Intelligent LLM routing based on query complexity\")\n",
    "print(\"‚Ä¢ Dynamic model selection with fallback strategies\")\n",
    "print(\"‚Ä¢ Cost optimization and performance tracking\")\n",
    "print(\"‚Ä¢ Real-time routing decisions\")\n",
    "print(\"‚Ä¢ Interactive demo with Gradio\")\n",
    "print(\"‚Ä¢ Real API integration with OpenAI\")\n",
    "\n",
    "print(\"\\nüöÄ Key Learning Outcomes:\")\n",
    "print(\"‚Ä¢ Query complexity analysis enables smart routing\")\n",
    "print(\"‚Ä¢ Fallback systems ensure high availability\")\n",
    "print(\"‚Ä¢ Cost optimization through model selection\")\n",
    "print(\"‚Ä¢ Performance tracking improves system efficiency\")\n",
    "print(\"‚Ä¢ Real API integration with OpenAI\")\n",
    "print(\"‚Ä¢ Practical hands-on implementation\")\n",
    "\n",
    "print(\"\\nüéØ Production-Ready Features:\")\n",
    "print(\"‚Ä¢ Multi-model LLM routing system\")\n",
    "print(\"‚Ä¢ Complexity-based model selection\")\n",
    "print(\"‚Ä¢ Automatic fallback mechanisms\")\n",
    "print(\"‚Ä¢ Cost and performance tracking\")\n",
    "print(\"‚Ä¢ Real-time routing decisions\")\n",
    "print(\"‚Ä¢ Interactive user interface\")\n",
    "\n",
    "print(\"\\nüìä System Statistics:\")\n",
    "stats = router.get_routing_stats()\n",
    "if isinstance(stats, dict):\n",
    "    print(f\"‚Ä¢ Total queries: {stats['total_queries']}\")\n",
    "    print(f\"‚Ä¢ Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"‚Ä¢ Fallback rate: {stats['fallback_rate']:.1f}%\")\n",
    "    print(f\"‚Ä¢ Total cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"‚Ä¢ Available models: {len(router.models)}\")\n",
    "    print(f\"‚Ä¢ Routing strategy: Complexity-based with fallbacks\")\n",
    "else:\n",
    "    print(\"‚Ä¢ System ready for queries\")\n",
    "    print(f\"‚Ä¢ Available models: {len(router.models)}\")\n",
    "    print(f\"‚Ä¢ Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
