{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Day 3 — Exercise 7: Multi-LLM Routing and Fallbacks\n",
    "## Practical Hands-on Implementation with Intelligent Model Selection\n",
    "\n",
    "### ✅ Objectives:\n",
    "- Build intelligent LLM routing system based on query complexity\n",
    "- Implement dynamic model selection with fallbacks\n",
    "- Create cost optimization and performance tracking\n",
    "- Demonstrate working LLM routing with real-time interaction\n",
    "- Show practical enterprise applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "✅ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-core\n",
    "!pip install -q gradio\n",
    "print(\"✅ All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API Key configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "print(\"✅ OpenAI API Key configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Multi-LLM Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_9412/1710044252.py:10: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-LLM Router initialized!\n",
      "📊 Available models: 3\n",
      "📊 Models: ['fast_model', 'balanced_model', 'quality_model']\n",
      "📊 Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "class MultiLLMRouter:\n",
    "    def __init__(self):\n",
    "        # Define different LLM configurations (simulating different models)\n",
    "        self.models = {\n",
    "            \"fast_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.3, max_tokens=100),\n",
    "                \"cost_per_token\": 0.0001,\n",
    "                \"speed\": 0.5,  # seconds\n",
    "                \"quality\": 0.7,\n",
    "                \"use_case\": \"Simple queries, quick responses\"\n",
    "            },\n",
    "            \"balanced_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.5, max_tokens=200),\n",
    "                \"cost_per_token\": 0.0002,\n",
    "                \"speed\": 1.0,  # seconds\n",
    "                \"quality\": 0.8,\n",
    "                \"use_case\": \"Medium complexity, balanced performance\"\n",
    "            },\n",
    "            \"quality_model\": {\n",
    "                \"llm\": OpenAI(temperature=0.7, max_tokens=500),\n",
    "                \"cost_per_token\": 0.0005,\n",
    "                \"speed\": 2.0,  # seconds\n",
    "                \"quality\": 0.9,\n",
    "                \"use_case\": \"Complex queries, high quality responses\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.routing_history = []\n",
    "        self.cost_tracker = {\"total_cost\": 0, \"requests\": 0}\n",
    "    \n",
    "    def analyze_query_complexity(self, query: str) -> dict:\n",
    "        \"\"\"Analyze query complexity to determine best model.\"\"\"\n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Length factor\n",
    "        if len(query) > 100:\n",
    "            complexity_score += 0.3\n",
    "        elif len(query) > 50:\n",
    "            complexity_score += 0.1\n",
    "        \n",
    "        # Complexity keywords\n",
    "        complex_keywords = [\"analyze\", \"compare\", \"evaluate\", \"complex\", \"detailed\", \"comprehensive\", \"research\"]\n",
    "        if any(keyword in query.lower() for keyword in complex_keywords):\n",
    "            complexity_score += 0.4\n",
    "        \n",
    "        # Question complexity\n",
    "        if \"?\" in query:\n",
    "            complexity_score += 0.2\n",
    "        \n",
    "        # Technical terms\n",
    "        technical_terms = [\"algorithm\", \"architecture\", \"framework\", \"methodology\", \"implementation\"]\n",
    "        if any(term in query.lower() for term in technical_terms):\n",
    "            complexity_score += 0.3\n",
    "        \n",
    "        return {\n",
    "            \"score\": complexity_score,\n",
    "            \"category\": \"simple\" if complexity_score < 0.3 else \"medium\" if complexity_score < 0.6 else \"complex\"\n",
    "        }\n",
    "    \n",
    "    def select_model(self, complexity: dict) -> str:\n",
    "        \"\"\"Select best model based on complexity analysis.\"\"\"\n",
    "        if complexity[\"category\"] == \"simple\":\n",
    "            return \"fast_model\"\n",
    "        elif complexity[\"category\"] == \"medium\":\n",
    "            return \"balanced_model\"\n",
    "        else:\n",
    "            return \"quality_model\"\n",
    "    \n",
    "    def route_query(self, query: str) -> dict:\n",
    "        \"\"\"Route query to appropriate LLM with fallback.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        complexity = self.analyze_query_complexity(query)\n",
    "        \n",
    "        # Select primary model\n",
    "        primary_model = self.select_model(complexity)\n",
    "        \n",
    "        try:\n",
    "            # Try primary model first\n",
    "            model_config = self.models[primary_model]\n",
    "            \n",
    "            # Simulate model response with occasional failures\n",
    "            if random.random() < 0.1:  # 10% failure rate for demo\n",
    "                raise Exception(f\"Primary model {primary_model} temporarily unavailable\")\n",
    "            \n",
    "            # Get response from primary model\n",
    "            response = model_config[\"llm\"].invoke(query)\n",
    "            \n",
    "            # Calculate costs and metrics\n",
    "            estimated_tokens = len(query.split()) + len(response.split())\n",
    "            cost = estimated_tokens * model_config[\"cost_per_token\"]\n",
    "            \n",
    "            # Update cost tracker\n",
    "            self.cost_tracker[\"total_cost\"] += cost\n",
    "            self.cost_tracker[\"requests\"] += 1\n",
    "            \n",
    "            # Log routing decision\n",
    "            routing_info = {\n",
    "                \"query\": query,\n",
    "                \"complexity\": complexity,\n",
    "                \"selected_model\": primary_model,\n",
    "                \"fallback_used\": False,\n",
    "                \"response\": response,\n",
    "                \"cost\": cost,\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"timestamp\": time.time()\n",
    "            }\n",
    "            \n",
    "            self.routing_history.append(routing_info)\n",
    "            \n",
    "            return routing_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to alternative model\n",
    "            print(f\"⚠️ Primary model failed: {str(e)}\")\n",
    "            \n",
    "            # Select fallback model\n",
    "            if primary_model == \"fast_model\":\n",
    "                fallback_model = \"balanced_model\"\n",
    "            elif primary_model == \"balanced_model\":\n",
    "                fallback_model = \"quality_model\"\n",
    "            else:\n",
    "                fallback_model = \"fast_model\"\n",
    "            \n",
    "            try:\n",
    "                fallback_config = self.models[fallback_model]\n",
    "                response = fallback_config[\"llm\"].invoke(query)\n",
    "                \n",
    "                estimated_tokens = len(query.split()) + len(response.split())\n",
    "                cost = estimated_tokens * fallback_config[\"cost_per_token\"]\n",
    "                \n",
    "                self.cost_tracker[\"total_cost\"] += cost\n",
    "                self.cost_tracker[\"requests\"] += 1\n",
    "                \n",
    "                routing_info = {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": fallback_model,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": response,\n",
    "                    \"cost\": cost,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                \n",
    "                self.routing_history.append(routing_info)\n",
    "                \n",
    "                return routing_info\n",
    "                \n",
    "            except Exception as e2:\n",
    "                # Final fallback - return error message\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"selected_model\": None,\n",
    "                    \"fallback_used\": True,\n",
    "                    \"response\": f\"All models are currently unavailable. Error: {str(e2)}\",\n",
    "                    \"cost\": 0,\n",
    "                    \"response_time\": time.time() - start_time,\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"error\": True\n",
    "                }\n",
    "    \n",
    "    def get_routing_stats(self):\n",
    "        \"\"\"Get routing statistics.\"\"\"\n",
    "        if not self.routing_history:\n",
    "            return \"No queries processed yet\"\n",
    "        \n",
    "        total_queries = len(self.routing_history)\n",
    "        successful_queries = len([r for r in self.routing_history if not r.get(\"error\", False)])\n",
    "        fallback_usage = len([r for r in self.routing_history if r.get(\"fallback_used\", False)])\n",
    "        \n",
    "        model_usage = {}\n",
    "        for routing in self.routing_history:\n",
    "            model = routing.get(\"selected_model\", \"unknown\")\n",
    "            model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"successful_queries\": successful_queries,\n",
    "            \"success_rate\": successful_queries / total_queries * 100,\n",
    "            \"fallback_usage\": fallback_usage,\n",
    "            \"fallback_rate\": fallback_usage / total_queries * 100,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"total_cost\": self.cost_tracker[\"total_cost\"],\n",
    "            \"avg_cost_per_query\": self.cost_tracker[\"total_cost\"] / total_queries\n",
    "        }\n",
    "\n",
    "# Initialize router\n",
    "router = MultiLLMRouter()\n",
    "\n",
    "print(\"✅ Multi-LLM Router initialized!\")\n",
    "print(f\"📊 Available models: {len(router.models)}\")\n",
    "print(f\"📊 Models: {list(router.models.keys())}\")\n",
    "print(f\"📊 Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test LLM Routing System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 TESTING LLM ROUTING SYSTEM:\n",
      "============================================================\n",
      "\n",
      "--- Test 1: Hello ---\n",
      "Complexity: simple (score: 0.00)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: , I am looking for a reliable and experienced writer who can write high-quality articles and web con...\n",
      "Cost: $0.0086\n",
      "Response Time: 1.90s\n",
      "✅ Success\n",
      "\n",
      "--- Test 2: What is machine learning? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Machine learning is a subset of artificial intelligence that involves the use of algorithms and st...\n",
      "Cost: $0.0094\n",
      "Response Time: 1.84s\n",
      "✅ Success\n",
      "\n",
      "--- Test 3: Analyze the comprehensive methodology for implemen... ---\n",
      "Complexity: complex (score: 1.00)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Distributed machine learning algorithms are becoming increasingly popular due to their ability to ...\n",
      "Cost: $0.2215\n",
      "Response Time: 4.81s\n",
      "✅ Success\n",
      "\n",
      "--- Test 4: How are you? ---\n",
      "Complexity: simple (score: 0.20)\n",
      "Selected Model: fast_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "I am an AI language model created by OpenAI, so I do not have the ability to feel emotions. But th...\n",
      "Cost: $0.0035\n",
      "Response Time: 1.33s\n",
      "✅ Success\n",
      "\n",
      "--- Test 5: Compare and evaluate different deep learning frame... ---\n",
      "Complexity: complex (score: 0.80)\n",
      "Selected Model: quality_model\n",
      "Fallback Used: False\n",
      "Response: \n",
      "\n",
      "Deep learning frameworks have become an essential tool for natural language processing tasks, as t...\n",
      "Cost: $0.2135\n",
      "Response Time: 4.40s\n",
      "✅ Success\n",
      "\n",
      "📊 ROUTING STATISTICS:\n",
      "============================================================\n",
      "Total Queries: 5\n",
      "Success Rate: 100.0%\n",
      "Fallback Rate: 0.0%\n",
      "Total Cost: $0.4565\n",
      "Average Cost per Query: $0.0913\n",
      "Model Usage: {'fast_model': 3, 'quality_model': 2}\n"
     ]
    }
   ],
   "source": [
    "# Test routing system with different query complexities\n",
    "test_queries = [\n",
    "    \"Hello\",  # Simple\n",
    "    \"What is machine learning?\",  # Medium\n",
    "    \"Analyze the comprehensive methodology for implementing distributed machine learning algorithms in cloud environments\",  # Complex\n",
    "    \"How are you?\",  # Simple\n",
    "    \"Compare and evaluate different deep learning frameworks for natural language processing tasks\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"🔄 TESTING LLM ROUTING SYSTEM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Test {i}: {query[:50]}{'...' if len(query) > 50 else ''} ---\")\n",
    "    \n",
    "    result = router.route_query(query)\n",
    "    \n",
    "    print(f\"Complexity: {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\")\n",
    "    print(f\"Selected Model: {result['selected_model']}\")\n",
    "    print(f\"Fallback Used: {result['fallback_used']}\")\n",
    "    print(f\"Response: {result['response'][:100]}...\")\n",
    "    print(f\"Cost: ${result['cost']:.4f}\")\n",
    "    print(f\"Response Time: {result['response_time']:.2f}s\")\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"❌ Error: {result.get('error')}\")\n",
    "    else:\n",
    "        print(\"✅ Success\")\n",
    "\n",
    "# Show routing statistics\n",
    "print(f\"\\n📊 ROUTING STATISTICS:\")\n",
    "print(\"=\" * 60)\n",
    "stats = router.get_routing_stats()\n",
    "print(f\"Total Queries: {stats['total_queries']}\")\n",
    "print(f\"Success Rate: {stats['success_rate']:.1f}%\")\n",
    "print(f\"Fallback Rate: {stats['fallback_rate']:.1f}%\")\n",
    "print(f\"Total Cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"Average Cost per Query: ${stats['avg_cost_per_query']:.4f}\")\n",
    "print(f\"Model Usage: {stats['model_usage']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interactive LLM Routing Demo with Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Interactive LLM Router ready!\n",
      "📊 Router: MultiLLMRouter\n",
      "📊 Models: 3 available\n",
      "📊 Routing: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create interactive LLM routing system\n",
    "class InteractiveLLMRouter:\n",
    "    def __init__(self):\n",
    "        self.router = router\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_query(self, query, history):\n",
    "        \"\"\"Process query through LLM routing system.\"\"\"\n",
    "        if not query.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Get routed response\n",
    "        result = self.router.route_query(query)\n",
    "        \n",
    "        # Format response for display\n",
    "        if result.get('error'):\n",
    "            response = f\"❌ **Error:** {result['response']}\"\n",
    "        else:\n",
    "            response = f\"\"\"**LLM Response:**\n",
    "{result['response']}\n",
    "\n",
    "**Routing Details:**\n",
    "• **Complexity:** {result['complexity']['category']} (score: {result['complexity']['score']:.2f})\n",
    "• **Model Used:** {result['selected_model']}\n",
    "• **Fallback Used:** {'Yes' if result['fallback_used'] else 'No'}\n",
    "• **Cost:** ${result['cost']:.4f}\n",
    "• **Response Time:** {result['response_time']:.2f}s\"\"\"\n",
    "        \n",
    "        # Update history\n",
    "        history.append([query, response])\n",
    "        \n",
    "        return history, \"\"\n",
    "    \n",
    "    def get_system_stats(self):\n",
    "        \"\"\"Get current system statistics.\"\"\"\n",
    "        stats = self.router.get_routing_stats()\n",
    "        if isinstance(stats, str):\n",
    "            return \"📊 LLM Router: Ready for queries\"\n",
    "        \n",
    "        return f\"📊 LLM Router: {stats['total_queries']} queries | {stats['success_rate']:.1f}% success | ${stats['total_cost']:.4f} total cost\"\n",
    "\n",
    "# Initialize interactive system\n",
    "interactive_router = InteractiveLLMRouter()\n",
    "\n",
    "print(\"✅ Interactive LLM Router ready!\")\n",
    "print(f\"📊 Router: {type(router).__name__}\")\n",
    "print(f\"📊 Models: {len(router.models)} available\")\n",
    "print(f\"📊 Routing: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LLM Routing Demo ready!\n",
      "🎯 Launch the demo to see intelligent model selection in action!\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://626f3709add6ccb707.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://626f3709add6ccb707.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://626f3709add6ccb707.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"LLM Routing Demo\") as demo:\n",
    "    gr.Markdown(\"# 🚀 Multi-LLM Routing Demo - See Intelligent Model Selection!\")\n",
    "    gr.Markdown(\"**This demo shows how queries are intelligently routed to different LLM models based on complexity!**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label=\"LLM-Routed Chat\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Query\", placeholder=\"Try: 'Hello' or 'Analyze machine learning algorithms'\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"Route to LLM\")\n",
    "                clear_btn = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            system_stats = gr.Textbox(label=\"System Statistics\", value=interactive_router.get_system_stats(), interactive=False)\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 🎯 Try These Queries:\")\n",
    "            gr.Markdown(\"• `Hello` - Simple query → Fast Model\")\n",
    "            gr.Markdown(\"• `What is AI?` - Medium complexity → Balanced Model\")\n",
    "            gr.Markdown(\"• `Analyze machine learning algorithms` - Complex → Quality Model\")\n",
    "            gr.Markdown(\"• `Compare deep learning frameworks` - Complex → Quality Model\")\n",
    "            gr.Markdown(\"• `How are you?` - Simple → Fast Model\")\n",
    "            \n",
    "            gr.Markdown(\"### 🤖 Available Models:\")\n",
    "            gr.Markdown(\"• **⚡ Fast Model** - Quick responses, low cost\")\n",
    "            gr.Markdown(\"• **⚖️ Balanced Model** - Good performance, moderate cost\")\n",
    "            gr.Markdown(\"• **🎯 Quality Model** - High quality, higher cost\")\n",
    "            \n",
    "            gr.Markdown(\"### 🧠 Routing Logic:\")\n",
    "            gr.Markdown(\"• **Query Length** - Longer queries = higher complexity\")\n",
    "            gr.Markdown(\"• **Keywords** - Technical terms increase complexity\")\n",
    "            gr.Markdown(\"• **Question Types** - Questions get medium complexity\")\n",
    "            gr.Markdown(\"• **Fallback System** - Automatic failover if primary model fails\")\n",
    "            \n",
    "            gr.Markdown(\"### 📊 Features:\")\n",
    "            gr.Markdown(\"• ✅ Intelligent model selection\")\n",
    "            gr.Markdown(\"• ✅ Automatic fallbacks\")\n",
    "            gr.Markdown(\"• ✅ Cost optimization\")\n",
    "            gr.Markdown(\"• ✅ Performance tracking\")\n",
    "            gr.Markdown(\"• ✅ Complexity analysis\")\n",
    "            gr.Markdown(\"• ✅ Real-time routing\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def submit_query(query, history):\n",
    "        if query.strip():\n",
    "            new_history, _ = interactive_router.process_query(query, history or [])\n",
    "            return new_history, \"\", interactive_router.get_system_stats()\n",
    "        return history, \"\", interactive_router.get_system_stats()\n",
    "    \n",
    "    def clear_chat():\n",
    "        return [], interactive_router.get_system_stats()\n",
    "    \n",
    "    # Connect events\n",
    "    msg.submit(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    send_btn.click(submit_query, [msg, chatbot], [chatbot, msg, system_stats])\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, system_stats])\n",
    "\n",
    "print(\"🚀 LLM Routing Demo ready!\")\n",
    "print(\"🎯 Launch the demo to see intelligent model selection in action!\")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary - What We've Built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 MULTI-LLM ROUTING EXERCISE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "✅ What We've Demonstrated:\n",
      "• Intelligent LLM routing based on query complexity\n",
      "• Dynamic model selection with fallback strategies\n",
      "• Cost optimization and performance tracking\n",
      "• Real-time routing decisions\n",
      "• Interactive demo with Gradio\n",
      "• Real API integration with OpenAI\n",
      "\n",
      "🚀 Key Learning Outcomes:\n",
      "• Query complexity analysis enables smart routing\n",
      "• Fallback systems ensure high availability\n",
      "• Cost optimization through model selection\n",
      "• Performance tracking improves system efficiency\n",
      "• Real API integration with OpenAI\n",
      "• Practical hands-on implementation\n",
      "\n",
      "🎯 Production-Ready Features:\n",
      "• Multi-model LLM routing system\n",
      "• Complexity-based model selection\n",
      "• Automatic fallback mechanisms\n",
      "• Cost and performance tracking\n",
      "• Real-time routing decisions\n",
      "• Interactive user interface\n",
      "\n",
      "📊 System Statistics:\n",
      "• Total queries: 5\n",
      "• Success rate: 100.0%\n",
      "• Fallback rate: 0.0%\n",
      "• Total cost: $0.4565\n",
      "• Available models: 3\n",
      "• Routing strategy: Complexity-based with fallbacks\n"
     ]
    }
   ],
   "source": [
    "print(\"🎉 MULTI-LLM ROUTING EXERCISE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✅ What We've Demonstrated:\")\n",
    "print(\"• Intelligent LLM routing based on query complexity\")\n",
    "print(\"• Dynamic model selection with fallback strategies\")\n",
    "print(\"• Cost optimization and performance tracking\")\n",
    "print(\"• Real-time routing decisions\")\n",
    "print(\"• Interactive demo with Gradio\")\n",
    "print(\"• Real API integration with OpenAI\")\n",
    "\n",
    "print(\"\\n🚀 Key Learning Outcomes:\")\n",
    "print(\"• Query complexity analysis enables smart routing\")\n",
    "print(\"• Fallback systems ensure high availability\")\n",
    "print(\"• Cost optimization through model selection\")\n",
    "print(\"• Performance tracking improves system efficiency\")\n",
    "print(\"• Real API integration with OpenAI\")\n",
    "print(\"• Practical hands-on implementation\")\n",
    "\n",
    "print(\"\\n🎯 Production-Ready Features:\")\n",
    "print(\"• Multi-model LLM routing system\")\n",
    "print(\"• Complexity-based model selection\")\n",
    "print(\"• Automatic fallback mechanisms\")\n",
    "print(\"• Cost and performance tracking\")\n",
    "print(\"• Real-time routing decisions\")\n",
    "print(\"• Interactive user interface\")\n",
    "\n",
    "print(\"\\n📊 System Statistics:\")\n",
    "stats = router.get_routing_stats()\n",
    "if isinstance(stats, dict):\n",
    "    print(f\"• Total queries: {stats['total_queries']}\")\n",
    "    print(f\"• Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"• Fallback rate: {stats['fallback_rate']:.1f}%\")\n",
    "    print(f\"• Total cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"• Available models: {len(router.models)}\")\n",
    "    print(f\"• Routing strategy: Complexity-based with fallbacks\")\n",
    "else:\n",
    "    print(\"• System ready for queries\")\n",
    "    print(f\"• Available models: {len(router.models)}\")\n",
    "    print(f\"• Routing strategy: Complexity-based with fallbacks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
