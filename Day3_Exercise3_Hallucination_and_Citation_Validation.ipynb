{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 â€” Exercise 3: Hallucination and Citation Validation\n",
    "\n",
    "## ðŸŽ¯ **Learning Objective**\n",
    "Detect and mitigate hallucinations in RAG outputs by implementing automated checks for hallucination and citation support, ensuring answers are grounded in retrieved documents with human-in-the-loop validation.\n",
    "\n",
    "## ðŸ“‹ **Exercise Structure & Navigation**\n",
    "\n",
    "### **ðŸ§­ Navigation Guide**\n",
    "| Section | What You'll Do | Expected Outcome | Time |\n",
    "|---------|----------------|------------------|------|\n",
    "| **Theory & Foundation** | Understand hallucination types and detection methods | Knowledge of validation frameworks | 15 min |\n",
    "| **Simple Implementation** | Build basic hallucination detection | Working validation system | 30 min |\n",
    "| **Intermediate Level** | Add automated citation validation | Advanced detection capabilities | 45 min |\n",
    "| **Advanced Implementation** | Human-in-the-loop validation | Production-ready correction system | 30 min |\n",
    "| **Enterprise Integration** | LightLLM hallucination detection | Complete validation pipeline | 20 min |\n",
    "\n",
    "### **ðŸ” Code Block Navigation**\n",
    "Each code block includes:\n",
    "- **ðŸŽ¯ Purpose**: What the code accomplishes\n",
    "- **ðŸ“Š Expected Output**: What you should see\n",
    "- **ðŸ’¡ Interpretation**: How to understand the results\n",
    "- **âš ï¸ Troubleshooting**: Common issues and solutions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š **Theory & Foundation: Understanding Hallucination in RAG Systems**\n",
    "\n",
    "### **What is Hallucination in RAG?**\n",
    "\n",
    "**Hallucination** refers to the generation of information that is:\n",
    "- **Factually incorrect**: Contradicts the retrieved context\n",
    "- **Not grounded**: Not supported by the source documents\n",
    "- **Fabricated**: Completely made up by the language model\n",
    "- **Misleading**: Partially correct but contains errors\n",
    "\n",
    "### **Types of Hallucinations**\n",
    "\n",
    "#### **1. Contextual Hallucination**\n",
    "- **Definition**: Information not present in the retrieved context\n",
    "- **Example**: Answer mentions \"Company X was founded in 1995\" when context only says \"Company X is a technology company\"\n",
    "- **Detection**: Check if claims are supported by retrieved documents\n",
    "\n",
    "#### **2. Factual Hallucination**\n",
    "- **Definition**: Incorrect factual information\n",
    "- **Example**: \"The capital of France is London\" (should be Paris)\n",
    "- **Detection**: Cross-reference with authoritative sources\n",
    "\n",
    "#### **3. Temporal Hallucination**\n",
    "- **Definition**: Incorrect time-related information\n",
    "- **Example**: \"The company launched in 2023\" when context says \"2022\"\n",
    "- **Detection**: Extract and validate temporal entities\n",
    "\n",
    "#### **4. Numerical Hallucination**\n",
    "- **Definition**: Incorrect numbers, statistics, or measurements\n",
    "- **Example**: \"Revenue increased by 150%\" when context says \"50%\"\n",
    "- **Detection**: Extract and validate numerical claims\n",
    "\n",
    "### **Citation Validation Framework**\n",
    "\n",
    "#### **Citation Quality Metrics**\n",
    "1. **Citation Accuracy**: Do citations support the claims?\n",
    "2. **Citation Completeness**: Are all claims supported by citations?\n",
    "3. **Citation Relevance**: Are cited passages relevant to the claim?\n",
    "4. **Citation Attribution**: Are sources properly attributed?\n",
    "\n",
    "#### **Validation Approaches**\n",
    "1. **Automated Detection**: Rule-based and ML-based methods\n",
    "2. **Human-in-the-Loop**: Expert review and correction\n",
    "3. **Hybrid Approach**: Combine automated and human validation\n",
    "\n",
    "### **Enterprise Considerations**\n",
    "\n",
    "#### **Risk Assessment**\n",
    "- **High-Risk Domains**: Medical, legal, financial advice\n",
    "- **Compliance Requirements**: Regulatory standards for accuracy\n",
    "- **Reputation Management**: Brand protection through accuracy\n",
    "- **Liability Reduction**: Minimizing misinformation risks\n",
    "\n",
    "#### **Quality Assurance**\n",
    "- **Validation Pipelines**: Automated quality checks\n",
    "- **Human Review**: Expert validation for critical content\n",
    "- **Audit Trails**: Documentation of validation decisions\n",
    "- **Continuous Monitoring**: Ongoing quality assessment\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **Simple Implementation: Basic Hallucination Detection**\n",
    "\n",
    "### **Step 1: Setting Up the Environment**\n",
    "\n",
    "**ðŸŽ¯ Purpose**: Import necessary libraries and set up the basic environment for hallucination detection.\n",
    "\n",
    "**ðŸ“Š Expected Output**: Confirmation that all libraries are imported and basic setup is complete.\n",
    "\n",
    "**ðŸ’¡ Interpretation**: This establishes the foundation for our hallucination detection system.\n",
    "\n",
    "**âš ï¸ Troubleshooting**: If any imports fail, install missing packages using `pip install package_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "ðŸ“… Notebook initialized on: 2025-09-20 19:00:52\n",
      "ðŸ” Ready for hallucination detection and citation validation!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries for hallucination detection and citation validation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Notebook initialized on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ðŸ” Ready for hallucination detection and citation validation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Creating Sample Data for Hallucination Detection**\n",
    "\n",
    "**ðŸŽ¯ Purpose**: Create realistic sample data that includes both accurate and hallucinated responses to test our detection systems.\n",
    "\n",
    "**ðŸ“Š Expected Output**: Sample queries, contexts, answers (some with hallucinations), and ground truth data for validation.\n",
    "\n",
    "**ðŸ’¡ Interpretation**: \n",
    "- **Clean Answers**: Responses that are fully supported by context\n",
    "- **Hallucinated Answers**: Responses containing unsupported claims\n",
    "- **Mixed Answers**: Responses with both accurate and hallucinated information\n",
    "\n",
    "**âš ï¸ Troubleshooting**: If you want to test with your own data, replace the sample data with your domain-specific examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample data created with 10 validation samples\n",
      "ðŸ“Š Hallucination types included:\n",
      "   â€¢ Numerical: 2 samples\n",
      "   â€¢ Contextual: 6 samples\n",
      "   â€¢ Temporal: 1 samples\n",
      "   â€¢ None: 1 samples\n",
      "\n",
      "ðŸŽ¯ Severity distribution:\n",
      "   â€¢ Medium: 5 samples\n",
      "   â€¢ Low: 2 samples\n",
      "   â€¢ High: 2 samples\n",
      "   â€¢ None: 1 samples\n",
      "\n",
      "ðŸ“ Sample data structure:\n",
      "   â€¢ Queries: 10\n",
      "   â€¢ Contexts: 10\n",
      "   â€¢ Citations: 10\n",
      "ðŸ” Ready for hallucination detection testing!\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive sample data for hallucination detection testing\n",
    "@dataclass\n",
    "class ValidationSample:\n",
    "    \"\"\"Data class for storing validation samples.\"\"\"\n",
    "    query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    citations: List[str]\n",
    "    ground_truth: str\n",
    "    hallucination_type: str  # 'none', 'contextual', 'factual', 'temporal', 'numerical'\n",
    "    severity: str  # 'low', 'medium', 'high'\n",
    "    expected_citations: List[str]\n",
    "\n",
    "# Sample data with various types of hallucinations\n",
    "sample_data = [\n",
    "    ValidationSample(\n",
    "        query=\"What is the company's revenue growth rate?\",\n",
    "        context=\"TechCorp reported revenue of $50 million in 2022, up from $40 million in 2021. The company's quarterly growth has been consistent at 15%.\",\n",
    "        answer=\"TechCorp achieved a revenue growth rate of 25% year-over-year, reaching $50 million in 2022. The company has maintained consistent quarterly growth.\",\n",
    "        citations=[\"TechCorp financial report 2022\"],\n",
    "        ground_truth=\"TechCorp achieved a revenue growth rate of 25% year-over-year (from $40M to $50M), reaching $50 million in 2022. The company has maintained consistent quarterly growth at 15%.\",\n",
    "        hallucination_type=\"numerical\",\n",
    "        severity=\"medium\",\n",
    "        expected_citations=[\"TechCorp financial report 2022\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"When was the product launched?\",\n",
    "        context=\"Our flagship product, DataFlow Pro, was launched in March 2023. It has since gained over 10,000 users.\",\n",
    "        answer=\"DataFlow Pro was launched in March 2023 and has gained over 10,000 users. The product was developed by a team of 15 engineers over 18 months.\",\n",
    "        citations=[\"Product launch announcement\"],\n",
    "        ground_truth=\"DataFlow Pro was launched in March 2023 and has gained over 10,000 users.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"low\",\n",
    "        expected_citations=[\"Product launch announcement\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What are the main features of the software?\",\n",
    "        context=\"The software includes real-time data processing, automated reporting, and cloud synchronization. It supports multiple data formats including CSV, JSON, and XML.\",\n",
    "        answer=\"The software includes real-time data processing, automated reporting, and cloud synchronization. It also features advanced machine learning algorithms and blockchain integration.\",\n",
    "        citations=[\"Software documentation\"],\n",
    "        ground_truth=\"The software includes real-time data processing, automated reporting, and cloud synchronization. It supports multiple data formats including CSV, JSON, and XML.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"high\",\n",
    "        expected_citations=[\"Software documentation\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"Who is the CEO of the company?\",\n",
    "        context=\"Sarah Johnson has been the CEO of TechCorp since 2020. She previously worked at Microsoft for 10 years before joining our company.\",\n",
    "        answer=\"Sarah Johnson has been the CEO of TechCorp since 2020. She previously worked at Microsoft for 10 years before joining our company in 2019.\",\n",
    "        citations=[\"Company leadership page\"],\n",
    "        ground_truth=\"Sarah Johnson has been the CEO of TechCorp since 2020. She previously worked at Microsoft for 10 years before joining our company.\",\n",
    "        hallucination_type=\"temporal\",\n",
    "        severity=\"medium\",\n",
    "        expected_citations=[\"Company leadership page\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What is the company's market share?\",\n",
    "        context=\"TechCorp holds a 12% market share in the enterprise software sector. The company has been growing steadily over the past three years.\",\n",
    "        answer=\"TechCorp holds a 12% market share in the enterprise software sector. The company has been growing steadily over the past three years.\",\n",
    "        citations=[\"Market research report 2023\"],\n",
    "        ground_truth=\"TechCorp holds a 12% market share in the enterprise software sector. The company has been growing steadily over the past three years.\",\n",
    "        hallucination_type=\"none\",\n",
    "        severity=\"none\",\n",
    "        expected_citations=[\"Market research report 2023\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"How many employees does the company have?\",\n",
    "        context=\"TechCorp currently employs 250 people across three offices. The company is planning to hire 50 more employees in 2024.\",\n",
    "        answer=\"TechCorp currently employs 250 people across three offices. The company has a diverse workforce with 60% of employees holding advanced degrees.\",\n",
    "        citations=[\"Company HR report\"],\n",
    "        ground_truth=\"TechCorp currently employs 250 people across three offices. The company is planning to hire 50 more employees in 2024.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"medium\",\n",
    "        expected_citations=[\"Company HR report\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What is the company's headquarters location?\",\n",
    "        context=\"TechCorp is headquartered in San Francisco, California. The company was founded in 2015 and has expanded to include offices in New York and London.\",\n",
    "        answer=\"TechCorp is headquartered in San Francisco, California. The company was founded in 2015 and has expanded to include offices in New York, London, and Tokyo.\",\n",
    "        citations=[\"Company website\"],\n",
    "        ground_truth=\"TechCorp is headquartered in San Francisco, California. The company was founded in 2015 and has expanded to include offices in New York and London.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"medium\",\n",
    "        expected_citations=[\"Company website\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What are the company's main competitors?\",\n",
    "        context=\"TechCorp's main competitors include DataSys Inc., CloudTech Solutions, and Analytics Pro. These companies compete in the enterprise data management space.\",\n",
    "        answer=\"TechCorp's main competitors include DataSys Inc., CloudTech Solutions, and Analytics Pro. The competitive landscape is dominated by these four companies.\",\n",
    "        citations=[\"Industry analysis report\"],\n",
    "        ground_truth=\"TechCorp's main competitors include DataSys Inc., CloudTech Solutions, and Analytics Pro. These companies compete in the enterprise data management space.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"low\",\n",
    "        expected_citations=[\"Industry analysis report\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What is the company's stock price?\",\n",
    "        context=\"TechCorp's stock (TECH) is currently trading at $45.50 per share. The stock has increased by 12% over the past month.\",\n",
    "        answer=\"TechCorp's stock (TECH) is currently trading at $45.50 per share. The stock has increased by 15% over the past month and is expected to reach $50 by year-end.\",\n",
    "        citations=[\"Financial market data\"],\n",
    "        ground_truth=\"TechCorp's stock (TECH) is currently trading at $45.50 per share. The stock has increased by 12% over the past month.\",\n",
    "        hallucination_type=\"numerical\",\n",
    "        severity=\"high\",\n",
    "        expected_citations=[\"Financial market data\"]\n",
    "    ),\n",
    "    \n",
    "    ValidationSample(\n",
    "        query=\"What awards has the company received?\",\n",
    "        context=\"TechCorp received the 'Best Enterprise Software' award at the Tech Innovation Summit in 2023. The company was also recognized for its environmental sustainability efforts.\",\n",
    "        answer=\"TechCorp received the 'Best Enterprise Software' award at the Tech Innovation Summit in 2023. The company was also recognized for its environmental sustainability efforts and won the 'Green Technology' award.\",\n",
    "        citations=[\"Award announcements\"],\n",
    "        ground_truth=\"TechCorp received the 'Best Enterprise Software' award at the Tech Innovation Summit in 2023. The company was also recognized for its environmental sustainability efforts.\",\n",
    "        hallucination_type=\"contextual\",\n",
    "        severity=\"medium\",\n",
    "        expected_citations=[\"Award announcements\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"âœ… Sample data created with {len(sample_data)} validation samples\")\n",
    "print(f\"ðŸ“Š Hallucination types included:\")\n",
    "hallucination_counts = Counter([sample.hallucination_type for sample in sample_data])\n",
    "for h_type, count in hallucination_counts.items():\n",
    "    print(f\"   â€¢ {h_type.title()}: {count} samples\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Severity distribution:\")\n",
    "severity_counts = Counter([sample.severity for sample in sample_data])\n",
    "for severity, count in severity_counts.items():\n",
    "    print(f\"   â€¢ {severity.title()}: {count} samples\")\n",
    "\n",
    "print(f\"\\nðŸ“ Sample data structure:\")\n",
    "print(f\"   â€¢ Queries: {len(set(sample.query for sample in sample_data))}\")\n",
    "print(f\"   â€¢ Contexts: {len(set(sample.context for sample in sample_data))}\")\n",
    "print(f\"   â€¢ Citations: {len(set(cite for sample in sample_data for cite in sample.citations))}\")\n",
    "print(\"ðŸ” Ready for hallucination detection testing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Basic Hallucination Detection System**\n",
    "\n",
    "**ðŸŽ¯ Purpose**: Implement a basic hallucination detection system that can identify unsupported claims in generated answers.\n",
    "\n",
    "**ðŸ“Š Expected Output**: A working hallucination detector that can classify answers as clean or containing hallucinations.\n",
    "\n",
    "**ðŸ’¡ Interpretation**: \n",
    "- **Hallucination Score**: 0.0 = no hallucinations, 1.0 = complete hallucination\n",
    "- **Detection Confidence**: How confident the system is in its assessment\n",
    "- **Supported Claims**: Claims that are backed by the retrieved context\n",
    "\n",
    "**âš ï¸ Troubleshooting**: If detection accuracy is low, consider adjusting the similarity thresholds or adding more sophisticated NLP techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Basic hallucination detector initialized!\n",
      "ðŸ” Detection capabilities:\n",
      "   â€¢ Numerical hallucination detection\n",
      "   â€¢ Temporal hallucination detection\n",
      "   â€¢ Contextual hallucination detection\n",
      "   â€¢ Overall hallucination scoring\n",
      "ðŸŽ¯ Ready for hallucination detection testing!\n"
     ]
    }
   ],
   "source": [
    "class BasicHallucinationDetector:\n",
    "    \"\"\"\n",
    "    Basic hallucination detection system for RAG outputs.\n",
    "    \n",
    "    This detector implements rule-based and similarity-based methods to identify\n",
    "    hallucinations in generated answers by comparing them against retrieved context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.detection_history = []\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def tokenize_and_normalize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and normalize text for comparison.\"\"\"\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        normalized = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        # Split into tokens and remove empty strings\n",
    "        tokens = [token.strip() for token in normalized.split() if token.strip()]\n",
    "        return tokens\n",
    "    \n",
    "    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate Jaccard similarity between two texts.\"\"\"\n",
    "        tokens1 = set(self.tokenize_and_normalize(text1))\n",
    "        tokens2 = set(self.tokenize_and_normalize(text2))\n",
    "        \n",
    "        if len(tokens1) == 0 and len(tokens2) == 0:\n",
    "            return 1.0\n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(tokens1.intersection(tokens2))\n",
    "        union = len(tokens1.union(tokens2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def extract_claims(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential claims from text using simple heuristics.\"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        claims = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10:  # Filter out very short sentences\n",
    "                # Look for factual statements (contain numbers, dates, or specific facts)\n",
    "                if (re.search(r'\\d+', sentence) or  # Contains numbers\n",
    "                    re.search(r'\\b(was|is|are|were|has|have|will|can|should|must)\\b', sentence) or  # Contains factual verbs\n",
    "                    re.search(r'\\b(in|on|at|since|from|to)\\b', sentence)):  # Contains temporal/location prepositions\n",
    "                    claims.append(sentence)\n",
    "        \n",
    "        return claims\n",
    "    \n",
    "    def detect_numerical_hallucinations(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect numerical hallucinations by comparing numbers in answer vs context.\"\"\"\n",
    "        # Extract numbers from answer and context\n",
    "        answer_numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?(?:%|million|billion|thousand)?\\b', answer.lower())\n",
    "        context_numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?(?:%|million|billion|thousand)?\\b', context.lower())\n",
    "        \n",
    "        # Check if answer numbers are present in context\n",
    "        unsupported_numbers = []\n",
    "        for num in answer_numbers:\n",
    "            if num not in context_numbers:\n",
    "                unsupported_numbers.append(num)\n",
    "        \n",
    "        hallucination_score = len(unsupported_numbers) / max(len(answer_numbers), 1)\n",
    "        \n",
    "        return {\n",
    "            \"has_numerical_hallucination\": len(unsupported_numbers) > 0,\n",
    "            \"unsupported_numbers\": unsupported_numbers,\n",
    "            \"numerical_hallucination_score\": hallucination_score,\n",
    "            \"answer_numbers\": answer_numbers,\n",
    "            \"context_numbers\": context_numbers\n",
    "        }\n",
    "    \n",
    "    def detect_temporal_hallucinations(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect temporal hallucinations by comparing dates and time references.\"\"\"\n",
    "        # Extract temporal expressions\n",
    "        temporal_patterns = [\n",
    "            r'\\b(19|20)\\d{2}\\b',  # Years\n",
    "            r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b',\n",
    "            r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',  # Dates\n",
    "            r'\\b\\d{1,2}-\\d{1,2}-\\d{2,4}\\b',  # Dates with dashes\n",
    "            r'\\b(in|since|from|until|before|after)\\s+\\d{4}\\b'  # Temporal prepositions with years\n",
    "        ]\n",
    "        \n",
    "        answer_temporal = []\n",
    "        context_temporal = []\n",
    "        \n",
    "        for pattern in temporal_patterns:\n",
    "            answer_temporal.extend(re.findall(pattern, answer.lower()))\n",
    "            context_temporal.extend(re.findall(pattern, context.lower()))\n",
    "        \n",
    "        # Check for unsupported temporal information\n",
    "        unsupported_temporal = []\n",
    "        for temp in answer_temporal:\n",
    "            if temp not in context_temporal:\n",
    "                unsupported_temporal.append(temp)\n",
    "        \n",
    "        hallucination_score = len(unsupported_temporal) / max(len(answer_temporal), 1)\n",
    "        \n",
    "        return {\n",
    "            \"has_temporal_hallucination\": len(unsupported_temporal) > 0,\n",
    "            \"unsupported_temporal\": unsupported_temporal,\n",
    "            \"temporal_hallucination_score\": hallucination_score,\n",
    "            \"answer_temporal\": answer_temporal,\n",
    "            \"context_temporal\": context_temporal\n",
    "        }\n",
    "    \n",
    "    def detect_contextual_hallucinations(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect contextual hallucinations by analyzing claim support.\"\"\"\n",
    "        # Extract claims from answer\n",
    "        claims = self.extract_claims(answer)\n",
    "        \n",
    "        # Check support for each claim\n",
    "        unsupported_claims = []\n",
    "        supported_claims = []\n",
    "        \n",
    "        for claim in claims:\n",
    "            # Calculate similarity between claim and context\n",
    "            similarity = self.calculate_jaccard_similarity(claim, context)\n",
    "            \n",
    "            # Threshold for considering a claim supported\n",
    "            support_threshold = 0.3\n",
    "            \n",
    "            if similarity < support_threshold:\n",
    "                unsupported_claims.append({\n",
    "                    \"claim\": claim,\n",
    "                    \"similarity\": similarity,\n",
    "                    \"reason\": \"low_similarity\"\n",
    "                })\n",
    "            else:\n",
    "                supported_claims.append({\n",
    "                    \"claim\": claim,\n",
    "                    \"similarity\": similarity\n",
    "                })\n",
    "        \n",
    "        hallucination_score = len(unsupported_claims) / max(len(claims), 1)\n",
    "        \n",
    "        return {\n",
    "            \"has_contextual_hallucination\": len(unsupported_claims) > 0,\n",
    "            \"unsupported_claims\": unsupported_claims,\n",
    "            \"supported_claims\": supported_claims,\n",
    "            \"contextual_hallucination_score\": hallucination_score,\n",
    "            \"total_claims\": len(claims)\n",
    "        }\n",
    "    \n",
    "    def detect_hallucinations(self, query: str, context: str, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive hallucination detection for a given query, context, and answer.\n",
    "        \n",
    "        Args:\n",
    "            query: Original query\n",
    "            context: Retrieved context\n",
    "            answer: Generated answer\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with hallucination detection results\n",
    "        \"\"\"\n",
    "        # Run different types of hallucination detection\n",
    "        numerical_results = self.detect_numerical_hallucinations(answer, context)\n",
    "        temporal_results = self.detect_temporal_hallucinations(answer, context)\n",
    "        contextual_results = self.detect_contextual_hallucinations(answer, context)\n",
    "        \n",
    "        # Calculate overall hallucination score\n",
    "        scores = [\n",
    "            numerical_results[\"numerical_hallucination_score\"],\n",
    "            temporal_results[\"temporal_hallucination_score\"],\n",
    "            contextual_results[\"contextual_hallucination_score\"]\n",
    "        ]\n",
    "        \n",
    "        overall_score = np.mean([score for score in scores if score is not None])\n",
    "        \n",
    "        # Determine if hallucination is present\n",
    "        has_hallucination = any([\n",
    "            numerical_results[\"has_numerical_hallucination\"],\n",
    "            temporal_results[\"has_temporal_hallucination\"],\n",
    "            contextual_results[\"has_contextual_hallucination\"]\n",
    "        ])\n",
    "        \n",
    "        # Calculate confidence based on consistency of detection methods\n",
    "        detection_methods = [\n",
    "            numerical_results[\"has_numerical_hallucination\"],\n",
    "            temporal_results[\"has_temporal_hallucination\"],\n",
    "            contextual_results[\"has_contextual_hallucination\"]\n",
    "        ]\n",
    "        \n",
    "        confidence = sum(detection_methods) / len(detection_methods)\n",
    "        \n",
    "        # Store detection result\n",
    "        detection_result = {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"has_hallucination\": has_hallucination,\n",
    "            \"overall_hallucination_score\": overall_score,\n",
    "            \"confidence\": confidence,\n",
    "            \"numerical_detection\": numerical_results,\n",
    "            \"temporal_detection\": temporal_results,\n",
    "            \"contextual_detection\": contextual_results,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.detection_history.append(detection_result)\n",
    "        \n",
    "        return detection_result\n",
    "    \n",
    "    def get_detection_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics of hallucination detection performance.\"\"\"\n",
    "        if not self.detection_history:\n",
    "            return {\"error\": \"No detection history available\"}\n",
    "        \n",
    "        total_detections = len(self.detection_history)\n",
    "        hallucinations_detected = sum(1 for result in self.detection_history if result[\"has_hallucination\"])\n",
    "        \n",
    "        avg_hallucination_score = np.mean([result[\"overall_hallucination_score\"] for result in self.detection_history])\n",
    "        avg_confidence = np.mean([result[\"confidence\"] for result in self.detection_history])\n",
    "        \n",
    "        # Breakdown by hallucination type\n",
    "        numerical_hallucinations = sum(1 for result in self.detection_history \n",
    "                                     if result[\"numerical_detection\"][\"has_numerical_hallucination\"])\n",
    "        temporal_hallucinations = sum(1 for result in self.detection_history \n",
    "                                    if result[\"temporal_detection\"][\"has_temporal_hallucination\"])\n",
    "        contextual_hallucinations = sum(1 for result in self.detection_history \n",
    "                                      if result[\"contextual_detection\"][\"has_contextual_hallucination\"])\n",
    "        \n",
    "        return {\n",
    "            \"total_detections\": total_detections,\n",
    "            \"hallucinations_detected\": hallucinations_detected,\n",
    "            \"detection_rate\": hallucinations_detected / total_detections,\n",
    "            \"average_hallucination_score\": avg_hallucination_score,\n",
    "            \"average_confidence\": avg_confidence,\n",
    "            \"hallucination_type_breakdown\": {\n",
    "                \"numerical\": numerical_hallucinations,\n",
    "                \"temporal\": temporal_hallucinations,\n",
    "                \"contextual\": contextual_hallucinations\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize the basic hallucination detector\n",
    "basic_detector = BasicHallucinationDetector()\n",
    "\n",
    "print(\"âœ… Basic hallucination detector initialized!\")\n",
    "print(\"ðŸ” Detection capabilities:\")\n",
    "print(\"   â€¢ Numerical hallucination detection\")\n",
    "print(\"   â€¢ Temporal hallucination detection\") \n",
    "print(\"   â€¢ Contextual hallucination detection\")\n",
    "print(\"   â€¢ Overall hallucination scoring\")\n",
    "print(\"ðŸŽ¯ Ready for hallucination detection testing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Testing Basic Hallucination Detection**\n",
    "\n",
    "**ðŸŽ¯ Purpose**: Test our basic hallucination detector with the sample data to see how well it identifies different types of hallucinations.\n",
    "\n",
    "**ðŸ“Š Expected Output**: Detection results for each sample, showing which hallucinations were correctly identified and the confidence scores.\n",
    "\n",
    "**ðŸ’¡ Interpretation**: \n",
    "- **High Detection Rate**: Good performance in identifying hallucinations\n",
    "- **Low False Positives**: Few clean answers incorrectly flagged as hallucinations\n",
    "- **Confidence Scores**: Indicate how certain the system is about its predictions\n",
    "\n",
    "**âš ï¸ Troubleshooting**: If detection accuracy is low, consider adjusting thresholds or adding more sophisticated detection methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Basic Hallucination Detection\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Sample 1: What is the company's revenue growth rate?...\n",
      "   ðŸŽ¯ Ground Truth: Numerical (medium)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.444\n",
      "   ðŸŽª Confidence: 0.667\n",
      "   ðŸ”¢ Numerical: ['25']\n",
      "   ðŸ“ Contextual: 2 unsupported claims\n",
      "\n",
      "ðŸ“ Sample 2: When was the product launched?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (low)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.300\n",
      "   ðŸŽª Confidence: 0.667\n",
      "   ðŸ”¢ Numerical: ['15', '18']\n",
      "   ðŸ“ Contextual: 1 unsupported claims\n",
      "\n",
      "ðŸ“ Sample 3: What are the main features of the software?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (high)\n",
      "   ðŸ” Detected Hallucination: No\n",
      "   ðŸ“Š Overall Score: 0.000\n",
      "   ðŸŽª Confidence: 0.000\n",
      "\n",
      "ðŸ“ Sample 4: Who is the CEO of the company?...\n",
      "   ðŸŽ¯ Ground Truth: Temporal (medium)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.194\n",
      "   ðŸŽª Confidence: 0.667\n",
      "   ðŸ”¢ Numerical: ['2019']\n",
      "   ðŸ“… Temporal: ['in']\n",
      "\n",
      "ðŸ“ Sample 5: What is the company's market share?...\n",
      "   ðŸŽ¯ Ground Truth: None (none)\n",
      "   ðŸ” Detected Hallucination: No\n",
      "   ðŸ“Š Overall Score: 0.000\n",
      "   ðŸŽª Confidence: 0.000\n",
      "\n",
      "ðŸ“ Sample 6: How many employees does the company have?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (medium)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.333\n",
      "   ðŸŽª Confidence: 0.667\n",
      "   ðŸ”¢ Numerical: ['60']\n",
      "   ðŸ“ Contextual: 1 unsupported claims\n",
      "\n",
      "ðŸ“ Sample 7: What is the company's headquarters location?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (medium)\n",
      "   ðŸ” Detected Hallucination: No\n",
      "   ðŸ“Š Overall Score: 0.000\n",
      "   ðŸŽª Confidence: 0.000\n",
      "\n",
      "ðŸ“ Sample 8: What are the company's main competitors?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (low)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.333\n",
      "   ðŸŽª Confidence: 0.333\n",
      "   ðŸ“ Contextual: 1 unsupported claims\n",
      "\n",
      "ðŸ“ Sample 9: What is the company's stock price?...\n",
      "   ðŸŽ¯ Ground Truth: Numerical (high)\n",
      "   ðŸ” Detected Hallucination: Yes\n",
      "   ðŸ“Š Overall Score: 0.333\n",
      "   ðŸŽª Confidence: 0.667\n",
      "   ðŸ”¢ Numerical: ['15', '50']\n",
      "   ðŸ“ Contextual: 1 unsupported claims\n",
      "\n",
      "ðŸ“ Sample 10: What awards has the company received?...\n",
      "   ðŸŽ¯ Ground Truth: Contextual (medium)\n",
      "   ðŸ” Detected Hallucination: No\n",
      "   ðŸ“Š Overall Score: 0.000\n",
      "   ðŸŽª Confidence: 0.000\n",
      "\n",
      "ðŸ“ˆ DETECTION PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "ðŸ“Š Overall Performance:\n",
      "   â€¢ Accuracy: 0.700 (7/10)\n",
      "   â€¢ Precision: 1.000\n",
      "   â€¢ Recall: 0.700\n",
      "   â€¢ F1 Score: 0.824\n",
      "   â€¢ False Positives: 0\n",
      "   â€¢ False Negatives: 3\n",
      "\n",
      "ðŸŽ¯ Performance by Hallucination Type:\n",
      "----------------------------------------\n",
      "   â€¢ None: 1.000 (1/1)\n",
      "   â€¢ Numerical: 1.000 (2/2)\n",
      "   â€¢ Temporal: 1.000 (1/1)\n",
      "   â€¢ Contextual: 0.500 (3/6)\n",
      "\n",
      "âš¡ Performance by Severity:\n",
      "------------------------------\n",
      "   â€¢ None: 1.000 (1/1)\n",
      "   â€¢ Low: 1.000 (2/2)\n",
      "   â€¢ Medium: 0.600 (3/5)\n",
      "   â€¢ High: 0.500 (1/2)\n",
      "\n",
      "ðŸ“‹ Detector Summary:\n",
      "-------------------------\n",
      "   â€¢ Total Detections: 10\n",
      "   â€¢ Hallucinations Detected: 6\n",
      "   â€¢ Detection Rate: 0.600\n",
      "   â€¢ Average Score: 0.194\n",
      "   â€¢ Average Confidence: 0.367\n",
      "\n",
      "âœ… Basic hallucination detection testing completed!\n",
      "ðŸŽ¯ Ready to implement advanced citation validation!\n"
     ]
    }
   ],
   "source": [
    "# Test basic hallucination detection on sample data\n",
    "print(\"ðŸ§ª Testing Basic Hallucination Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "detection_results = []\n",
    "\n",
    "for i, sample in enumerate(sample_data, 1):\n",
    "    print(f\"\\nðŸ“ Sample {i}: {sample.query[:50]}...\")\n",
    "    \n",
    "    # Run hallucination detection\n",
    "    result = basic_detector.detect_hallucinations(\n",
    "        query=sample.query,\n",
    "        context=sample.context,\n",
    "        answer=sample.answer\n",
    "    )\n",
    "    \n",
    "    # Store result with ground truth for evaluation\n",
    "    detection_results.append({\n",
    "        'sample': sample,\n",
    "        'detection_result': result,\n",
    "        'ground_truth_type': sample.hallucination_type,\n",
    "        'ground_truth_severity': sample.severity\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"   ðŸŽ¯ Ground Truth: {sample.hallucination_type.title()} ({sample.severity})\")\n",
    "    print(f\"   ðŸ” Detected Hallucination: {'Yes' if result['has_hallucination'] else 'No'}\")\n",
    "    print(f\"   ðŸ“Š Overall Score: {result['overall_hallucination_score']:.3f}\")\n",
    "    print(f\"   ðŸŽª Confidence: {result['confidence']:.3f}\")\n",
    "    \n",
    "    # Show specific detection details\n",
    "    if result['numerical_detection']['has_numerical_hallucination']:\n",
    "        print(f\"   ðŸ”¢ Numerical: {result['numerical_detection']['unsupported_numbers']}\")\n",
    "    \n",
    "    if result['temporal_detection']['has_temporal_hallucination']:\n",
    "        print(f\"   ðŸ“… Temporal: {result['temporal_detection']['unsupported_temporal']}\")\n",
    "    \n",
    "    if result['contextual_detection']['has_contextual_hallucination']:\n",
    "        unsupported_count = len(result['contextual_detection']['unsupported_claims'])\n",
    "        print(f\"   ðŸ“ Contextual: {unsupported_count} unsupported claims\")\n",
    "\n",
    "# Calculate detection performance\n",
    "print(f\"\\nðŸ“ˆ DETECTION PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count correct detections\n",
    "correct_detections = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for result in detection_results:\n",
    "    sample = result['sample']\n",
    "    detection = result['detection_result']\n",
    "    \n",
    "    # Ground truth: has hallucination (not 'none')\n",
    "    ground_truth_has_hallucination = sample.hallucination_type != 'none'\n",
    "    detected_hallucination = detection['has_hallucination']\n",
    "    \n",
    "    if ground_truth_has_hallucination and detected_hallucination:\n",
    "        correct_detections += 1\n",
    "    elif not ground_truth_has_hallucination and not detected_hallucination:\n",
    "        correct_detections += 1\n",
    "    elif not ground_truth_has_hallucination and detected_hallucination:\n",
    "        false_positives += 1\n",
    "    elif ground_truth_has_hallucination and not detected_hallucination:\n",
    "        false_negatives += 1\n",
    "\n",
    "total_samples = len(detection_results)\n",
    "accuracy = correct_detections / total_samples\n",
    "precision = correct_detections / (correct_detections + false_positives) if (correct_detections + false_positives) > 0 else 0\n",
    "recall = correct_detections / (correct_detections + false_negatives) if (correct_detections + false_negatives) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"ðŸ“Š Overall Performance:\")\n",
    "print(f\"   â€¢ Accuracy: {accuracy:.3f} ({correct_detections}/{total_samples})\")\n",
    "print(f\"   â€¢ Precision: {precision:.3f}\")\n",
    "print(f\"   â€¢ Recall: {recall:.3f}\")\n",
    "print(f\"   â€¢ F1 Score: {f1_score:.3f}\")\n",
    "print(f\"   â€¢ False Positives: {false_positives}\")\n",
    "print(f\"   â€¢ False Negatives: {false_negatives}\")\n",
    "\n",
    "# Performance by hallucination type\n",
    "print(f\"\\nðŸŽ¯ Performance by Hallucination Type:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "hallucination_types = ['none', 'numerical', 'temporal', 'contextual']\n",
    "for h_type in hallucination_types:\n",
    "    type_samples = [r for r in detection_results if r['ground_truth_type'] == h_type]\n",
    "    if type_samples:\n",
    "        correct = sum(1 for r in type_samples \n",
    "                     if (h_type == 'none' and not r['detection_result']['has_hallucination']) or\n",
    "                        (h_type != 'none' and r['detection_result']['has_hallucination']))\n",
    "        type_accuracy = correct / len(type_samples)\n",
    "        print(f\"   â€¢ {h_type.title()}: {type_accuracy:.3f} ({correct}/{len(type_samples)})\")\n",
    "\n",
    "# Performance by severity\n",
    "print(f\"\\nâš¡ Performance by Severity:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "severities = ['none', 'low', 'medium', 'high']\n",
    "for severity in severities:\n",
    "    severity_samples = [r for r in detection_results if r['ground_truth_severity'] == severity]\n",
    "    if severity_samples:\n",
    "        correct = sum(1 for r in severity_samples \n",
    "                     if (severity == 'none' and not r['detection_result']['has_hallucination']) or\n",
    "                        (severity != 'none' and r['detection_result']['has_hallucination']))\n",
    "        severity_accuracy = correct / len(severity_samples)\n",
    "        print(f\"   â€¢ {severity.title()}: {severity_accuracy:.3f} ({correct}/{len(severity_samples)})\")\n",
    "\n",
    "# Get summary from detector\n",
    "summary = basic_detector.get_detection_summary()\n",
    "print(f\"\\nðŸ“‹ Detector Summary:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"   â€¢ Total Detections: {summary['total_detections']}\")\n",
    "print(f\"   â€¢ Hallucinations Detected: {summary['hallucinations_detected']}\")\n",
    "print(f\"   â€¢ Detection Rate: {summary['detection_rate']:.3f}\")\n",
    "print(f\"   â€¢ Average Score: {summary['average_hallucination_score']:.3f}\")\n",
    "print(f\"   â€¢ Average Confidence: {summary['average_confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… Basic hallucination detection testing completed!\")\n",
    "print(\"ðŸŽ¯ Ready to implement advanced citation validation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ **Intermediate Level: Advanced Citation Validation and Automated Detection**\n",
    "\n",
    "### **Step 5: Citation Validation System**\n",
    "\n",
    "**ðŸŽ¯ Purpose**: Implement comprehensive citation validation to ensure claims are properly supported by cited sources.\n",
    "\n",
    "**ðŸ“Š Expected Output**: Advanced citation validator that can assess citation quality, completeness, and relevance.\n",
    "\n",
    "**ðŸ’¡ Interpretation**: \n",
    "- **Citation Accuracy**: How well citations support the claims made\n",
    "- **Citation Completeness**: Whether all claims have supporting citations\n",
    "- **Citation Relevance**: How relevant cited passages are to the claims\n",
    "\n",
    "**âš ï¸ Troubleshooting**: If citation scores are unexpectedly low, check if citation formats are consistent and properly formatted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Citation validator initialized!\n",
      "ðŸ” Validation capabilities:\n",
      "   â€¢ Citation extraction and parsing\n",
      "   â€¢ Citation accuracy validation\n",
      "   â€¢ Citation completeness assessment\n",
      "   â€¢ Citation relevance evaluation\n",
      "   â€¢ Comprehensive citation scoring\n",
      "ðŸŽ¯ Ready for advanced citation validation!\n"
     ]
    }
   ],
   "source": [
    "class CitationValidator:\n",
    "    \"\"\"\n",
    "    Advanced citation validation system for RAG outputs.\n",
    "    \n",
    "    This validator assesses the quality, completeness, and relevance of citations\n",
    "    in generated answers to ensure proper source attribution and claim support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_history = []\n",
    "        self.citation_patterns = [\n",
    "            r'\\[(\\d+)\\]',  # [1], [2], etc.\n",
    "            r'\\(([^)]+)\\)',  # (Source name), (Author, Year)\n",
    "            r'According to ([^,]+),',  # According to Source,\n",
    "            r'As stated in ([^,]+),',  # As stated in Source,\n",
    "            r'Source: ([^\\n]+)',  # Source: Name\n",
    "            r'Reference: ([^\\n]+)'  # Reference: Name\n",
    "        ]\n",
    "    \n",
    "    def extract_citations(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract citations from text using pattern matching.\"\"\"\n",
    "        citations = []\n",
    "        \n",
    "        for pattern in self.citation_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                citation_text = match.group(1) if match.groups() else match.group(0)\n",
    "                citations.append({\n",
    "                    'text': citation_text,\n",
    "                    'full_match': match.group(0),\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end(),\n",
    "                    'pattern_type': pattern\n",
    "                })\n",
    "        \n",
    "        return citations\n",
    "    \n",
    "    def extract_claims_with_citations(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract claims and their associated citations from text.\"\"\"\n",
    "        # Split text into sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        claims_with_citations = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10:\n",
    "                # Extract citations in this sentence\n",
    "                citations = self.extract_citations(sentence)\n",
    "                \n",
    "                # Check if sentence contains factual claims\n",
    "                if self._contains_factual_claim(sentence):\n",
    "                    claims_with_citations.append({\n",
    "                        'sentence': sentence,\n",
    "                        'citations': citations,\n",
    "                        'has_citation': len(citations) > 0,\n",
    "                        'citation_count': len(citations)\n",
    "                    })\n",
    "        \n",
    "        return claims_with_citations\n",
    "    \n",
    "    def _contains_factual_claim(self, sentence: str) -> bool:\n",
    "        \"\"\"Check if a sentence contains factual claims.\"\"\"\n",
    "        # Look for factual indicators\n",
    "        factual_indicators = [\n",
    "            r'\\b(is|are|was|were|has|have|will|can|should|must)\\b',\n",
    "            r'\\b(\\d+(?:\\.\\d+)?(?:%|million|billion|thousand)?)\\b',  # Numbers\n",
    "            r'\\b(19|20)\\d{2}\\b',  # Years\n",
    "            r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in factual_indicators:\n",
    "            if re.search(pattern, sentence, re.IGNORECASE):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def validate_citation_accuracy(self, claim: str, citation_text: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate if a citation accurately supports the claim.\"\"\"\n",
    "        # Calculate similarity between claim and context around citation\n",
    "        citation_context = self._extract_citation_context(context, citation_text)\n",
    "        \n",
    "        if citation_context:\n",
    "            similarity = self._calculate_semantic_similarity(claim, citation_context)\n",
    "            \n",
    "            # Check if citation mentions are present in context\n",
    "            citation_mentioned = citation_text.lower() in context.lower()\n",
    "            \n",
    "            # Check if claim information is present in citation context\n",
    "            claim_supported = self._check_claim_support(claim, citation_context)\n",
    "            \n",
    "            accuracy_score = (similarity * 0.4 + \n",
    "                            (1.0 if citation_mentioned else 0.0) * 0.3 + \n",
    "                            (1.0 if claim_supported else 0.0) * 0.3)\n",
    "        else:\n",
    "            accuracy_score = 0.0\n",
    "            citation_mentioned = False\n",
    "            claim_supported = False\n",
    "        \n",
    "        return {\n",
    "            'accuracy_score': accuracy_score,\n",
    "            'citation_mentioned': citation_mentioned,\n",
    "            'claim_supported': claim_supported,\n",
    "            'citation_context': citation_context\n",
    "        }\n",
    "    \n",
    "    def _extract_citation_context(self, context: str, citation_text: str) -> str:\n",
    "        \"\"\"Extract context around a citation mention.\"\"\"\n",
    "        # Look for citation text in context (case-insensitive)\n",
    "        context_lower = context.lower()\n",
    "        citation_lower = citation_text.lower()\n",
    "        \n",
    "        if citation_lower in context_lower:\n",
    "            # Find the position of citation in context\n",
    "            start_pos = context_lower.find(citation_lower)\n",
    "            # Extract surrounding context (100 characters before and after)\n",
    "            context_start = max(0, start_pos - 100)\n",
    "            context_end = min(len(context), start_pos + len(citation_text) + 100)\n",
    "            return context[context_start:context_end]\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity between two texts.\"\"\"\n",
    "        # Simple word overlap similarity (can be enhanced with embeddings)\n",
    "        words1 = set(re.findall(r'\\b\\w+\\b', text1.lower()))\n",
    "        words2 = set(re.findall(r'\\b\\w+\\b', text2.lower()))\n",
    "        \n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def _check_claim_support(self, claim: str, citation_context: str) -> bool:\n",
    "        \"\"\"Check if a claim is supported by citation context.\"\"\"\n",
    "        # Extract key information from claim\n",
    "        claim_numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?(?:%|million|billion|thousand)?\\b', claim.lower())\n",
    "        claim_years = re.findall(r'\\b(19|20)\\d{2}\\b', claim.lower())\n",
    "        claim_names = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', claim)\n",
    "        \n",
    "        # Check if key information appears in citation context\n",
    "        context_lower = citation_context.lower()\n",
    "        \n",
    "        # Check numbers\n",
    "        numbers_supported = all(num in context_lower for num in claim_numbers)\n",
    "        \n",
    "        # Check years\n",
    "        years_supported = all(year in context_lower for year in claim_years)\n",
    "        \n",
    "        # Check names (at least one should be present)\n",
    "        names_supported = any(name.lower() in context_lower for name in claim_names)\n",
    "        \n",
    "        # Overall support: at least 2 out of 3 criteria should be met\n",
    "        support_criteria = [numbers_supported, years_supported, names_supported]\n",
    "        support_count = sum(support_criteria)\n",
    "        \n",
    "        return support_count >= 2 or (len(claim_numbers) == 0 and len(claim_years) == 0 and len(claim_names) == 0)\n",
    "    \n",
    "    def validate_citation_completeness(self, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate if all claims have supporting citations.\"\"\"\n",
    "        claims_with_citations = self.extract_claims_with_citations(answer)\n",
    "        \n",
    "        total_claims = len(claims_with_citations)\n",
    "        claims_with_citations_count = sum(1 for claim in claims_with_citations if claim['has_citation'])\n",
    "        \n",
    "        completeness_score = claims_with_citations_count / total_claims if total_claims > 0 else 1.0\n",
    "        \n",
    "        # Identify uncited claims\n",
    "        uncited_claims = [claim['sentence'] for claim in claims_with_citations if not claim['has_citation']]\n",
    "        \n",
    "        return {\n",
    "            'completeness_score': completeness_score,\n",
    "            'total_claims': total_claims,\n",
    "            'cited_claims': claims_with_citations_count,\n",
    "            'uncited_claims': uncited_claims,\n",
    "            'citation_rate': claims_with_citations_count / total_claims if total_claims > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def validate_citation_relevance(self, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate if citations are relevant to the claims.\"\"\"\n",
    "        claims_with_citations = self.extract_claims_with_citations(answer)\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for claim_data in claims_with_citations:\n",
    "            if claim_data['has_citation']:\n",
    "                claim = claim_data['sentence']\n",
    "                citations = claim_data['citations']\n",
    "                \n",
    "                # Calculate relevance for each citation\n",
    "                citation_relevances = []\n",
    "                for citation in citations:\n",
    "                    relevance_result = self.validate_citation_accuracy(claim, citation['text'], context)\n",
    "                    citation_relevances.append(relevance_result['accuracy_score'])\n",
    "                \n",
    "                # Average relevance for this claim\n",
    "                if citation_relevances:\n",
    "                    claim_relevance = np.mean(citation_relevances)\n",
    "                    relevance_scores.append(claim_relevance)\n",
    "        \n",
    "        overall_relevance = np.mean(relevance_scores) if relevance_scores else 0.0\n",
    "        \n",
    "        return {\n",
    "            'overall_relevance_score': overall_relevance,\n",
    "            'individual_relevance_scores': relevance_scores,\n",
    "            'total_cited_claims': len(relevance_scores),\n",
    "            'high_relevance_claims': sum(1 for score in relevance_scores if score > 0.7),\n",
    "            'low_relevance_claims': sum(1 for score in relevance_scores if score < 0.3)\n",
    "        }\n",
    "    \n",
    "    def comprehensive_citation_validation(self, query: str, context: str, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform comprehensive citation validation.\"\"\"\n",
    "        # Run all validation checks\n",
    "        completeness_result = self.validate_citation_completeness(answer)\n",
    "        relevance_result = self.validate_citation_relevance(answer, context)\n",
    "        \n",
    "        # Extract all citations\n",
    "        citations = self.extract_citations(answer)\n",
    "        \n",
    "        # Validate each citation for accuracy\n",
    "        accuracy_results = []\n",
    "        for citation in citations:\n",
    "            accuracy_result = self.validate_citation_accuracy(answer, citation['text'], context)\n",
    "            accuracy_results.append(accuracy_result)\n",
    "        \n",
    "        avg_accuracy = np.mean([result['accuracy_score'] for result in accuracy_results]) if accuracy_results else 0.0\n",
    "        \n",
    "        # Calculate overall citation quality score\n",
    "        overall_score = (completeness_result['completeness_score'] * 0.3 + \n",
    "                        relevance_result['overall_relevance_score'] * 0.4 + \n",
    "                        avg_accuracy * 0.3)\n",
    "        \n",
    "        validation_result = {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'context': context,\n",
    "            'overall_citation_score': overall_score,\n",
    "            'completeness': completeness_result,\n",
    "            'relevance': relevance_result,\n",
    "            'accuracy': {\n",
    "                'average_accuracy': avg_accuracy,\n",
    "                'individual_results': accuracy_results\n",
    "            },\n",
    "            'citations': citations,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.validation_history.append(validation_result)\n",
    "        \n",
    "        return validation_result\n",
    "    \n",
    "    def get_validation_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of citation validation performance.\"\"\"\n",
    "        if not self.validation_history:\n",
    "            return {\"error\": \"No validation history available\"}\n",
    "        \n",
    "        scores = [result['overall_citation_score'] for result in self.validation_history]\n",
    "        completeness_scores = [result['completeness']['completeness_score'] for result in self.validation_history]\n",
    "        relevance_scores = [result['relevance']['overall_relevance_score'] for result in self.validation_history]\n",
    "        accuracy_scores = [result['accuracy']['average_accuracy'] for result in self.validation_history]\n",
    "        \n",
    "        return {\n",
    "            'total_validations': len(self.validation_history),\n",
    "            'average_overall_score': np.mean(scores),\n",
    "            'average_completeness': np.mean(completeness_scores),\n",
    "            'average_relevance': np.mean(relevance_scores),\n",
    "            'average_accuracy': np.mean(accuracy_scores),\n",
    "            'score_distribution': {\n",
    "                'excellent': sum(1 for score in scores if score > 0.8),\n",
    "                'good': sum(1 for score in scores if 0.6 <= score <= 0.8),\n",
    "                'fair': sum(1 for score in scores if 0.4 <= score < 0.6),\n",
    "                'poor': sum(1 for score in scores if score < 0.4)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize citation validator\n",
    "citation_validator = CitationValidator()\n",
    "\n",
    "print(\"âœ… Citation validator initialized!\")\n",
    "print(\"ðŸ” Validation capabilities:\")\n",
    "print(\"   â€¢ Citation extraction and parsing\")\n",
    "print(\"   â€¢ Citation accuracy validation\")\n",
    "print(\"   â€¢ Citation completeness assessment\")\n",
    "print(\"   â€¢ Citation relevance evaluation\")\n",
    "print(\"   â€¢ Comprehensive citation scoring\")\n",
    "print(\"ðŸŽ¯ Ready for advanced citation validation!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
