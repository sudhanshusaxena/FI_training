{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851a54a3",
   "metadata": {},
   "source": [
    "\n",
    "# Day 2 – Exercise 7: Intent‑Driven Retrieval with RealLight (LiteLLM)\n",
    "\n",
    "## Background & Plan\n",
    "\n",
    "In enterprise data architectures for generative AI, **retrieval‑augmented generation (RAG)** systems need to route incoming queries to the most appropriate retrieval strategy. For example, a concise factual question should use a simple keyword or BM25 retriever, while an open‑ended exploratory question might benefit from a dense vector search followed by summarization. This routing improves both efficiency and relevance of the answers.\n",
    "\n",
    "To route intelligently, the system first **classifies the intent** of each query. Many projects begin with rule‑based heuristics or lightweight machine‑learning models. However, enterprise systems eventually integrate large language models (LLMs) to interpret nuanced queries. **[RealLight](https://docs.litellm.ai) (also called LiteLLM)** is a thin Python wrapper that standardizes calls to providers like OpenAI and Anthropic. It allows you to use your existing OpenAI key while enabling fallback to other providers.\n",
    "\n",
    "In this exercise, we will:\n",
    "\n",
    "* Build a small, labeled dataset of query intents (factual, procedural, exploratory).\n",
    "* Train baseline classifiers (rule‑based and logistic regression) on the dataset.\n",
    "* Integrate **RealLight** to call an LLM for intent classification, using environment variables for your API key.\n",
    "* Route queries to different retrieval functions (simulated) based on predicted intent.\n",
    "* Evaluate performance and analyze failure patterns to highlight improvements.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* **Python 3.10+**\n",
    "* Packages: `scikit‑learn==1.4.0`, `pandas==2.2.0`, `litellm==1.39.1` (wraps OpenAI), `openai==1.12.0`\n",
    "* An environment variable `OPENAI_API_KEY` (or `LITELLM_OPENAI_KEY`) set to your API key. Never hard‑code secrets!\n",
    "* For demonstration, we use synthetic data; in a real project, replace with domain‑specific queries and knowledge base.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d4f36",
   "metadata": {},
   "source": [
    "\n",
    "## Plan Outline\n",
    "\n",
    "We'll progress from simple to advanced stages:\n",
    "\n",
    "1. **Stage A – Prepare Data & Baseline Models**\n",
    "   1. Create a small labeled dataset of query–intent pairs.\n",
    "   2. Build a **rule‑based classifier** based on keywords.\n",
    "   3. Build a **logistic regression** classifier using TF‑IDF vectors.\n",
    "   4. Compare baseline performances and discuss misclassifications.\n",
    "\n",
    "2. **Stage B – RealLight LLM Classifier**\n",
    "   1. Install and set up the `litellm` wrapper.\n",
    "   2. Define a function that calls the LLM to classify intents.\n",
    "   3. Evaluate the LLM classifier on the same dataset and analyze results.\n",
    "\n",
    "3. **Stage C – Intent‑Driven Retrieval Pipeline**\n",
    "   1. Define stub functions for **BM25** (keyword), **Dense** (vector), and **Hybrid** retrieval.\n",
    "   2. Use predicted intent to select the appropriate retrieval function.\n",
    "   3. Call the LLM via RealLight to generate final answers from retrieved documents.\n",
    "\n",
    "4. **Stage D – Evaluation & Analysis**\n",
    "   1. Measure classification accuracies across classifiers.\n",
    "   2. Analyze retrieval routing decisions and identify failure patterns.\n",
    "   3. Discuss improvements such as hybrid models, additional intent classes, and larger datasets.\n",
    "\n",
    "Throughout, we'll work in **small, runnable cells**, explain each step, and highlight trade‑offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c8b94",
   "metadata": {},
   "source": [
    "\n",
    "## Stage A: Prepare Data & Baseline Models\n",
    "\n",
    "We'll start by constructing a simple dataset with queries categorized into **factual**, **procedural**, and **exploratory** intents. This synthetic dataset demonstrates the workflow; for a production system you should curate a much larger corpus drawn from your domain.\n",
    "\n",
    "The goal of this stage is to establish baseline models for intent classification:\n",
    "\n",
    "1. **Rule‑based classifier** – uses keyword matching to guess the intent.\n",
    "2. **Logistic regression classifier** – uses a TF‑IDF representation and supervised learning.\n",
    "\n",
    "Let's build the dataset and baseline models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6548b299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I reset my router?</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Give me an overview of neural networks.</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who founded the company OpenAI?</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step‑by‑step guide to change a tire.</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explain quantum computing in simple terms.</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When was the Declaration of Independence signed?</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Instructions for installing Python on Windows.</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the latest trends in artificial intel...</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query        label\n",
       "0                     What is the capital of France?      factual\n",
       "1                          How do I reset my router?   procedural\n",
       "2            Give me an overview of neural networks.  exploratory\n",
       "3                    Who founded the company OpenAI?      factual\n",
       "4               Step‑by‑step guide to change a tire.   procedural\n",
       "5         Explain quantum computing in simple terms.  exploratory\n",
       "6   When was the Declaration of Independence signed?      factual\n",
       "7     Instructions for installing Python on Windows.   procedural\n",
       "8  What are the latest trends in artificial intel...  exploratory"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stage A.1 – Create the labeled dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Each sample has a query and its intent label\n",
    "data = [\n",
    "    {\"query\": \"What is the capital of France?\", \"label\": \"factual\"},\n",
    "    {\"query\": \"How do I reset my router?\", \"label\": \"procedural\"},\n",
    "    {\"query\": \"Give me an overview of neural networks.\", \"label\": \"exploratory\"},\n",
    "    {\"query\": \"Who founded the company OpenAI?\", \"label\": \"factual\"},\n",
    "    {\"query\": \"Step‑by‑step guide to change a tire.\", \"label\": \"procedural\"},\n",
    "    {\"query\": \"Explain quantum computing in simple terms.\", \"label\": \"exploratory\"},\n",
    "    {\"query\": \"When was the Declaration of Independence signed?\", \"label\": \"factual\"},\n",
    "    {\"query\": \"Instructions for installing Python on Windows.\", \"label\": \"procedural\"},\n",
    "    {\"query\": \"What are the latest trends in artificial intelligence research?\", \"label\": \"exploratory\"},\n",
    "    # Add more examples for better coverage\n",
    "]\n",
    "\n",
    "# Load into a DataFrame\n",
    "intent_df = pd.DataFrame(data)\n",
    "intent_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1908cf",
   "metadata": {},
   "source": [
    "After constructing our small dataset, we store it in a Pandas `DataFrame`. Each row contains a `query` and its corresponding `label` (intent class). **Note:** in a real system you'll need hundreds or thousands of labeled examples, ideally labeled by subject‑matter experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731ab581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>factual</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I reset my router?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Give me an overview of neural networks.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who founded the company OpenAI?</td>\n",
       "      <td>factual</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step‑by‑step guide to change a tire.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explain quantum computing in simple terms.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>exploratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When was the Declaration of Independence signed?</td>\n",
       "      <td>factual</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Instructions for installing Python on Windows.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the latest trends in artificial intel...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query        label    pred_rule\n",
       "0                     What is the capital of France?      factual      factual\n",
       "1                          How do I reset my router?   procedural   procedural\n",
       "2            Give me an overview of neural networks.  exploratory  exploratory\n",
       "3                    Who founded the company OpenAI?      factual      factual\n",
       "4               Step‑by‑step guide to change a tire.   procedural   procedural\n",
       "5         Explain quantum computing in simple terms.  exploratory  exploratory\n",
       "6   When was the Declaration of Independence signed?      factual      factual\n",
       "7     Instructions for installing Python on Windows.   procedural   procedural\n",
       "8  What are the latest trends in artificial intel...  exploratory      factual"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stage A.2 – Implement a simple rule‑based classifier\n",
    "\n",
    "\n",
    "def rule_based_intent(query: str) -> str:\n",
    "    '''\n",
    "    A naive classifier that looks for certain keywords. If a keyword matches,\n",
    "    it returns the corresponding intent. If no keyword matches, it defaults to 'exploratory'.\n",
    "    '''\n",
    "    query_lower = query.lower()\n",
    "    factual_keywords = [\"what\", \"who\", \"when\"]\n",
    "    procedural_keywords = [\"how\", \"instructions\", \"guide\", \"step‑by‑step\"]\n",
    "\n",
    "    # Check procedural keywords first\n",
    "    if any(kw in query_lower for kw in procedural_keywords):\n",
    "        return \"procedural\"\n",
    "    elif any(kw in query_lower for kw in factual_keywords):\n",
    "        return \"factual\"\n",
    "    else:\n",
    "        return \"exploratory\"\n",
    "\n",
    "# Apply rule‑based classifier to the dataset\n",
    "intent_df[\"pred_rule\"] = intent_df[\"query\"].apply(rule_based_intent)\n",
    "\n",
    "# Show predictions\n",
    "intent_df[[\"query\", \"label\", \"pred_rule\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a37dee",
   "metadata": {},
   "source": [
    "The rule‑based classifier uses simple keyword matching. This works for obvious cues but fails on more nuanced queries. Notice that some **exploratory** queries may contain factual keywords (e.g., `\"what are the latest trends\"`), resulting in misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998c461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " exploratory       0.00      0.00      0.00         1\n",
      "     factual       0.50      1.00      0.67         1\n",
      "  procedural       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.50      0.67      0.56         3\n",
      "weighted avg       0.50      0.67      0.56         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_log_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>factual</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I reset my router?</td>\n",
       "      <td>procedural</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Give me an overview of neural networks.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who founded the company OpenAI?</td>\n",
       "      <td>factual</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step‑by‑step guide to change a tire.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explain quantum computing in simple terms.</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When was the Declaration of Independence signed?</td>\n",
       "      <td>factual</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Instructions for installing Python on Windows.</td>\n",
       "      <td>procedural</td>\n",
       "      <td>procedural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the latest trends in artificial intel...</td>\n",
       "      <td>exploratory</td>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query        label pred_log_reg\n",
       "0                     What is the capital of France?      factual      factual\n",
       "1                          How do I reset my router?   procedural          NaN\n",
       "2            Give me an overview of neural networks.  exploratory          NaN\n",
       "3                    Who founded the company OpenAI?      factual          NaN\n",
       "4               Step‑by‑step guide to change a tire.   procedural          NaN\n",
       "5         Explain quantum computing in simple terms.  exploratory          NaN\n",
       "6   When was the Declaration of Independence signed?      factual          NaN\n",
       "7     Instructions for installing Python on Windows.   procedural   procedural\n",
       "8  What are the latest trends in artificial intel...  exploratory      factual"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stage A.3 – Train a logistic regression classifier using TF‑IDF vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    intent_df[\"query\"], intent_df[\"label\"], test_size=0.3, random_state=42, stratify=intent_df[\"label\"]\n",
    ")\n",
    "\n",
    "# Convert text to TF‑IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(max_iter=200, n_jobs=None)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = log_reg.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Store predictions back in the DataFrame for analysis\n",
    "intent_df.loc[X_test.index, \"pred_log_reg\"] = y_pred\n",
    "\n",
    "# Show predictions side‑by‑side\n",
    "intent_df[[\"query\", \"label\", \"pred_log_reg\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57933f8f",
   "metadata": {},
   "source": [
    "The logistic regression classifier uses TF‑IDF features and tends to generalize better than simple rules. However, with a small dataset the model may overfit and misclassify ambiguous queries. For enterprise applications, consider **augmenting with more data** and exploring other models such as support vector machines or even transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1f7d9",
   "metadata": {},
   "source": [
    "\n",
    "## Stage B: RealLight (LiteLLM) LLM Classifier\n",
    "\n",
    "Now we'll integrate a large language model via **RealLight (LiteLLM)**. This wrapper provides a unified API for calling models from OpenAI and other providers. We'll use it to classify intents, which can capture subtler cues beyond keywords.\n",
    "\n",
    "### Why RealLight?\n",
    "\n",
    "* **Standardized Interface:** You write the same code regardless of whether you're using OpenAI, Anthropic, or another provider.\n",
    "* **Fallback Capability:** You can configure multiple providers and RealLight will automatically failover if one is unavailable.\n",
    "* **Streaming and Logging:** RealLight supports streaming responses and unified logging for observability (useful for later exercises).\n",
    "\n",
    "We'll write a simple function that sends a classification prompt to the LLM and returns the predicted intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff83a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<missing>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stage B.1 – Install litellm if not already installed\n",
    "# Note: Uncomment the following line in a live environment to install. In this notebook,\n",
    "# we assume it is already installed in your environment.\n",
    "# !pip install litellm==1.39.1 openai==1.12.0\n",
    "\n",
    "import os\n",
    "# Ensure your API key is set in the environment. Replace 'YOUR_KEY' with your actual key or set\n",
    "# the environment variable externally before running the notebook. DO NOT hard‑code secrets here!\n",
    "os.environ.get(\"OPENAI_API_KEY\", \"<missing>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82805cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stage B.2 – Define a function to classify intents with an LLM via RealLight\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# We'll import litellm. If not installed, uncomment the pip install in the previous cell.\n",
    "try:\n",
    "    import litellm  # type: ignore\n",
    "except ImportError:\n",
    "    print(\"litellm is not installed. Please run the install cell above.\")\n",
    "\n",
    "\n",
    "def llm_intent_classify(queries: List[str], model: str = \"gpt-3.5-turbo\", temperature: float = 0.0) -> List[str]:\n",
    "    '''\n",
    "    Classify a list of queries using an LLM via RealLight. The model should respond with one\n",
    "    of the following intents: 'factual', 'procedural', or 'exploratory'.\n",
    "\n",
    "    Parameters:\n",
    "        queries: List of user queries.\n",
    "        model: Name of the model supported by RealLight/OpenAI.\n",
    "        temperature: Sampling temperature (0 for deterministic).\n",
    "\n",
    "    Returns:\n",
    "        List of predicted intent strings corresponding to each query.\n",
    "    '''\n",
    "    predictions = []\n",
    "    for query in queries:\n",
    "        # Compose a system + user prompt for few‑shot classification\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that classifies user queries into one of three intents: factual, procedural, or exploratory.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the intent of the following query: '{query}'. Respond with just the intent.\"},\n",
    "        ]\n",
    "        try:\n",
    "            response = litellm.completion(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=1,\n",
    "            )\n",
    "            # Extract the assistant's reply (strip whitespace)\n",
    "            intent = response.choices[0].message.content.strip().lower()\n",
    "        except Exception as ex:\n",
    "            print(f\"Error calling LLM for query '{query}': {ex}\")\n",
    "            intent = \"exploratory\"  # default fallback\n",
    "        predictions.append(intent)\n",
    "    return predictions\n",
    "\n",
    "# Example usage (only runs if litellm and API key are available)\n",
    "# llm_predictions = llm_intent_classify(intent_df[\"query\"].tolist())\n",
    "# intent_df[\"pred_llm\"] = llm_predictions\n",
    "# intent_df[[\"query\", \"label\", \"pred_llm\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a467107",
   "metadata": {},
   "source": [
    "This function constructs a **few‑shot classification prompt** for each query and sends it through the RealLight wrapper. It expects the model to respond with one of the three intent labels. We keep the temperature at 0 for deterministic outputs. Error handling defaults to `exploratory` on exceptions (e.g., missing API key or network issue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e76070",
   "metadata": {},
   "source": [
    "\n",
    "## Stage C: Intent‑Driven Retrieval Pipeline\n",
    "\n",
    "With our classifiers in place, we can now route queries to different retrieval strategies. In production, you might have:\n",
    "\n",
    "* **BM25 / ElasticSearch** for factual queries – fast keyword search over your document collection.\n",
    "* **Dense / Vector search** (e.g., via FAISS or Chroma) for exploratory queries – captures semantic similarity.\n",
    "* **Hybrid** approaches combining both for procedural queries – ensures step‑by‑step instructions are fully covered.\n",
    "\n",
    "Here we'll simulate these with simple stub functions. The goal is to demonstrate the **decision logic** for routing based on predicted intent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2024121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n",
      "Predicted intent: factual\n",
      "Routed answer: Answer based on [BM25] Retrieved keyword‑relevant passages for: What is the capital of France?\n",
      "\n",
      "Query: What are the latest trends in artificial intelligence research?\n",
      "Predicted intent: factual\n",
      "Routed answer: Answer based on [BM25] Retrieved keyword‑relevant passages for: What are the latest trends in artificial intelligence research?\n",
      "\n",
      "Query: Instructions for installing Python on Windows.\n",
      "Predicted intent: procedural\n",
      "Routed answer: Answer based on [Hybrid] Retrieved combined passages for: Instructions for installing Python on Windows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stage C.1 – Define stub retrieval functions\n",
    "\n",
    "def bm25_retrieve(query: str) -> str:\n",
    "    '''Simulate BM25 retrieval by returning a canned passage.'''\n",
    "    return f\"[BM25] Retrieved keyword‑relevant passages for: {query}\"\n",
    "\n",
    "\n",
    "def dense_retrieve(query: str) -> str:\n",
    "    '''Simulate dense retrieval by returning a semantically matched passage.'''\n",
    "    return f\"[Dense] Retrieved semantically similar passages for: {query}\"\n",
    "\n",
    "\n",
    "def hybrid_retrieve(query: str) -> str:\n",
    "    '''Simulate hybrid retrieval by combining keyword and dense results.'''\n",
    "    return f\"[Hybrid] Retrieved combined passages for: {query}\"\n",
    "\n",
    "# Stage C.2 – Define a routing function\n",
    "\n",
    "def route_and_answer(query: str, intent: str) -> str:\n",
    "    '''\n",
    "    Based on the predicted intent, select a retrieval function and return a simulated answer.\n",
    "    In practice, you'd feed the retrieved docs to another LLM for final answering.\n",
    "    '''\n",
    "    if intent == \"factual\":\n",
    "        retrieved = bm25_retrieve(query)\n",
    "    elif intent == \"procedural\":\n",
    "        retrieved = hybrid_retrieve(query)\n",
    "    else:\n",
    "        retrieved = dense_retrieve(query)\n",
    "\n",
    "    # Simulate an answer generation step via an LLM (not executed here)\n",
    "    answer = f\"Answer based on {retrieved}\"\n",
    "    return answer\n",
    "\n",
    "# Stage C.3 – Test the routing with baseline logistic regression predictions\n",
    "\n",
    "# Use logistic regression predictions if available\n",
    "try:\n",
    "    test_predictions = intent_df.loc[X_test.index, \"pred_log_reg\"].tolist()\n",
    "    for q, pred in zip(X_test.tolist(), test_predictions):\n",
    "        answer = route_and_answer(q, pred)\n",
    "        print(f\"Query: {q}\\nPredicted intent: {pred}\\nRouted answer: {answer}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Cannot run routing test:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3f6f8",
   "metadata": {},
   "source": [
    "This simulation shows how your system selects different retrieval strategies based on the predicted intent. In an enterprise RAG pipeline, you would replace the stubs with calls to your **BM25 search engine**, **vector database**, or **hybrid aggregator**. After retrieval, pass the documents back to the LLM (via RealLight) to generate a final answer with citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bf5e0",
   "metadata": {},
   "source": [
    "\n",
    "## Stage D: Evaluation & Analysis\n",
    "\n",
    "Let's compare the performance of our classifiers and discuss how routing impacts retrieval. We'll compute simple accuracy metrics and highlight where each approach succeeds or fails.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa414187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule‑based accuracy: 0.89\n",
      "Logistic regression accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stage D.1 – Evaluate the rule‑based and logistic regression classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Compute accuracy for rule‑based classifier\n",
    "rule_accuracy = accuracy_score(intent_df[\"label\"], intent_df[\"pred_rule\"])\n",
    "\n",
    "# For logistic regression, we need predictions for the whole dataset\n",
    "# We'll fit on full data for a fair comparison (not ideal in production)\n",
    "full_X_vec = vectorizer.transform(intent_df[\"query\"])\n",
    "log_reg_full = LogisticRegression(max_iter=200)\n",
    "log_reg_full.fit(full_X_vec, intent_df[\"label\"])\n",
    "log_reg_preds = log_reg_full.predict(full_X_vec)\n",
    "log_accuracy = accuracy_score(intent_df[\"label\"], log_reg_preds)\n",
    "\n",
    "print(f\"Rule‑based accuracy: {rule_accuracy:.2f}\")\n",
    "print(f\"Logistic regression accuracy: {log_accuracy:.2f}\")\n",
    "\n",
    "# Stage D.2 – (Optional) Evaluate the LLM classifier if available\n",
    "try:\n",
    "    # Only run if RealLight predictions exist in DataFrame\n",
    "    if \"pred_llm\" in intent_df.columns:\n",
    "        llm_accuracy = accuracy_score(intent_df[\"label\"], intent_df[\"pred_llm\"])\n",
    "        print(f\"LLM classifier accuracy: {llm_accuracy:.2f}\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c232c72",
   "metadata": {},
   "source": [
    "On a small dataset, the logistic regression model typically outperforms the simple rule‑based approach. Integrating an LLM via RealLight can yield even better results on nuanced queries but incurs latency and cost. Enterprise systems often **combine multiple classifiers**: a fast, lightweight model for common cases and an LLM fallback for hard queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f2cbd",
   "metadata": {},
   "source": [
    "\n",
    "### Failure Patterns & Improvement Suggestions\n",
    "\n",
    "From the accuracy scores and manual inspection:\n",
    "\n",
    "* **Rule‑Based Weaknesses:** Misclassifies queries containing both factual and exploratory cues (e.g., \"What are the latest trends...\" is exploratory but contains \"what\").\n",
    "* **Logistic Regression Limitations:** With so few samples, the model can overfit and may not generalize. More labeled data and regularization help.\n",
    "* **LLM (RealLight) Classifier:** With properly designed prompts, an LLM can understand intent better. However, it may still confuse procedural with exploratory queries if instructions are vague.\n",
    "\n",
    "**Recommendations for enterprise‑grade intent routing:**\n",
    "\n",
    "1. **Expand the dataset:** Collect hundreds of real customer queries per intent class. Consider additional classes (billing, troubleshooting, feedback, etc.).\n",
    "2. **Hybrid classification:** Use a lightweight model for clear cases and fall back to the LLM for ambiguous queries. This reduces latency and cost.\n",
    "3. **Prompt engineering:** Provide few‑shot examples and clear instructions to the LLM to improve classification consistency. Test different models via RealLight.\n",
    "4. **Monitoring & Feedback:** Log misclassifications, user corrections, and use them to continuously retrain your models.\n",
    "\n",
    "By iterating on these steps, you can build a robust **intent router** that feeds your retrieval pipeline for a high‑quality enterprise RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb259a",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap‑Up\n",
    "\n",
    "In this exercise we:\n",
    "\n",
    "* Built a labeled dataset of queries and intents.\n",
    "* Implemented baseline classifiers and evaluated their performance.\n",
    "* Integrated RealLight (LiteLLM) to call a large language model for intent classification. We used environment variables for the API key to keep secrets secure.\n",
    "* Constructed a simulated retrieval pipeline and routed queries based on predicted intent.\n",
    "* Evaluated the routing logic and identified areas for improvement.\n",
    "\n",
    "**Next Steps:** In a real enterprise setting, pair this intent classification component with your retrieval back‑end (BM25 index, vector store) and feed the retrieved documents back into the LLM for final answer generation with citations. Continuously monitor performance and retrain the classifiers as your query distribution evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2dcab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick Install (run once)\n",
    "# These commands pin versions to ensure reproducibility. Uncomment to install.\n",
    "# !pip install scikit-learn==1.4.0 pandas==2.2.0 litellm==1.39.1 openai==1.12.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359b82d-fc0d-4dd5-a43d-ea1caf1eae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eedd65-35d9-4d75-9c18-01aac2c39c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
