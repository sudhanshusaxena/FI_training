{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "LGLAZ2IgLnPz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41856,
     "status": "ok",
     "timestamp": 1741260024567,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "LGLAZ2IgLnPz",
    "outputId": "ab8f7e08-d03a-4e4a-e069-8547c9e38029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.93.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: pypdf in /opt/anaconda3/lib/python3.12/site-packages (5.7.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.67)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (0.3.67)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.4)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.14.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (2.11.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai faiss-cpu pypdf\n",
    "!pip install langchain-community langchain-core\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e416bff-7d84-4448-90c0-a89f5611c8f6",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741260034839,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "7e416bff-7d84-4448-90c0-a89f5611c8f6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e414eaa-9c80-4373-89ae-e736b309e8c3",
   "metadata": {
    "id": "8e414eaa-9c80-4373-89ae-e736b309e8c3"
   },
   "source": [
    "2. **Define Long-Term Memory**\n",
    "   \n",
    "    Use a database or persistent storage to maintain long-term memory across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6100cf9-275e-4ece-9dc9-fc55504e0699",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8464,
     "status": "ok",
     "timestamp": 1741260051305,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "f6100cf9-275e-4ece-9dc9-fc55504e0699",
    "outputId": "7dd48b0a-9094-4e22-80a9-819d52de5780"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_80539/584890007.py:33: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_80539/584890007.py:38: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simulating a persistent storage (In a real application, use a database)\n",
    "class PersistentMemory:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def save(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def load(self, key):\n",
    "        return self.data.get(key, \"\")\n",
    "\n",
    "# Instantiate persistent memory\n",
    "persistent_memory = PersistentMemory()\n",
    "\n",
    "# Define the memory with persistent storage\n",
    "class PersistentConversationMemory(ConversationBufferMemory):\n",
    "    def save_context(self, inputs, outputs):\n",
    "        super().save_context(inputs, outputs)\n",
    "        persistent_memory.save(\"history\", self.memory)\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        self.memory = persistent_memory.load(\"history\")\n",
    "        return super().load_memory_variables(inputs)\n",
    "\n",
    "memory = PersistentConversationMemory(memory_key=\"history\")\n",
    "\n",
    "# Define the LLM and prompt\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "template = \"You are an assistant that remembers past interactions.\\n\\n{history}\\nUser: {input}\\nAssistant:\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "# Create the LLMChain with persistent memory\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911a12-56a2-4ae4-ad3e-b30b34d3009f",
   "metadata": {
    "id": "d7911a12-56a2-4ae4-ad3e-b30b34d3009f"
   },
   "source": [
    "3. **Implement User-Specific Memory**\n",
    "   \n",
    "    Create separate memory instances for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dffcd4-5884-405b-9531-e9eb550c3c4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2532,
     "status": "ok",
     "timestamp": 1741260069584,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "42dffcd4-5884-405b-9531-e9eb550c3c4a",
    "outputId": "91142200-c886-4ac4-8f07-0a3e80148b17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/jcp2dsss28lbqc7_f9j6vdb00000gn/T/ipykernel_80539/2150458432.py:59: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input=user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Nice to meet you, Alice! Is there anything I can help you with?\n",
      "User 2:  Hello again, Bob. How are you doing? Is there something else you needed?\n",
      "User 2:  Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "\n",
    "# Simulating a persistent storage (In a real application, use a database)\n",
    "class PersistentMemory:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def save(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def load(self, key):\n",
    "        return self.data.get(key, ChatMessageHistory())\n",
    "\n",
    "# Instantiate persistent memory\n",
    "persistent_memory = PersistentMemory()\n",
    "\n",
    "# Define the memory with persistent storage\n",
    "class PersistentConversationMemory(ConversationBufferMemory):\n",
    "    def save_context(self, inputs, outputs):\n",
    "        super().save_context(inputs, outputs)\n",
    "        persistent_memory.save(\"history\", self.chat_memory)\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        self.chat_memory = persistent_memory.load(\"history\")\n",
    "        return super().load_memory_variables(inputs)\n",
    "\n",
    "# Function to manage user-specific memories\n",
    "class UserSpecificMemoryManager:\n",
    "    def __init__(self):\n",
    "        self.user_memories = {}\n",
    "\n",
    "    def get_memory(self, user_id):\n",
    "        if user_id not in self.user_memories:\n",
    "            self.user_memories[user_id] = PersistentConversationMemory(memory_key=\"history\")\n",
    "        return self.user_memories[user_id]\n",
    "\n",
    "user_memory_manager = UserSpecificMemoryManager()\n",
    "\n",
    "# Define the LLM and prompt\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "template = \"You are an assistant that remembers past interactions.\\n\\n{history}\\nUser: {input}\\nAssistant:\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "# Function to get chain for a specific user\n",
    "def get_user_chain(user_id):\n",
    "    memory = user_memory_manager.get_memory(user_id)\n",
    "    return LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# Example usage for two different users\n",
    "user_id_1 = \"user_1\"\n",
    "user_id_2 = \"user_2\"\n",
    "\n",
    "# User 1 interaction\n",
    "user_input = \"My name is Alice.\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n",
    "\n",
    "# User 2 interaction\n",
    "user_input = \"My name is Bob.\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)\n",
    "\n",
    "# Follow-up interaction for User 2\n",
    "user_input = \"What is my name?\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c892565-62ad-41bd-8624-dc6fa7008a47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2206,
     "status": "ok",
     "timestamp": 1741260125362,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "1c892565-62ad-41bd-8624-dc6fa7008a47",
    "outputId": "55152bba-742a-4ced-a04a-745a04796251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "User 2:  Hello Bob, how can I assist you today?\n",
      "User 1:  Your name is Alice.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "\n",
    "# Simulating a persistent storage (In a real application, use a database)\n",
    "class PersistentMemory:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def save(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def load(self, key):\n",
    "        return self.data.get(key, ChatMessageHistory())\n",
    "\n",
    "# Instantiate persistent memory\n",
    "persistent_memory = PersistentMemory()\n",
    "\n",
    "# Define the memory with persistent storage\n",
    "class PersistentConversationMemory(ConversationBufferMemory):\n",
    "    user_id: str = Field(default=None)\n",
    "\n",
    "    def __init__(self, user_id, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.user_id = user_id\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        super().save_context(inputs, outputs)\n",
    "        persistent_memory.save(self.user_id, self.chat_memory)\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        self.chat_memory = persistent_memory.load(self.user_id)\n",
    "        return super().load_memory_variables(inputs)\n",
    "\n",
    "# Function to manage user-specific memories\n",
    "class UserSpecificMemoryManager:\n",
    "    def __init__(self):\n",
    "        self.user_memories = {}\n",
    "\n",
    "    def get_memory(self, user_id):\n",
    "        if user_id not in self.user_memories:\n",
    "            self.user_memories[user_id] = PersistentConversationMemory(user_id=user_id, memory_key=\"history\")\n",
    "        return self.user_memories[user_id]\n",
    "\n",
    "user_memory_manager = UserSpecificMemoryManager()\n",
    "\n",
    "# Define the LLM and prompt\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "template = \"You are an assistant that remembers past interactions.\\n\\n{history}\\nUser: {input}\\nAssistant:\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "# Function to get chain for a specific user\n",
    "def get_user_chain(user_id):\n",
    "    memory = user_memory_manager.get_memory(user_id)\n",
    "    return LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# Example usage for two different users\n",
    "user_id_1 = \"user_1\"\n",
    "user_id_2 = \"user_2\"\n",
    "\n",
    "# User 1 interaction\n",
    "user_input = \"My name is Alice.\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n",
    "\n",
    "# User 2 interaction\n",
    "user_input = \"My name is Bob.\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)\n",
    "\n",
    "# Follow-up interaction for User 1\n",
    "user_input = \"What is my name?\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f3e926-38e5-4bc3-9d7b-49f1a3cd2d21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1741260136673,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "29f3e926-38e5-4bc3-9d7b-49f1a3cd2d21",
    "outputId": "63a4dbb5-db42-428d-c470-0c4ee697ecd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Your name is Bob. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "# Follow-up interaction for User 2\n",
    "user_input = \"What is my name again?\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ca6348-980c-4461-94e9-d978342dc8c2",
   "metadata": {
    "id": "c5ca6348-980c-4461-94e9-d978342dc8c2",
    "outputId": "840c119e-ab21-4790-abd2-02c19df60902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Noted. I will remember your food preferences for future recommendations. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "# Follow-up interaction for User 1\n",
    "user_input = \"I like choclates, tacos, I dont like plant based foods\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62601ec-7817-4701-965e-0f35b54fb7a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1741260165635,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "e62601ec-7817-4701-965e-0f35b54fb7a5",
    "outputId": "b3d7badf-9792-42ae-91b1-2e3b540650e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 2:  Got it, I will make sure to only suggest plant-based options for you in the future. Is there anything else you would like me to remember?\n"
     ]
    }
   ],
   "source": [
    "# Follow-up interaction for User 2\n",
    "user_input = \"I am a vegan and I like only plant based food\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f1ca0-bb2c-4adb-8886-9ed671f4518e",
   "metadata": {
    "id": "d44f1ca0-bb2c-4adb-8886-9ed671f4518e"
   },
   "source": [
    "4. **Manage Context Over Extended Conversations**\n",
    "   \n",
    "    Ensure that the memory system can handle and manage context over long conversations effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018f1ab6-3a40-45b7-8c1f-f780eff13d8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1741260177515,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "018f1ab6-3a40-45b7-8c1f-f780eff13d8b",
    "outputId": "d1530ecf-963b-45a4-e87c-95d2d28e2f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Why couldn't the bicycle stand up by itself? Because it was two-tired.\n",
      "User 1:  The joke I told you was: \"Why couldn't the bicycle stand up by itself? Because it was two-tired.\" Did you enjoy it?\n"
     ]
    }
   ],
   "source": [
    "# Adding more interactions to demonstrate context management\n",
    "user_input = \"Tell me a joke.\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n",
    "\n",
    "user_input = \"What was the joke you told me?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f29c49-4909-4f50-b67c-da301c67a314",
   "metadata": {
    "id": "a6f29c49-4909-4f50-b67c-da301c67a314",
    "outputId": "92481836-eac3-43c8-e553-06a68c763ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Hello again, Alice! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# User 1 Interaction - Initial Query:\n",
    "\n",
    "user_id_1 = \"user_1\"\n",
    "\n",
    "# Initial interaction for User 1\n",
    "user_input = \"My name is Alice.\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d46522-9728-4661-91d7-11494c17f51d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1741260227406,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "b5d46522-9728-4661-91d7-11494c17f51d",
    "outputId": "b327045d-8af4-4df1-c86d-26e6beb63db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Food preferences are the types of foods that you like or do not like. Based on your previous interaction, you mentioned that you like chocolates and tacos, but you do not like plant-based foods. Some delicious options for you could be a chocolate lava cake, crispy fish tacos, or a cheesy beef and bean burrito. Would you like me to suggest any specific dishes or restaurants for you?\n"
     ]
    }
   ],
   "source": [
    "# User 1 Follow-Up Interaction:\n",
    "# Follow-up interaction for User 1\n",
    "user_input = \"What is food preferences and help me  with some delicious foods available for me ?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad8707-7c28-4b93-b43e-4f598045df41",
   "metadata": {
    "id": "7fad8707-7c28-4b93-b43e-4f598045df41"
   },
   "source": [
    "# User 1 Extended Conversation - More Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a801e1b1-ae35-4fc3-b5e6-c3c2602d09b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1741260326247,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "a801e1b1-ae35-4fc3-b5e6-c3c2602d09b7",
    "outputId": "5c787bf4-3c73-44ac-d391-5f9bccf1e88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Why was the math book sad? Because it had too many problems.\n"
     ]
    }
   ],
   "source": [
    "# Ask more questions to showcase memory capabilities\n",
    "user_input = \"Tell me a joke.\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ef79d55-d508-4092-bc16-acdd47a71979",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1741260330510,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "9ef79d55-d508-4092-bc16-acdd47a71979",
    "outputId": "4f5ee246-046e-4144-cb84-e2ddf66c3925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  The joke I told you was: Why was the math book sad? Because it had too many problems. Did you enjoy it?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What was the joke you told me?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ee20ac-2bc6-4348-a174-cdfdf679a244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1741260335746,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "14ee20ac-2bc6-4348-a174-cdfdf679a244",
    "outputId": "09ee5165-c461-4bf6-d300-d6a3db08b5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Sure thing, Alice the Great! Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Now call me Alice the Great.\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3edac7-2c63-41f6-bce7-1230f893064d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1741260342901,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "9b3edac7-2c63-41f6-bce7-1230f893064d",
    "outputId": "6cb69146-01cf-42bf-f354-3120fa8d1cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Your name is still Alice the Great.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is my name now?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2452ae-b0f5-4951-a579-1f1c3b7d15b1",
   "metadata": {
    "id": "ea2452ae-b0f5-4951-a579-1f1c3b7d15b1",
    "outputId": "6447e15e-4fd5-4f62-f87d-cc3c1085aba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Noted. I will remember that you like ice cream for future recommendations. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember that I like ice cream.\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a52b65-6658-4147-963b-4e9503755c0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1741260359520,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "e6a52b65-6658-4147-963b-4e9503755c0a",
    "outputId": "fa1d3943-ca58-4942-b6fa-41636fd5f4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Based on our previous interactions, you mentioned that you like chocolate, tacos, and ice cream. Is there anything else you would like me to remember?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What do I like?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d89424e-6e8c-4f75-ba94-72722864d98d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1741260379610,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "0d89424e-6e8c-4f75-ba94-72722864d98d",
    "outputId": "418062c8-e68f-4411-8cb3-364f2018945c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Noted. I will add that to your list of food preferences. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Remember that I like Turkey Hill's Colombian Coffee also .\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf1eae0f-6a38-4d03-b5fa-927b80f2d888",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1741260443843,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "cf1eae0f-6a38-4d03-b5fa-927b80f2d888",
    "outputId": "b2a19252-9ad5-4d33-ed5c-3c54787ba23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 2:  Based on our previous interactions, you like chocolate, tacos, ice cream, and Turkey Hill's Colombian Coffee. Is there anything else you would like me to remember?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What all things I like ?\"\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380c9a1-ae82-4eab-a5e9-25b23fdd1a2a",
   "metadata": {
    "id": "c380c9a1-ae82-4eab-a5e9-25b23fdd1a2a"
   },
   "source": [
    "# Implementing the Conversational Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d32656d-f14b-43a1-9056-948f449a9046",
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1741260472738,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "6d32656d-f14b-43a1-9056-948f449a9046"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "template = \"You are an assistant that remembers past interactions.\\n\\n{history}\\nUser: {input}\\nAssistant:\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c151b8e9-fb34-4ef2-9f4f-58589749f6d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 431215,
     "status": "ok",
     "timestamp": 1741260906662,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "c151b8e9-fb34-4ef2-9f4f-58589749f6d1",
    "outputId": "7b2678f9-3f90-46a3-ce60-f79ea4b6f93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to converse! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def run_conversational_bot(user_id):\n",
    "    print(\"Chatbot is ready to converse! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        chain = get_user_chain(user_id)\n",
    "        response = chain.run(input=user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Start the conversational bot for a specific user\n",
    "user_id = \"user_1\"\n",
    "run_conversational_bot(user_id_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3024eb03-4d13-412e-bf84-16e3fa7cb496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26886,
     "status": "ok",
     "timestamp": 1741261169129,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "3024eb03-4d13-412e-bf84-16e3fa7cb496",
    "outputId": "5227019e-303a-48ba-b10e-46a04c99d380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to converse! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def run_conversational_bot(user_id):\n",
    "    print(\"Chatbot is ready to converse! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        chain = get_user_chain(user_id_2)\n",
    "        response = chain.run(input=user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Start the conversational bot for a specific user\n",
    "user_id = \"user_2\"\n",
    "run_conversational_bot(user_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "412bfde5-fdc0-4d0c-bc73-45d97e9bb5bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1741261175153,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "412bfde5-fdc0-4d0c-bc73-45d97e9bb5bb",
    "outputId": "586d0d97-900f-49f0-9381-14125375d5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1:  Your food preferences are chocolate, tacos, ice cream, and Turkey Hill's Colombian Coffee. Is there anything else you would like me to remember?\n"
     ]
    }
   ],
   "source": [
    "# Initial interaction for User 1\n",
    "user_input = \"what are my food preferneces\"\n",
    "chain = get_user_chain(user_id_1)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 1:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5aa0fb4-c087-49eb-b641-7fdfbfe08300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1741261193024,
     "user": {
      "displayName": "Sudhanshu Saxena",
      "userId": "06805949978483919255"
     },
     "user_tz": -330
    },
    "id": "b5aa0fb4-c087-49eb-b641-7fdfbfe08300",
    "outputId": "514b0904-deb5-47e7-fd10-46899ca9de3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 2:  Your food preferences are vegan and plant-based. Would you like me to suggest any vegan recipes for you?\n"
     ]
    }
   ],
   "source": [
    "# Follow-up interaction for User 2\n",
    "user_input = \"what are my food preferneces ?\"\n",
    "chain = get_user_chain(user_id_2)\n",
    "response = chain.run(input=user_input)\n",
    "print(\"User 2:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba5792-7429-46d2-9a06-883ae8777343",
   "metadata": {
    "id": "b9ba5792-7429-46d2-9a06-883ae8777343"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
